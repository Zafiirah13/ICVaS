{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# Good Code for Hierarchical Classification\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "#plt.switch_backend('agg')\n",
    "% matplotlib inline\n",
    "\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import FATS\n",
    "\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "figSize  = (12, 8)\n",
    "fontSize = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "from scipy import interp\n",
    "from itertools import cycle, islice\n",
    "from george import kernels\n",
    "import george\n",
    "import scipy.optimize as op\n",
    "\n",
    "\n",
    "# Some preprocessing utilities\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.manifold.t_sne import TSNE\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# The different classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Section Needs to be changed each time\n",
    "\n",
    "nFeatures = 7 # Number of features considered\n",
    "\n",
    "# All Labels of the different variabe stars\n",
    "true_class_1=1;true_class_2=2;true_class_3=3;true_class_4=4;true_class_5=5;true_class_6=6;true_class_7=7;\\\n",
    "true_class_8=8;true_class_9=9;true_class_10=10;true_class_11=11;true_class_12=12;true_class_13=13\n",
    "\n",
    "eclipsing_label = 20;rotational_label = 21;pulsating_label = 22;RR_Lyrae_label = 23; LPV_label = 24;\\\n",
    "delta_scuti_label = 25; cepheids_label   = 26\n",
    "\n",
    "# The directory to save the files\n",
    "plots_dir                 = './hierarchical-results_gp/plots/'\n",
    "results_dir               = './hierarchical-results_gp/results/'\n",
    "misclassify_dir           = r'./hierarchical-results_gp/results/Misclassification_'\n",
    "dotfile_dir               = './hierarchical-results_gp/Decision_tree_plots/'\n",
    "savedir_decision_boundary ='./hierarchical-results_gp/decision_boundary_plots/'\n",
    "features_dir              ='./hierarchical-results_gp/feature_analysis/'\n",
    "lC_dir                    = '../SSS_Per_Var_Cat/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stars_label(data, label):\n",
    "    '''Set variable names to specific class label'''\n",
    "    stars = data[data.Type == label]\n",
    "    return stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Layer Hierarchical Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_layer():\n",
    "    '''\n",
    "    We define first layer of the hierarchical tree. The first layer consists of Eclipsing Binaries, Rotational,\n",
    "    and Pulsating \n",
    "    '''\n",
    "    \n",
    "    # First Layer\n",
    "    eclipsing_binary_train       = pd.concat([contact_Bi_train, semi_det_Bi_train], axis=0)\n",
    "    eclipsing_binary_train_class = np.full(len(eclipsing_binary_train), eclipsing_label, dtype=int)\n",
    "\n",
    "    rotational_train       = rot_train\n",
    "    rotational_train_class = np.full(len(rotational_train),rotational_label, dtype=int)\n",
    "\n",
    "    pulsating_train       = pd.concat([RRab_train, RRc_train, RRd_train, blazhko_train, LPV_train, delta_scuti_train, ACEP_train, cep_ii_train] ,axis=0)\n",
    "    pulsating_train_class = np.full(len(pulsating_train), pulsating_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"eclipsing_binary_train has {}\".format(eclipsing_binary_train.shape))\n",
    "    print(\"pulsating_train has {}\".format(pulsating_train.shape))\n",
    "    print(\"rotational_train has {}\".format(rotational_train.shape))\n",
    "\n",
    "    eclipsing_binary_test       = pd.concat([contact_Bi_test, semi_det_Bi_test], axis=0)\n",
    "    eclipsing_binary_test_class = np.full(len(eclipsing_binary_test), eclipsing_label, dtype=int)\n",
    "\n",
    "    rotational_test       = rot_test\n",
    "    rotational_test_class = np.full(len(rotational_test), rotational_label, dtype=int)\n",
    "\n",
    "    pulsating_test       = pd.concat([RRab_test, RRc_test, RRd_test, blazhko_test, LPV_test, delta_scuti_test, ACEP_test, cep_ii_test] ,axis=0)\n",
    "    pulsating_test_class = np.full(len(pulsating_test), pulsating_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"eclipsing_binary_test has {}\".format(eclipsing_binary_test.shape))\n",
    "    print(\"pulsating_test has {}\".format(pulsating_test.shape))\n",
    "    print(\"rotational_test has {}\".format(rotational_test.shape))\n",
    "    \n",
    "    first_layer_train       = pd.concat([eclipsing_binary_train, rotational_train, pulsating_train], axis=0)\n",
    "    first_layer_train_class = np.concatenate((eclipsing_binary_train_class, rotational_train_class, pulsating_train_class), axis=0)\n",
    "    training_data_FL        = pd.DataFrame(first_layer_train)\n",
    "    training_data_FL['New_label'] = first_layer_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    first_layer_test       = pd.concat([eclipsing_binary_test, rotational_test, pulsating_test], axis=0)\n",
    "    first_layer_test_class = np.concatenate((eclipsing_binary_test_class, rotational_test_class, pulsating_test_class), axis=0)\n",
    "    testing_data_FL        = pd.DataFrame(first_layer_test)\n",
    "    testing_data_FL['New_label'] = first_layer_test_class\n",
    "    \n",
    "    y_FL_training, y_FL_training_counts = np.unique(first_layer_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_FL, testing_data_FL, y_FL_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Layer Hierarchical level for first Branch: Eclipsing Binaries (Ecl & EA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def second_layer_EB():\n",
    "    \n",
    "    # Second Layer Eclipsing Binary    \n",
    "    ecl_train = contact_Bi_train\n",
    "    ecl_train_class = np.full(len(ecl_train), true_class_5, dtype=int)\n",
    "\n",
    "    EA_train       = semi_det_Bi_train\n",
    "    EA_train_class = np.full(len(EA_train),true_class_6, dtype=int)\n",
    " \n",
    "    print(\"ecl train has {}\".format(ecl_train.shape))\n",
    "    print(\"EA_train has {}\".format(EA_train.shape))\n",
    "\n",
    "    ecl_test       = contact_Bi_test\n",
    "    ecl_test_class = np.full(len(ecl_test), true_class_5, dtype=int)\n",
    "\n",
    "    EA_test       = semi_det_Bi_test\n",
    "    EA_test_class = np.full(len(EA_test), true_class_6, dtype=int)\n",
    "\n",
    "    print(\"ecl_test has {}\".format(ecl_test.shape))\n",
    "    print(\"EA_test has {}\".format(EA_test.shape))\n",
    "\n",
    "    \n",
    "    second_layer_EB_train       = pd.concat([ecl_train, EA_train], axis=0)\n",
    "    second_layer_EB_train_class = np.concatenate((ecl_train_class,EA_train_class), axis=0)\n",
    "    training_data_SL_EB         = pd.DataFrame(second_layer_EB_train)\n",
    "    training_data_SL_EB['New_label'] = second_layer_EB_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    second_layer_EB_test       = pd.concat([ecl_test, EA_test], axis=0)\n",
    "    second_layer_EB_test_class = np.concatenate((ecl_test_class, EA_test_class), axis=0)\n",
    "    testing_data_SL_EB         = pd.DataFrame(second_layer_EB_test)\n",
    "    testing_data_SL_EB['New_label'] = second_layer_EB_test_class\n",
    "    \n",
    "    y_SL_EB_training, y_SL_EB_training_counts = np.unique(second_layer_EB_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_SL_EB, testing_data_SL_EB, y_SL_EB_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Layer Hierarchical level for 2nd Branch: RLCD\n",
    "### RR Lyrae, LPV, Cepheid and $\\delta$-Scuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 2 RR Lyrae, LPV, Cepheid, Delta-Scuti\n",
    "def second_layer_RLCD():\n",
    "    \n",
    "    # First Layer\n",
    "    RR_Lyrae_train       = pd.concat([RRab_train,RRc_train,RRd_train,blazhko_train], axis=0)\n",
    "    RR_Lyrae_train_class = np.full(len(RR_Lyrae_train), RR_Lyrae_label, dtype=int)\n",
    "\n",
    "    LPV_train_class = np.full(len(LPV_train),LPV_label, dtype=int)\n",
    "\n",
    "    cepheids_train       = pd.concat([ACEP_train,cep_ii_train] ,axis=0)\n",
    "    cepheids_train_class = np.full(len(cepheids_train), cepheids_label, dtype=int)\n",
    "    \n",
    "    ds_train       = delta_scuti_train\n",
    "    ds_train_class = np.full(len(ds_train), delta_scuti_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"RR Lyrae train has {}\".format(RR_Lyrae_train.shape))\n",
    "    print(\"LPV train has {}\".format(LPV_train.shape))\n",
    "    print(\"Cepheids train has {}\".format(cepheids_train.shape))\n",
    "    print(\"Delta Scuti train has {}\".format(ds_train.shape))\n",
    "\n",
    "    RR_Lyrae_test       = pd.concat([RRab_test,RRc_test,RRd_test,blazhko_test], axis=0)\n",
    "    RR_Lyrae_test_class = np.full(len(RR_Lyrae_test), RR_Lyrae_label, dtype=int)\n",
    "\n",
    "    LPV_test_class = np.full(len(LPV_test), LPV_label, dtype=int)\n",
    "\n",
    "    cepheids_test       = pd.concat([ACEP_test, cep_ii_test] ,axis=0)\n",
    "    cepheids_test_class = np.full(len(cepheids_test), cepheids_label, dtype=int)\n",
    "    \n",
    "    ds_test       = delta_scuti_test\n",
    "    ds_test_class = np.full(len(ds_test), delta_scuti_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"RR_Lyrae_test has {}\".format(RR_Lyrae_test.shape))\n",
    "    print(\"LPV_test has {}\".format(LPV_test.shape))\n",
    "    print(\"cepheids_test has {}\".format(cepheids_test.shape))\n",
    "    print(\"Delta Scuti test has {}\".format(ds_test.shape))\n",
    "    \n",
    "    second_layer_RLCD_train       = pd.concat([RR_Lyrae_train,LPV_train,cepheids_train,ds_train], axis=0)\n",
    "    second_layer_RLCD_train_class = np.concatenate((RR_Lyrae_train_class,LPV_train_class,cepheids_train_class,ds_train_class), axis=0)\n",
    "    training_data_SL_RLCD         = pd.DataFrame(second_layer_RLCD_train)\n",
    "    training_data_SL_RLCD['New_label'] = second_layer_RLCD_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    second_layer_RLCD_test       = pd.concat([RR_Lyrae_test,LPV_test,cepheids_test,ds_test], axis=0)\n",
    "    second_layer_RLCD_test_class = np.concatenate((RR_Lyrae_test_class,LPV_test_class,cepheids_test_class,ds_test_class), axis=0)\n",
    "    testing_data_SL_RLCD         = pd.DataFrame(second_layer_RLCD_test)\n",
    "    testing_data_SL_RLCD['New_label'] = second_layer_RLCD_test_class\n",
    "    \n",
    "    y_SL_RLCD_training, y_SL_RLCD_training_counts = np.unique(second_layer_RLCD_train_class, return_counts=True)\n",
    "\n",
    "    print(y_SL_RLCD_training)\n",
    "    print(y_SL_RLCD_training_counts)\n",
    "    return training_data_SL_RLCD, testing_data_SL_RLCD, y_SL_RLCD_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Layer Hierarchical level for first Branch: RRLyrae\n",
    "### RRab, RRc, RRd, and Blazhko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 3 RR Lyrae classes\n",
    "def third_layer_RRLyrae():\n",
    "    \n",
    "    # Third Layer\n",
    "    RRab_train_class    = np.full(len(RRab_train), true_class_1, dtype=int)\n",
    "    RRc_train_class     = np.full(len(RRc_train), true_class_2, dtype=int)\n",
    "    RRd_train_class     = np.full(len(RRd_train), true_class_3, dtype=int)\n",
    "    blazhko_train_class = np.full(len(blazhko_train), true_class_4, dtype=int)\n",
    "\n",
    "    print(\"RRab train has {}\".format(RRab_train.shape))\n",
    "    print(\"RRc train has {}\".format(RRc_train.shape))\n",
    "    print(\"RRd train has {}\".format(RRd_train.shape))\n",
    "    print(\"Blazhko train has {}\".format(blazhko_train.shape))\n",
    "    \n",
    "    RRab_test_class    = np.full(len(RRab_test), true_class_1, dtype=int)\n",
    "    RRc_test_class     = np.full(len(RRc_test), true_class_2, dtype=int)\n",
    "    RRd_test_class     = np.full(len(RRd_test), true_class_3, dtype=int)\n",
    "    blazhko_test_class = np.full(len(blazhko_test), true_class_4, dtype=int)\n",
    "\n",
    "    print(\"RRab test has {}\".format(RRab_test.shape))\n",
    "    print(\"RRc test has {}\".format(RRc_test.shape))\n",
    "    print(\"RRd test has {}\".format(RRd_test.shape))\n",
    "    print(\"Blazhko test has {}\".format(blazhko_test.shape))\n",
    "\n",
    "    \n",
    "    third_layer_RRLyrae_train       = pd.concat([RRab_train,RRc_train,RRd_train,blazhko_train], axis=0)\n",
    "    third_layer_RRLyrae_train_class = np.concatenate((RRab_train_class,RRc_train_class,RRd_train_class,blazhko_train_class), axis=0)\n",
    "    training_data_TL_RRLyrae        = pd.DataFrame(third_layer_RRLyrae_train)\n",
    "    training_data_TL_RRLyrae['New_label'] = third_layer_RRLyrae_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    third_layer_RRLyrae_test       = pd.concat([RRab_test,RRc_test,RRd_test,blazhko_test], axis=0)\n",
    "    third_layer_RRLyrae_test_class = np.concatenate((RRab_test_class,RRc_test_class,RRd_test_class,blazhko_test_class), axis=0)\n",
    "    testing_data_TL_RRLyrae         = pd.DataFrame(third_layer_RRLyrae_test)\n",
    "    testing_data_TL_RRLyrae['New_label'] = third_layer_RRLyrae_test_class\n",
    "    \n",
    "    y_TL_RRLyrae_training, y_TL_RRLyrae_training_counts = np.unique(third_layer_RRLyrae_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_TL_RRLyrae, testing_data_TL_RRLyrae, y_TL_RRLyrae_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Layer Hierarchical level for 2nd Branch: Cepheids\n",
    "### ACEP and Cep-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 3 RR Lyrae classes\n",
    "def third_layer_Cepheids():\n",
    "    \n",
    "    # Third Layer\n",
    "    ACEP_train_class   = np.full(len(ACEP_train), true_class_10, dtype=int)\n",
    "    cep_ii_train_class = np.full(len(cep_ii_train), true_class_12, dtype=int)\n",
    "\n",
    "    print(\"ACEP train has {}\".format(ACEP_train.shape))\n",
    "    print(\"Cep-II train has {}\".format(cep_ii_train.shape))\n",
    "\n",
    "\n",
    "    ACEP_test_class   = np.full(len(ACEP_test), true_class_10, dtype=int)\n",
    "    cep_ii_test_class = np.full(len(cep_ii_test), true_class_12, dtype=int)\n",
    "\n",
    "    print(\"ACEP test has {}\".format(ACEP_test.shape))\n",
    "    print(\"Cep-II test has {}\".format(cep_ii_test.shape))\n",
    "    \n",
    "    third_layer_cep_train       = pd.concat([ACEP_train,cep_ii_train], axis=0)\n",
    "    third_layer_cep_train_class = np.concatenate((ACEP_train_class,cep_ii_train_class), axis=0)\n",
    "    training_data_TL_cep        = pd.DataFrame(third_layer_cep_train)\n",
    "    training_data_TL_cep['New_label'] = third_layer_cep_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    third_layer_cep_test       = pd.concat([ACEP_test,cep_ii_test], axis=0)\n",
    "    third_layer_cep_test_class = np.concatenate((ACEP_test_class,cep_ii_test_class), axis=0)\n",
    "    testing_data_TL_cep        = pd.DataFrame(third_layer_cep_test)\n",
    "    testing_data_TL_cep['New_label'] = third_layer_cep_test_class\n",
    "    \n",
    "    y_TL_cep_training, y_TL_cep_training_counts = np.unique(third_layer_cep_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_TL_cep, testing_data_TL_cep, y_TL_cep_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculates how much each class should be augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_augmentation(nAugmentation, y_training_counts):    \n",
    "    '''\n",
    "    This section calculates the number of augmentation to be carried out for each specific class\n",
    "    Parameters: nAugmentation\n",
    "                Integer values. Specify the total number of samples to generate\n",
    "                \n",
    "                y_training_counts\n",
    "                A list of total number of examples each unique class has. For instance [Type 1: 1 Type 2: 3] has a \n",
    "                list of [3905 2898]\n",
    "                \n",
    "    Returns: number of samples\n",
    "             The number of times each class will be augmented\n",
    "    '''\n",
    "    number_of_samples = []\n",
    "    for i in range(len(y_training_counts)):\n",
    "        floatNsamples   = nAugmentation/y_training_counts[i] # Calculate the number of times the class need to be augmented - in float\n",
    "        nSamples        = Decimal(str(floatNsamples)).quantize(Decimal(\"1\"), rounding=ROUND_HALF_UP) # convert float to integer values\n",
    "        total_augmented = y_training_counts[i]*nSamples\n",
    "        number_of_samples.append(int(nSamples))\n",
    "        print('The number of sample in Class {} is {} and is now augmented by {} times. The augmented samples are {}'.format(i,y_training_counts[i],nSamples,total_augmented))\n",
    "    return number_of_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample period from true period distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling_period(true_class,num_samples,distribution='Normal'):\n",
    "    '''\n",
    "    For each augmented sample, we will assign a period that has been sampled from the true period distribution \n",
    "    of their respective class.\n",
    "    \n",
    "    Parameters: true_class\n",
    "                The class of the variable star. Integer values varies from 0 to 13\n",
    "                \n",
    "                num_samples\n",
    "                The number of period to sample from the distribution. In our case we use num_samples=1 as we \n",
    "                sample 1 period each time\n",
    "                \n",
    "                distribution\n",
    "                1. Normal: The mean and the std of the true period distribution is calculated and we sample one period\n",
    "                           from a normal distribution using this mean and std\n",
    "                           \n",
    "                2.Random: We sample randomly one period from the true period distribution\n",
    "                \n",
    "    Returns: new_period\n",
    "             The new period for the augmented data set\n",
    "    '''\n",
    "    period_data = ascii_data[['Period', 'File_Name', 'Type']]\n",
    "    types       = period_data[period_data.Type==true_class]\n",
    "    \n",
    "    if (distribution=='Normal'):\n",
    "        mu          = np.mean(types.Period)\n",
    "        std         = np.std(types.Period)#/3.0\n",
    "        new_period  = np.abs(np.random.normal(mu, std, num_samples))\n",
    "    \n",
    "    if (distribution=='Random'):\n",
    "        new_period  = np.abs(np.random.choice(types.Period, num_samples))\n",
    "        \n",
    "    return new_period\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction_training(augmented_data,file_name,data_columns,features,true_class,new_label):\n",
    "    '''\n",
    "    Features extraction using FATS for training set (consists both real and augmented samples)\n",
    "    \n",
    "    Parameters: augmented_data\n",
    "                A table with the following format: Time, True flux, Fake flux1, Fake flux2 ..., Fake fluxN\n",
    "                \n",
    "                file_name\n",
    "                The name for each files - to keep track of each variable stars\n",
    "                \n",
    "                data_columns\n",
    "                The information the data has. Either data_columns = [magnitude', 'std magnitude', 'time']\n",
    "                or data_columns = ['magnitude', time']\n",
    "                \n",
    "                features\n",
    "                The list of features to be extracted: \n",
    "                features = ['Skew', 'Mean', 'Std', 'SmallKurtosis', 'Amplitude', 'Meanvariance']\n",
    "                \n",
    "                true_class\n",
    "                The class of the variable stars. Integer values varies from 0 to 10\n",
    "                \n",
    "                new_label\n",
    "                The new label assigned to the aggregated classes. Integer values\n",
    "                \n",
    "    Returns: Feature_file\n",
    "             A table that contains the six features with true period for the real examples and sampling period for \n",
    "             augmented examples. The feature file has these columns\n",
    "             ['Skew','Mean','Std','SmallKurtosis','Amplitude','Meanvariance','Period','File_Name','True_class_label','New_label']\n",
    "    '''\n",
    "    feature_file = pd.DataFrame()\n",
    "        \n",
    "    #data  = pd.read_csv(augmented_data, sep=',', header=None)\n",
    "    data  = augmented_data\n",
    "    time  = data.iloc[:,0].values\n",
    "\n",
    "    \n",
    "    for j in range(1, data.shape[1]):\n",
    "        magnitude       = data.iloc[:,j].values\n",
    "        lc              = np.array([ magnitude, time])\n",
    "        feature_extract = FATS.FeatureSpace(Data=data_columns, featureList = features)\n",
    "        features_cal    = feature_extract.calculateFeature(lc)\n",
    "        features_name   = features_cal.result(method='features')\n",
    "        features_value  = features_cal.result(method='array')\n",
    "        features_df     = pd.DataFrame(features_value.reshape(1,len(features))) \n",
    "\n",
    "        features_df['Period']            = sampling_period(true_class,num_samples=1,distribution='Random')\n",
    "        features_df['File_Name']         =  str(file_name)+'_'+str(j)\n",
    "        features_df['True_class_labels'] =  true_class\n",
    "        features_df['New_label']         =  new_label\n",
    "        feature_file                     = feature_file.append(features_df)\n",
    "\n",
    "    return feature_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def augmentation_and_featureExtraction(data_dir, data_,split_num,data_columns,features,update_ascii_period,\\\n",
    "                                      number_of_samples,save_folder_training = './data/GP/training_set/'):        \n",
    "    '''\n",
    "    Perform data augmentation using Gaussian Process and extract features using some functionality above (for e.g number_of_samples\n",
    "    and feature_extraction_training).This section finds all the filenames for each specific class, starting \n",
    "    from class 0 to class 12. For each specific class, it loads the filename, then perform data augmentation\n",
    "    of this object using the number of times this object needs to be augmented. Augmentation is done using\n",
    "    Gaussian process and fake light curves are randomly sampled within the 3-sigma interval. \n",
    "    \n",
    "    Parameters: data_dir\n",
    "                The directory that contains all the raw files (.dat)\n",
    "                \n",
    "                data_\n",
    "                The training data. Here for training data, we have given some aggregated classes a label where we\n",
    "                have put them under the 'New_label' columns.\n",
    "                \n",
    "                data_columns\n",
    "                The information the data has. Either data_columns = [magnitude', 'std magnitude', 'time']\n",
    "                or data_columns = ['magnitude', time']\n",
    "                \n",
    "                features\n",
    "                The list of features to be extracted: \n",
    "                features = ['Skew', 'Mean', 'Std', 'SmallKurtosis', 'Amplitude', 'Meanvariance']\n",
    "                \n",
    "                update_ascii_period\n",
    "                This data file include only filename that ends with '_1' with its correcponding true period from the \n",
    "                ascii catalog. This means that this data set consists of only real examples of variable stars. We need this\n",
    "                to get the true period.\n",
    "                \n",
    "                number_of_samples\n",
    "                We use the function above 'num_augmentation()' to find the number of times each class will be augmented\n",
    "                \n",
    "                save_folder_training\n",
    "                The directory we save the training features\n",
    "                \n",
    "    Returns: augmented_data\n",
    "             The real and augmented data Time Real_mag Fake_mag1 Fake_mag2... Fake_magN\n",
    "             \n",
    "             feature_file\n",
    "             The save file for features containing both real and augmented. This file will be loaded and use for \n",
    "             classification.\n",
    "    '''\n",
    "\n",
    "    foldername  = \"Split_\"+str(split_num)\n",
    "    if os.path.exists(foldername):\n",
    "        shutil.rmtree(foldername)\n",
    "    os.makedirs(save_folder_training+foldername, 0o755)   \n",
    "        \n",
    "    files_trueClass = np.unique(data_.New_label.values)\n",
    "#     files_trueClass = files_trueClass[0:7] # Need to comment\n",
    "#     print(files_trueClass)\n",
    "\n",
    "    i=0\n",
    "    feature_file = pd.DataFrame()\n",
    "    for files in files_trueClass: \n",
    "        file_name = data_.File_Name[data_.New_label.values == files]\n",
    "        file_name = file_name.values\n",
    "#         file_name = file_name[0:10] # Need to comment\n",
    "#         print(file_name)\n",
    "                \n",
    "        for datafile in file_name:\n",
    "            file           = str(data_dir)+str(datafile)+'.txt'\n",
    "            print(file)\n",
    "            data   = np.loadtxt(file)\n",
    "            time   = data[:,0]\n",
    "            x      = data[:,1]\n",
    "            y      = data[:,2]\n",
    "            yerr   = data[:,3]\n",
    "            augmented_data = pd.DataFrame() \n",
    "            \n",
    "            \n",
    "            kernel = np.var(y) * kernels.Matern52Kernel(metric=0.1, ndim=1)\n",
    "            gp     = george.GP(kernel)\n",
    "            comp   = gp.compute(x, yerr)\n",
    "            covariance_matrix = gp.get_matrix(x)\n",
    "            grad_likelihood   = gp.grad_log_likelihood(y, quiet=False)\n",
    "            likelihood        = gp.log_likelihood(y, quiet=False)\n",
    "\n",
    "            # Define the objective function (negative log-likelihood in this case).\n",
    "            def nll(p):\n",
    "                gp.set_parameter_vector(p)\n",
    "                ll = gp.log_likelihood(y, quiet=True)\n",
    "                return -ll if np.isfinite(ll) else 1e25\n",
    "\n",
    "            # And the gradient of the objective function.\n",
    "            def grad_nll(p):\n",
    "                gp.set_parameter_vector(p)\n",
    "                return -gp.grad_log_likelihood(y, quiet=True)\n",
    "            \n",
    "            # Run the optimization routine.\n",
    "            p0 = gp.get_parameter_vector()\n",
    "            results = op.minimize(nll, p0, jac=grad_nll, method=\"L-BFGS-B\")\n",
    "\n",
    "            # Update the kernel and print the final log-likelihood.\n",
    "            gp.set_parameter_vector(results.x)\n",
    "\n",
    "            #Prediction: Calculate the mean and variance\n",
    "            x_pred         = x# np.linspace(min(x), max(x), 1000)#x # The coordinates where the predictive distribution should be computed.\n",
    "            pred, pred_var = gp.predict(y, x_pred, return_var=True)\n",
    "            samples_lc     = gp.sample_conditional(y, x_pred, number_of_samples[i])   \n",
    "\n",
    "            df         = pd.DataFrame(np.transpose(samples_lc))\n",
    "#             df.insert(loc=0, column='0', value=time)\n",
    "            df.insert(loc=0, column='0', value=x_pred)  \n",
    "            df.insert(loc=1, column='1', value=y)\n",
    "\n",
    "            augmented_data = augmented_data.append(df)\n",
    "            true_labelling = period_data.Type[period_data.File_Name.values == datafile].values\n",
    "            features_df    = feature_extraction_training(augmented_data=augmented_data,file_name=datafile,true_class=true_labelling[0],new_label=files,data_columns=data_columns,features=features)\n",
    "            feature_file   = feature_file.append(features_df)\n",
    "        i += 1\n",
    "    Newfeature_file     = feature_file.join(update_ascii_period.set_index('File_Name'), on='File_Name', lsuffix='_sample', rsuffix='_true')    \n",
    "    newFeature_data     = Newfeature_file\n",
    "\n",
    "    newFeature_data.Period_true.fillna(newFeature_data.Period_sample, inplace=True)\n",
    "    newFeature_data_df = newFeature_data.drop(labels='Period_sample', axis=1)\n",
    "    final_feature_file = newFeature_data_df[[0,1,2,3,4,5,'Period_true', 'File_Name', 'True_class_labels', 'New_label']]\n",
    "    final_feature_file = final_feature_file.rename(columns={'Period_true': 'Period'})\n",
    "    final_feature_file.to_csv(save_folder_training+foldername+'/Training_features.csv',index=None)#'/Type'+str(files)+'_features.csv',index=None)\n",
    "\n",
    "    return augmented_data, feature_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction_test_set(data_dir,X_testing,data_columns,features,split_num,save_folder_test  = './data/GP/test_set/'):\n",
    "    '''\n",
    "    Perform extraction of features for test set.\n",
    "    \n",
    "    Parameters: data_dir\n",
    "                The directory that contains all the raw files (.dat)\n",
    "                \n",
    "                X_testing\n",
    "                The testing data. Here for testing data, we have given some aggregated classes a label where we\n",
    "                have put them under the 'New_label' columns.\n",
    "                \n",
    "                data_columns\n",
    "                The information the data has. Either data_columns = [magnitude', 'std magnitude', 'time']\n",
    "                or data_columns = ['magnitude', time']\n",
    "                \n",
    "                features\n",
    "                The list of features to be extracted: \n",
    "                features = ['Skew', 'Mean', 'Std', 'SmallKurtosis', 'Amplitude', 'Meanvariance']\n",
    "                                \n",
    "                number_of_samples\n",
    "                We use the function above 'num_augmentation()' to find the number of times each class will be augmented\n",
    "                \n",
    "                save_folder_testing\n",
    "                The directory we save the testing features\n",
    "                \n",
    "    Returns: feature_file_testSet\n",
    "             The save file for features containing only real examples for test set. This file will be loaded and use for \n",
    "             classification.\n",
    "    '''\n",
    "#     periods           = ascii_data[['File_Name', 'Period']]\n",
    "    foldername        = \"Split_\"+str(split_num)\n",
    "    if os.path.exists(foldername):\n",
    "        shutil.rmtree(foldername)\n",
    "    os.makedirs(save_folder_test+foldername, 0o755)\n",
    "    nFeatures    = len(features)\n",
    "    feature_file_test = pd.DataFrame()\n",
    "        \n",
    "    files_trueClass_test = np.unique(X_testing.New_label.values)\n",
    "#     files_trueClass_test = files_trueClass_test[0:5] # Need to comment\n",
    "#     print(files_trueClass_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    feature_file_test = pd.DataFrame()\n",
    "    for files_test in files_trueClass_test:\n",
    "        file_name_test    = X_testing.File_Name[X_testing.New_label.values == files_test]\n",
    "        file_name_test = file_name_test.values\n",
    "#         file_name_test = file_name_test[0:3] # Need to comment\n",
    "#         print(file_name_test)\n",
    "        \n",
    "        for datafile_test in file_name_test:\n",
    "            file_test   = str(data_dir)+str(datafile_test)+'.txt'\n",
    "            data_test   = np.loadtxt(file_test)\n",
    "            time_test      = data_test[:,1]\n",
    "            magnitude_test = data_test[:,2]\n",
    "            std_mag_test   = data_test[:,3]\n",
    "            \n",
    "            lc              = np.array([ magnitude_test, time_test])\n",
    "            feature_extract = FATS.FeatureSpace(Data=data_columns, featureList = features)\n",
    "            features_cal    = feature_extract.calculateFeature(lc)\n",
    "            features_name   = features_cal.result(method='features')\n",
    "            features_value  = features_cal.result(method='array')\n",
    "\n",
    "\n",
    "            features_df_test              = pd.DataFrame(features_value.reshape(1,len(features)))            \n",
    "            features_df_test['File_Name'] =  str(datafile_test)   \n",
    "            true_label_test               = period_data.Type[period_data.File_Name.values == datafile_test].values\n",
    "            features_df_test['True_class_labels'] = true_label_test\n",
    "            features_df_test['New_label']         = files_test\n",
    "            feature_file_test                     = feature_file_test.append(features_df_test)          \n",
    "        \n",
    "        feature_file_test['File_Name'] = feature_file_test['File_Name'].astype(int)\n",
    "        feature_file_testSet = feature_file_test.join(periods.set_index('File_Name'), on='File_Name')\n",
    "        feature_file_testSet = feature_file_testSet[[0,1,2,3,4,5,'Period', 'File_Name', 'True_class_labels','New_label']]\n",
    "#        print(feature_file_testSet)\n",
    "        feature_file_testSet.to_csv(save_folder_test+foldername+'/Test_features.csv',index=None)\n",
    "    return feature_file_testSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Classification Pipeline\n",
    "- Split our whole dataset into 5 fold stratified splitting, that is, we have split our dataset into training set and test set. \n",
    "\n",
    "- Using the training set, we augment each layer depending on how much we want to augment the data in such a way that we have balanced classes. Augmentation is carried out by sampling from normal distribution assuming the mean of the gaussian to be the magnitude and the std to be the error on the magnitude, provided in the data.\n",
    "- Using both the augmented data and the real data, we extract 6 features and then use the period from the ascii-catalogue to assign it to their respective class and for augmented data, we randomly sample from the true distribution of the true period. Therefore, we have 7 features that best describe each variable stars.\n",
    "- For the test set, we have use only real example and features are extracted and true period from the ascii file is assigned to the respective class.\n",
    "- The training and testing set features are saved in separate files such that for each iteration (in our case, there are 5 iterations), they are saved inseparate folders.\n",
    "- For the classification, we then load the features, perform a normalisation and classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalisation(x_train,x_test):\n",
    "    scaler                = StandardScaler().fit(x_train.iloc[:,0:nFeatures])\n",
    "    X_train_normalisation = pd.DataFrame(scaler.transform(x_train.iloc[:,0:nFeatures]))\n",
    "    y_train_label         = x_train.New_label\n",
    "    filename_train        = x_train.File_Name\n",
    "\n",
    "    X_test_normalisation = pd.DataFrame(scaler.transform(x_test.iloc[:,0:nFeatures]))\n",
    "    y_test_label         = x_test.New_label\n",
    "    filename_test        = x_test.File_Name\n",
    "    \n",
    "    # A check to see whether the mean of x_train and X_test are ~ 0 with std 1.0\n",
    "#     print(X_train_normalisation.mean(axis=0))\n",
    "#     print(X_train_normalisation.std(axis=0))\n",
    "#     print(X_test_normalisation.mean(axis=0))\n",
    "#     print(X_test_normalisation.std(axis=0))\n",
    "    \n",
    "    return X_train_normalisation, y_train_label, filename_train, X_test_normalisation,\\\n",
    "           y_test_label, filename_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gridsearch(classifer, param_grid, n_iter, cv, X_train,y_train,filename='./results'):\n",
    "    grid  = RandomizedSearchCV(classifer, param_grid, n_iter = n_iter, cv = cv, scoring = \"accuracy\", n_jobs = -1,random_state=1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    opt_parameters = grid.best_params_\n",
    "    params_file = open(filename, 'w')\n",
    "    params_file.write(str(grid.best_params_))\n",
    "    params_file.close()\n",
    "    return opt_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_save(classifier_optimize, X_train, y_train, filename_model, save_model=False):\n",
    "    fit_model      = classifier_optimize.fit(X_train, y_train)\n",
    "    \n",
    "    if save_model:\n",
    "        pickle.dump(fit_model, open(filename_model, 'wb'))\n",
    "        \n",
    "    return fit_model\n",
    "\n",
    "def model_fit(fit_model, filename_model, X_train, y_train, X_test, y_test, classifier_model='Random Forest Classifier',classes=[\"Type 1\" , \"Type 2\"], filename ='./results/',load_model=False):\n",
    "    if load_model:\n",
    "        fit_model      = pickle.load(open(filename_model, 'rb'))\n",
    "    \n",
    "    else:\n",
    "        fit_model = fit_model\n",
    "        \n",
    "    ypred          = fit_model.predict(X_test)\n",
    "    probability    = fit_model.predict_proba(X_test)\n",
    "    accuracy       = accuracy_score(y_test, ypred)\n",
    "    MCC            = matthews_corrcoef(y_test, ypred)\n",
    "    conf_mat       = confusion_matrix(y_test, ypred)\n",
    "    misclassified  = np.where(y_test != ypred)[0]\n",
    "    \n",
    "    name_file = open(filename + \".txt\", 'w')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('******* Testing Phase '+ str(classifier_model) +' for ' + str(classes) + ' *******\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(\"Accuracy: \"                    + \"%f\" % float(accuracy) + '\\n')\n",
    "    name_file.write(\"Mathews Correlation Coef: \"    + \"%f\" % float(MCC)      + '\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('Classification Report\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(classification_report(y_test, ypred, target_names = classes)+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.close()    \n",
    "    \n",
    "    return ypred, accuracy, MCC, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes_types,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Reds):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(9,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=16)\n",
    "    cb=plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    cb.ax.tick_params(labelsize=16)\n",
    "    tick_marks = np.arange(len(classes_types))\n",
    "    plt.xticks(tick_marks, classes_types, rotation=45)\n",
    "    plt.yticks(tick_marks, classes_types)\n",
    "    plt.tick_params(axis='x', labelsize=16)\n",
    "    plt.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if (cm[i, j] < 0.01) or (cm[i,j] >= 0.75)  else \"black\",fontsize=16)\n",
    "\n",
    "    \n",
    "    plt.ylabel('True label',fontsize = 16)\n",
    "    plt.xlabel('Predicted label', fontsize = 16)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(conf_mat, classes_types, classifier_model, plot_title, X_test, y_test, nClasses,cmap=plt.cm.Reds):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plot_confusion_matrix(conf_mat, classes_types, normalize=True, title='Confusion matrix for ' + str(classifier_model) )\n",
    "    plt.savefig(plot_title +'_CM.pdf')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analysis_rf(X_train, y_train, types,save_model=False):\n",
    "    n_estimators      = np.arange(50,1000,100)\n",
    "    max_features      = ['auto', 'sqrt', 'log2']\n",
    "    min_samples_split = np.arange(1,20,1)\n",
    "    param_grid        = dict(n_estimators=n_estimators,max_features=max_features,min_samples_split=min_samples_split)        \n",
    "    opt_parameters_rf = gridsearch(RandomForestClassifier(), param_grid, n_iter = 2, cv = 5, X_train=X_train, y_train=y_train,filename= results_dir + types+'_RF_hyparameters.txt')\n",
    "    fit_model = model_save(RandomForestClassifier(**opt_parameters_rf), X_train=X_train, y_train=y_train, \\\n",
    "                           filename_model= results_dir + types+'_RF_model.sav', save_model=save_model)\n",
    "    return opt_parameters_rf, fit_model\n",
    "\n",
    "def final_prediction(fitModel,X_train, y_train, X_test, y_test, classes, types, nClasses,load_model=False):\n",
    "    ypred, accuracy, MCC, conf_mat = model_fit(fitModel,filename_model= results_dir + types +'_RF_model.sav', X_train=X_train, y_train=y_train, X_test = X_test, y_test=y_test,\\\n",
    "                                                 classifier_model='Random Forest Classifier',classes=classes, filename =results_dir + types+'_RF',load_model=load_model)\n",
    "    \n",
    "    plotting = plot(conf_mat, classes_types=classes, classifier_model='Random Forest Classifier',\\\n",
    "                                  plot_title= plots_dir + types + '_RF', X_test=X_test, y_test=y_test, nClasses=nClasses,cmap=plt.cm.Reds)\n",
    "\n",
    "    return ypred, accuracy, MCC, conf_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analysis_XGB(X_train, y_train, types,save_model=False,multi=True):\n",
    "    eta       = [0.01]    \n",
    "    if multi:\n",
    "        objective = ['multi:softmax']\n",
    "    else: \n",
    "        objective = ['binary:logistic']\n",
    "\n",
    "    max_depth = np.arange(1,12,2)\n",
    "    subsample = [0.5]\n",
    "    param_grid  = dict(eta=eta,objective=objective,max_depth=max_depth,subsample=subsample)\n",
    "        \n",
    "    opt_parameters_XGB = gridsearch(XGBClassifier(), param_grid, n_iter = 5, cv = 5, X_train=X_train, y_train=y_train, filename= results_dir + types+ '_XGB_hyparameters.txt')\n",
    "    fit_model = model_save(XGBClassifier(**opt_parameters_XGB), X_train=X_train, y_train=y_train, \\\n",
    "                           filename_model= results_dir + types + '_XGB_model.sav', save_model=save_model)\n",
    "    return opt_parameters_XGB, fit_model\n",
    "\n",
    "def final_prediction_XGB(fitModel,X_train, y_train, X_test, y_test, classes, types,nClasses,load_model=False):\n",
    "    ypred, accuracy, MCC, conf_mat  = model_fit(fitModel,filename_model= results_dir + types +'_XGB_model.sav', X_train=X_train, y_train=y_train, X_test = X_test, y_test=y_test,\\\n",
    "                                                 classifier_model='XGBoost Classifier',classes=classes, filename =results_dir + types +'_XGB', load_model=load_model)\n",
    "\n",
    "    plotting = plot(conf_mat, classes_types=classes, classifier_model='XGBoost Classifier',\\\n",
    "                                  plot_title= plots_dir + types +'_XGB', X_test=X_test, y_test=y_test, nClasses=nClasses,cmap=plt.cm.Blues)\n",
    "    return ypred, accuracy, MCC, conf_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Part: DATA AUGMENTATION, FEATURE EXTRACTION and CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23143, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the photometry data to get the filenames and their respective classes\n",
    "ascii_data  = pd.read_csv('../Ascii_SSS_Per_Table.txt',delim_whitespace=True,names = [\"SSS_ID\", \"File_Name\", \"RA\", \"Dec\", \"Period\", \"V_CSS\", \"Npts\", \"V_amp\", \"Type\", \"Prior_ID\", \"No_Name1\", 'No_Name2'])\n",
    "ascii_files = ascii_data[['File_Name', 'Type']] # Selecting only the filename and the type of stars\n",
    "\n",
    "'''\n",
    "Peforming a downsammpling of Type 5- We downsample class 5 from 18000 to 4509. And we remove class 11 and 13.\n",
    "'''\n",
    "type_5      = ascii_files[ascii_files.Type==true_class_5][4509::]\n",
    "type_11     = ascii_files[ascii_files.Type==true_class_11]\n",
    "type_13     = ascii_files[ascii_files.Type==true_class_13]\n",
    "ascii_files = ascii_files.drop(type_5.index)\n",
    "ascii_files = ascii_files.drop(type_11.index)\n",
    "ascii_files = ascii_files.drop(type_13.index)\n",
    "ascii_files.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA AUGMENTATION AND FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([    0,     1,     2, ..., 23140, 23141, 23142]), 'TEST:', array([    4,     5,    12, ..., 23125, 23132, 23137]))\n",
      "eclipsing_binary_train has (7214, 2)\n",
      "pulsating_train has (8387, 2)\n",
      "rotational_train has (2908, 2)\n",
      "eclipsing_binary_test has (1804, 2)\n",
      "pulsating_test has (2102, 2)\n",
      "rotational_test has (728, 2)\n",
      "The number of sample in Class 0 is 7214 and is now augmented by 2 times. The augmented samples are 14428\n",
      "The number of sample in Class 1 is 2908 and is now augmented by 5 times. The augmented samples are 14540\n",
      "The number of sample in Class 2 is 8387 and is now augmented by 2 times. The augmented samples are 16774\n",
      "[2, 5, 2]\n",
      "[20 21 22]\n",
      "[3039001008411 3047001026124 3037001022068 3035001009810 3031001017967\n",
      " 3035001017496 3043001020365 3039001005538 3023001017263 3031001013347]\n",
      "./data/phase_lc/3039001008411.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/george/utils.py:30: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  samples = np.random.multivariate_normal(mean, matrix, N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/phase_lc/3047001026124.txt\n",
      "./data/phase_lc/3037001022068.txt\n",
      "./data/phase_lc/3035001009810.txt\n",
      "./data/phase_lc/3031001017967.txt\n",
      "./data/phase_lc/3035001017496.txt\n",
      "./data/phase_lc/3043001020365.txt\n",
      "./data/phase_lc/3039001005538.txt\n",
      "./data/phase_lc/3023001017263.txt\n",
      "./data/phase_lc/3031001013347.txt\n",
      "[3057001012326 3029001005060 3025002005197 3041002022608 3051002017867\n",
      " 3065001010858 3039002016264 3031003018713 3033003008133 3059003010721]\n",
      "./data/phase_lc/3057001012326.txt\n",
      "./data/phase_lc/3029001005060.txt\n",
      "./data/phase_lc/3025002005197.txt\n",
      "./data/phase_lc/3041002022608.txt\n",
      "./data/phase_lc/3051002017867.txt\n",
      "./data/phase_lc/3065001010858.txt\n",
      "./data/phase_lc/3039002016264.txt\n",
      "./data/phase_lc/3031003018713.txt\n",
      "./data/phase_lc/3033003008133.txt\n",
      "./data/phase_lc/3059003010721.txt\n",
      "[3041001010523 3025001011039 3025001012723 3025001014168 3059001016897\n",
      " 3063001012862 3035001006590 3063001015629 3065001030354 3021001012412]\n",
      "./data/phase_lc/3041001010523.txt\n",
      "./data/phase_lc/3025001011039.txt\n",
      "./data/phase_lc/3025001012723.txt\n",
      "./data/phase_lc/3025001014168.txt\n",
      "./data/phase_lc/3059001016897.txt\n",
      "./data/phase_lc/3063001012862.txt\n",
      "./data/phase_lc/3035001006590.txt\n",
      "./data/phase_lc/3063001015629.txt\n",
      "./data/phase_lc/3065001030354.txt\n",
      "./data/phase_lc/3021001012412.txt\n",
      "[20 21 22]\n",
      "[3033001008037 3033001020380 3039001006111]\n",
      "[3029003013827 3033005013071 3039005009686]\n",
      "[3029001015115 3025001019803 3043001007206]\n",
      "ecl train has (3607, 2)\n",
      "EA_train has (3607, 2)\n",
      "ecl_test has (902, 2)\n",
      "EA_test has (902, 2)\n",
      "The number of sample in Class 0 is 3607 and is now augmented by 2 times. The augmented samples are 7214\n",
      "The number of sample in Class 1 is 3607 and is now augmented by 2 times. The augmented samples are 7214\n",
      "[2, 2]\n",
      "[5 6]\n",
      "[3039001008411 3047001026124 3037001022068 3035001009810 3031001017967\n",
      " 3035001017496 3043001020365 3039001005538 3023001017263 3031001013347]\n",
      "./data/phase_lc/3039001008411.txt\n",
      "./data/phase_lc/3047001026124.txt\n",
      "./data/phase_lc/3037001022068.txt\n",
      "./data/phase_lc/3035001009810.txt\n",
      "./data/phase_lc/3031001017967.txt\n",
      "./data/phase_lc/3035001017496.txt\n",
      "./data/phase_lc/3043001020365.txt\n",
      "./data/phase_lc/3039001005538.txt\n",
      "./data/phase_lc/3023001017263.txt\n",
      "./data/phase_lc/3031001013347.txt\n",
      "[3027001016241 3041001017008 3037001023690 3043001017345 3035002022562\n",
      " 3037002016175 3039002021528 3049002005470 3049002016589 3027003007069]\n",
      "./data/phase_lc/3027001016241.txt\n",
      "./data/phase_lc/3041001017008.txt\n",
      "./data/phase_lc/3037001023690.txt\n",
      "./data/phase_lc/3043001017345.txt\n",
      "./data/phase_lc/3035002022562.txt\n",
      "./data/phase_lc/3037002016175.txt\n",
      "./data/phase_lc/3039002021528.txt\n",
      "./data/phase_lc/3049002005470.txt\n",
      "./data/phase_lc/3049002016589.txt\n",
      "./data/phase_lc/3027003007069.txt\n",
      "[5 6]\n",
      "[3033001008037 3033001020380 3039001006111]\n",
      "[3023001018280 3037002017551 3041002003991]\n",
      "RR Lyrae train has (6998, 2)\n",
      "LPV train has (1028, 2)\n",
      "Cepheids train has (244, 2)\n",
      "Delta Scuti train has (117, 2)\n",
      "RR_Lyrae_test has (1752, 2)\n",
      "LPV_test has (258, 2)\n",
      "cepheids_test has (62, 2)\n",
      "Delta Scuti test has (30, 2)\n",
      "[23 24 25 26]\n",
      "[6998 1028  117  244]\n",
      "The number of sample in Class 0 is 6998 and is now augmented by 2 times. The augmented samples are 13996\n",
      "The number of sample in Class 1 is 1028 and is now augmented by 16 times. The augmented samples are 16448\n",
      "The number of sample in Class 2 is 117 and is now augmented by 145 times. The augmented samples are 16965\n",
      "The number of sample in Class 3 is 244 and is now augmented by 69 times. The augmented samples are 16836\n",
      "[2, 16, 145, 69]\n",
      "[23 24 25 26]\n",
      "[3041001010523 3025001011039 3025001012723 3025001014168 3059001016897\n",
      " 3063001012862 3035001006590 3063001015629 3065001030354 3021001012412]\n",
      "./data/phase_lc/3041001010523.txt\n",
      "./data/phase_lc/3025001011039.txt\n",
      "./data/phase_lc/3025001012723.txt\n",
      "./data/phase_lc/3025001014168.txt\n",
      "./data/phase_lc/3059001016897.txt\n",
      "./data/phase_lc/3063001012862.txt\n",
      "./data/phase_lc/3035001006590.txt\n",
      "./data/phase_lc/3063001015629.txt\n",
      "./data/phase_lc/3065001030354.txt\n",
      "./data/phase_lc/3021001012412.txt\n",
      "[3023002016726 3043002004587 3033007012816 3033007004821 3065004018724\n",
      " 3041014023399 3033017019433 3035017021291 3033017001367 3035017033877]\n",
      "./data/phase_lc/3023002016726.txt\n",
      "./data/phase_lc/3043002004587.txt\n",
      "./data/phase_lc/3033007012816.txt\n",
      "./data/phase_lc/3033007004821.txt\n",
      "./data/phase_lc/3065004018724.txt\n",
      "./data/phase_lc/3041014023399.txt\n",
      "./data/phase_lc/3033017019433.txt\n",
      "./data/phase_lc/3035017021291.txt\n",
      "./data/phase_lc/3033017001367.txt\n",
      "./data/phase_lc/3035017033877.txt\n",
      "[3029009009865 3041013019377 3053012007582 3053014006946 3027023013859\n",
      " 3045027006853 3055022007791 3031035026267 3031039022708 3031039018082]\n",
      "./data/phase_lc/3029009009865.txt\n",
      "./data/phase_lc/3041013019377.txt\n",
      "./data/phase_lc/3053012007582.txt\n",
      "./data/phase_lc/3053014006946.txt\n",
      "./data/phase_lc/3027023013859.txt\n",
      "./data/phase_lc/3045027006853.txt\n",
      "./data/phase_lc/3055022007791.txt\n",
      "./data/phase_lc/3031035026267.txt\n",
      "./data/phase_lc/3031039022708.txt\n",
      "./data/phase_lc/3031039018082.txt\n",
      "[3043003001790 3033007011579 3033007006272 3069005028014 3033012018924\n",
      " 3023014002004 3057009011334 3027026012731 3023036023403 3029039024714]\n",
      "./data/phase_lc/3043003001790.txt\n",
      "./data/phase_lc/3033007011579.txt\n",
      "./data/phase_lc/3033007006272.txt\n",
      "./data/phase_lc/3069005028014.txt\n",
      "./data/phase_lc/3033012018924.txt\n",
      "./data/phase_lc/3023014002004.txt\n",
      "./data/phase_lc/3057009011334.txt\n",
      "./data/phase_lc/3027026012731.txt\n",
      "./data/phase_lc/3023036023403.txt\n",
      "./data/phase_lc/3029039024714.txt\n",
      "[23 24 25 26]\n",
      "[3029001015115 3025001019803 3043001007206]\n",
      "[3035017021821 3035017023361 3033017002537]\n",
      "[3061023005687 3053029008825 3027042046559]\n",
      "[3039011003666 3063013006226 3063019100837]\n",
      "RRab train has (3460, 2)\n",
      "RRc train has (3001, 2)\n",
      "RRd train has (401, 2)\n",
      "Blazhko train has (136, 2)\n",
      "RRab test has (865, 2)\n",
      "RRc test has (751, 2)\n",
      "RRd test has (101, 2)\n",
      "Blazhko test has (35, 2)\n",
      "The number of sample in Class 0 is 3460 and is now augmented by 2 times. The augmented samples are 6920\n",
      "The number of sample in Class 1 is 3001 and is now augmented by 3 times. The augmented samples are 9003\n",
      "The number of sample in Class 2 is 401 and is now augmented by 24 times. The augmented samples are 9624\n",
      "The number of sample in Class 3 is 136 and is now augmented by 73 times. The augmented samples are 9928\n",
      "[2, 3, 24, 73]\n",
      "[1 2 3 4]\n",
      "[3041001010523 3025001011039 3025001012723 3025001014168 3059001016897\n",
      " 3063001012862 3035001006590 3063001015629 3065001030354 3021001012412]\n",
      "./data/phase_lc/3041001010523.txt\n",
      "./data/phase_lc/3025001011039.txt\n",
      "./data/phase_lc/3025001012723.txt\n",
      "./data/phase_lc/3025001014168.txt\n",
      "./data/phase_lc/3059001016897.txt\n",
      "./data/phase_lc/3063001012862.txt\n",
      "./data/phase_lc/3035001006590.txt\n",
      "./data/phase_lc/3063001015629.txt\n",
      "./data/phase_lc/3065001030354.txt\n",
      "./data/phase_lc/3021001012412.txt\n",
      "[3043001016067 3023001007314 3031001016447 3037001007717 3027001005640\n",
      " 3063001015076 3041001008329 3053001003881 3057001008573 3025002006386]\n",
      "./data/phase_lc/3043001016067.txt\n",
      "./data/phase_lc/3023001007314.txt\n",
      "./data/phase_lc/3031001016447.txt\n",
      "./data/phase_lc/3037001007717.txt\n",
      "./data/phase_lc/3027001005640.txt\n",
      "./data/phase_lc/3063001015076.txt\n",
      "./data/phase_lc/3041001008329.txt\n",
      "./data/phase_lc/3053001003881.txt\n",
      "./data/phase_lc/3057001008573.txt\n",
      "./data/phase_lc/3025002006386.txt\n",
      "[3029002016482 3057002021108 3027004010129 3061003016275 3039006008644\n",
      " 3039007017994 3021009013686 3047007014167 3031009008983 3049007004217]\n",
      "./data/phase_lc/3029002016482.txt\n",
      "./data/phase_lc/3057002021108.txt\n",
      "./data/phase_lc/3027004010129.txt\n",
      "./data/phase_lc/3061003016275.txt\n",
      "./data/phase_lc/3039006008644.txt\n",
      "./data/phase_lc/3039007017994.txt\n",
      "./data/phase_lc/3021009013686.txt\n",
      "./data/phase_lc/3047007014167.txt\n",
      "./data/phase_lc/3031009008983.txt\n",
      "./data/phase_lc/3049007004217.txt\n",
      "[3059001014294 3043004012966 3031006006054 3049008006074 3047009004897\n",
      " 3053016010811 3029029004674 3061017021825 3051023002638 3047027025878]\n",
      "./data/phase_lc/3059001014294.txt\n",
      "./data/phase_lc/3043004012966.txt\n",
      "./data/phase_lc/3031006006054.txt\n",
      "./data/phase_lc/3049008006074.txt\n",
      "./data/phase_lc/3047009004897.txt\n",
      "./data/phase_lc/3053016010811.txt\n",
      "./data/phase_lc/3029029004674.txt\n",
      "./data/phase_lc/3061017021825.txt\n",
      "./data/phase_lc/3051023002638.txt\n",
      "./data/phase_lc/3047027025878.txt\n",
      "[1 2 3 4]\n",
      "[3029001015115 3025001019803 3043001007206]\n",
      "[3063001013464 3021003003795 3031003014908]\n",
      "[3025002019709 3037003016926 3027013015834]\n",
      "[3035016002621 3041017007885 3047019008810]\n",
      "ACEP train has (122, 2)\n",
      "Cep-II train has (122, 2)\n",
      "ACEP test has (31, 2)\n",
      "Cep-II test has (31, 2)\n",
      "The number of sample in Class 0 is 122 and is now augmented by 40 times. The augmented samples are 4880\n",
      "The number of sample in Class 1 is 122 and is now augmented by 40 times. The augmented samples are 4880\n",
      "[40, 40]\n",
      "[10 12]\n",
      "[3043003001790 3033007011579 3033007006272 3069005028014 3033012018924\n",
      " 3023014002004 3057009011334 3027026012731 3023036023403 3029039024714]\n",
      "./data/phase_lc/3043003001790.txt\n",
      "./data/phase_lc/3033007011579.txt\n",
      "./data/phase_lc/3033007006272.txt\n",
      "./data/phase_lc/3069005028014.txt\n",
      "./data/phase_lc/3033012018924.txt\n",
      "./data/phase_lc/3023014002004.txt\n",
      "./data/phase_lc/3057009011334.txt\n",
      "./data/phase_lc/3027026012731.txt\n",
      "./data/phase_lc/3023036023403.txt\n",
      "./data/phase_lc/3029039024714.txt\n",
      "[3053004015219 3031007013721 3059004009102 3035017018795 3063015005948\n",
      " 3027030003005 3061020025779 3065020091392 3021044048741 3035039028451]\n",
      "./data/phase_lc/3053004015219.txt\n",
      "./data/phase_lc/3031007013721.txt\n",
      "./data/phase_lc/3059004009102.txt\n",
      "./data/phase_lc/3035017018795.txt\n",
      "./data/phase_lc/3063015005948.txt\n",
      "./data/phase_lc/3027030003005.txt\n",
      "./data/phase_lc/3061020025779.txt\n",
      "./data/phase_lc/3065020091392.txt\n",
      "./data/phase_lc/3021044048741.txt\n",
      "./data/phase_lc/3035039028451.txt\n",
      "[10 12]\n",
      "[3039011003666 3063013006226 3063019100837]\n",
      "[3031025007790 3035029003470 3063020061457]\n",
      "--------------------------------------------------\n",
      "Feature Extraction for Split 1 is finished\n",
      "('TRAIN:', array([    1,     2,     3, ..., 23135, 23137, 23138]), 'TEST:', array([    0,     6,    10, ..., 23140, 23141, 23142]))\n",
      "eclipsing_binary_train has (7214, 2)\n",
      "pulsating_train has (8389, 2)\n",
      "rotational_train has (2909, 2)\n",
      "eclipsing_binary_test has (1804, 2)\n",
      "pulsating_test has (2100, 2)\n",
      "rotational_test has (727, 2)\n",
      "The number of sample in Class 0 is 7214 and is now augmented by 2 times. The augmented samples are 14428\n",
      "The number of sample in Class 1 is 2909 and is now augmented by 5 times. The augmented samples are 14545\n",
      "The number of sample in Class 2 is 8389 and is now augmented by 2 times. The augmented samples are 16778\n",
      "[2, 5, 2]\n",
      "[20 21 22]\n",
      "[3047001026124 3037001022068 3033001008037 3031001017967 3039001005538\n",
      " 3031001013347 3053001009124 3043001006551 3063001006018 3035001009157]\n",
      "./data/phase_lc/3047001026124.txt\n",
      "./data/phase_lc/3037001022068.txt\n",
      "./data/phase_lc/3033001008037.txt\n",
      "./data/phase_lc/3031001017967.txt\n",
      "./data/phase_lc/3039001005538.txt\n",
      "./data/phase_lc/3031001013347.txt\n",
      "./data/phase_lc/3053001009124.txt\n",
      "./data/phase_lc/3043001006551.txt\n",
      "./data/phase_lc/3063001006018.txt\n",
      "./data/phase_lc/3035001009157.txt\n",
      "[3057001012326 3029001005060 3025002005197 3051002017867 3065001010858\n",
      " 3031003018713 3029003013827 3033003008133 3059003010721 3033005013071]\n",
      "./data/phase_lc/3057001012326.txt\n",
      "./data/phase_lc/3029001005060.txt\n",
      "./data/phase_lc/3025002005197.txt\n",
      "./data/phase_lc/3051002017867.txt\n",
      "./data/phase_lc/3065001010858.txt\n",
      "./data/phase_lc/3031003018713.txt\n",
      "./data/phase_lc/3029003013827.txt\n",
      "./data/phase_lc/3033003008133.txt\n",
      "./data/phase_lc/3059003010721.txt\n",
      "./data/phase_lc/3033005013071.txt\n",
      "[3041001010523 3025001011039 3029001015115 3025001012723 3025001014168\n",
      " 3059001016897 3063001012862 3025001019803 3043001007206 3035001006590]\n",
      "./data/phase_lc/3041001010523.txt\n",
      "./data/phase_lc/3025001011039.txt\n",
      "./data/phase_lc/3029001015115.txt\n",
      "./data/phase_lc/3025001012723.txt\n",
      "./data/phase_lc/3025001014168.txt\n",
      "./data/phase_lc/3059001016897.txt\n",
      "./data/phase_lc/3063001012862.txt\n",
      "./data/phase_lc/3025001019803.txt\n",
      "./data/phase_lc/3043001007206.txt\n",
      "./data/phase_lc/3035001006590.txt\n",
      "[20 21 22]\n",
      "[3039001008411 3035001009810 3035001017496]\n",
      "[3041002022608 3039002016264 3033005006377]\n",
      "[3021001008691 3025002012208 3025002007364]\n",
      "ecl train has (3607, 2)\n",
      "EA_train has (3607, 2)\n",
      "ecl_test has (902, 2)\n",
      "EA_test has (902, 2)\n",
      "The number of sample in Class 0 is 3607 and is now augmented by 2 times. The augmented samples are 7214\n",
      "The number of sample in Class 1 is 3607 and is now augmented by 2 times. The augmented samples are 7214\n",
      "[2, 2]\n",
      "[5 6]\n",
      "[3047001026124 3037001022068 3033001008037 3031001017967 3039001005538\n",
      " 3031001013347 3053001009124 3043001006551 3063001006018 3035001009157]\n",
      "./data/phase_lc/3047001026124.txt\n",
      "./data/phase_lc/3037001022068.txt\n",
      "./data/phase_lc/3033001008037.txt\n",
      "./data/phase_lc/3031001017967.txt\n",
      "./data/phase_lc/3039001005538.txt\n",
      "./data/phase_lc/3031001013347.txt\n",
      "./data/phase_lc/3053001009124.txt\n",
      "./data/phase_lc/3043001006551.txt\n",
      "./data/phase_lc/3063001006018.txt\n",
      "./data/phase_lc/3035001009157.txt\n",
      "[3023001018280 3041001017008 3043001017345 3035002022562 3037002017551\n",
      " 3037002016175 3039002021528 3041002003991 3049002016589 3045002020508]\n",
      "./data/phase_lc/3023001018280.txt\n",
      "./data/phase_lc/3041001017008.txt\n",
      "./data/phase_lc/3043001017345.txt\n",
      "./data/phase_lc/3035002022562.txt\n",
      "./data/phase_lc/3037002017551.txt\n",
      "./data/phase_lc/3037002016175.txt\n",
      "./data/phase_lc/3039002021528.txt\n",
      "./data/phase_lc/3041002003991.txt\n",
      "./data/phase_lc/3049002016589.txt\n",
      "./data/phase_lc/3045002020508.txt\n",
      "[5 6]\n",
      "[3039001008411 3035001009810 3035001017496]\n",
      "[3027001016241 3037001023690 3049002005470]\n",
      "RR Lyrae train has (6999, 2)\n",
      "LPV train has (1029, 2)\n",
      "Cepheids train has (244, 2)\n",
      "Delta Scuti train has (117, 2)\n",
      "RR_Lyrae_test has (1751, 2)\n",
      "LPV_test has (257, 2)\n",
      "cepheids_test has (62, 2)\n",
      "Delta Scuti test has (30, 2)\n",
      "[23 24 25 26]\n",
      "[6999 1029  117  244]\n",
      "The number of sample in Class 0 is 6999 and is now augmented by 2 times. The augmented samples are 13998\n",
      "The number of sample in Class 1 is 1029 and is now augmented by 16 times. The augmented samples are 16464\n",
      "The number of sample in Class 2 is 117 and is now augmented by 145 times. The augmented samples are 16965\n",
      "The number of sample in Class 3 is 244 and is now augmented by 69 times. The augmented samples are 16836\n",
      "[2, 16, 145, 69]\n",
      "[23 24 25 26]\n",
      "[3041001010523 3025001011039 3029001015115 3025001012723 3025001014168\n",
      " 3059001016897 3063001012862 3025001019803 3043001007206 3035001006590]\n",
      "./data/phase_lc/3041001010523.txt\n",
      "./data/phase_lc/3025001011039.txt\n",
      "./data/phase_lc/3029001015115.txt\n",
      "./data/phase_lc/3025001012723.txt\n",
      "./data/phase_lc/3025001014168.txt\n",
      "./data/phase_lc/3059001016897.txt\n",
      "./data/phase_lc/3063001012862.txt\n",
      "./data/phase_lc/3025001019803.txt\n",
      "./data/phase_lc/3043001007206.txt\n",
      "./data/phase_lc/3035001006590.txt\n",
      "[3023002016726 3033007012816 3033007004821 3065004018724 3041014023399\n",
      " 3033017019433 3035017021821 3035017021291 3033017001367 3035017033877]\n",
      "./data/phase_lc/3023002016726.txt\n",
      "./data/phase_lc/3033007012816.txt\n",
      "./data/phase_lc/3033007004821.txt\n",
      "./data/phase_lc/3065004018724.txt\n",
      "./data/phase_lc/3041014023399.txt\n",
      "./data/phase_lc/3033017019433.txt\n",
      "./data/phase_lc/3035017021821.txt\n",
      "./data/phase_lc/3035017021291.txt\n",
      "./data/phase_lc/3033017001367.txt\n",
      "./data/phase_lc/3035017033877.txt\n",
      "[3029009009865 3041013019377 3053012007582 3053014006946 3027023013859\n",
      " 3045027006853 3055022007791 3031035026267 3031039022708 3031039018082]\n",
      "./data/phase_lc/3029009009865.txt\n",
      "./data/phase_lc/3041013019377.txt\n",
      "./data/phase_lc/3053012007582.txt\n",
      "./data/phase_lc/3053014006946.txt\n",
      "./data/phase_lc/3027023013859.txt\n",
      "./data/phase_lc/3045027006853.txt\n",
      "./data/phase_lc/3055022007791.txt\n",
      "./data/phase_lc/3031035026267.txt\n",
      "./data/phase_lc/3031039022708.txt\n",
      "./data/phase_lc/3031039018082.txt\n",
      "[3043003001790 3033007011579 3033007006272 3033012018924 3039011003666\n",
      " 3057009011334 3063013006226 3027026012731 3023036023403 3063019100837]\n",
      "./data/phase_lc/3043003001790.txt\n",
      "./data/phase_lc/3033007011579.txt\n",
      "./data/phase_lc/3033007006272.txt\n",
      "./data/phase_lc/3033012018924.txt\n",
      "./data/phase_lc/3039011003666.txt\n",
      "./data/phase_lc/3057009011334.txt\n",
      "./data/phase_lc/3063013006226.txt\n",
      "./data/phase_lc/3027026012731.txt\n",
      "./data/phase_lc/3023036023403.txt\n",
      "./data/phase_lc/3063019100837.txt\n",
      "[23 24 25 26]\n",
      "[3021001008691 3025002012208 3025002007364]\n",
      "[3043002004587 3035017023797 3065014005840]\n",
      "[3027042040337 3023046071881 3059029019271]\n",
      "[3069005028014 3023014002004 3051032037255]\n",
      "RRab train has (3460, 2)\n",
      "RRc train has (3001, 2)\n",
      "RRd train has (401, 2)\n",
      "Blazhko train has (137, 2)\n",
      "RRab test has (865, 2)\n",
      "RRc test has (751, 2)\n",
      "RRd test has (101, 2)\n",
      "Blazhko test has (34, 2)\n",
      "The number of sample in Class 0 is 3460 and is now augmented by 2 times. The augmented samples are 6920\n",
      "The number of sample in Class 1 is 3001 and is now augmented by 3 times. The augmented samples are 9003\n",
      "The number of sample in Class 2 is 401 and is now augmented by 24 times. The augmented samples are 9624\n",
      "The number of sample in Class 3 is 137 and is now augmented by 72 times. The augmented samples are 9864\n",
      "[2, 3, 24, 72]\n",
      "[1 2 3 4]\n",
      "[3041001010523 3025001011039 3029001015115 3025001012723 3025001014168\n",
      " 3059001016897 3063001012862 3025001019803 3043001007206 3035001006590]\n",
      "./data/phase_lc/3041001010523.txt\n",
      "./data/phase_lc/3025001011039.txt\n",
      "./data/phase_lc/3029001015115.txt\n",
      "./data/phase_lc/3025001012723.txt\n",
      "./data/phase_lc/3025001014168.txt\n",
      "./data/phase_lc/3059001016897.txt\n",
      "./data/phase_lc/3063001012862.txt\n",
      "./data/phase_lc/3025001019803.txt\n",
      "./data/phase_lc/3043001007206.txt\n",
      "./data/phase_lc/3035001006590.txt\n",
      "[3043001016067 3023001007314 3037001007717 3063001015076 3041001008329\n",
      " 3053001003881 3063001013464 3057001008573 3021002019821 3063001021212]\n",
      "./data/phase_lc/3043001016067.txt\n",
      "./data/phase_lc/3023001007314.txt\n",
      "./data/phase_lc/3037001007717.txt\n",
      "./data/phase_lc/3063001015076.txt\n",
      "./data/phase_lc/3041001008329.txt\n",
      "./data/phase_lc/3053001003881.txt\n",
      "./data/phase_lc/3063001013464.txt\n",
      "./data/phase_lc/3057001008573.txt\n",
      "./data/phase_lc/3021002019821.txt\n",
      "./data/phase_lc/3063001021212.txt\n",
      "[3025002019709 3029002016482 3057002021108 3037003016926 3027004010129\n",
      " 3061003016275 3039006008644 3039007017994 3047007014167 3049007004217]\n",
      "./data/phase_lc/3025002019709.txt\n",
      "./data/phase_lc/3029002016482.txt\n",
      "./data/phase_lc/3057002021108.txt\n",
      "./data/phase_lc/3037003016926.txt\n",
      "./data/phase_lc/3027004010129.txt\n",
      "./data/phase_lc/3061003016275.txt\n",
      "./data/phase_lc/3039006008644.txt\n",
      "./data/phase_lc/3039007017994.txt\n",
      "./data/phase_lc/3047007014167.txt\n",
      "./data/phase_lc/3049007004217.txt\n",
      "[3059001014294 3043004012966 3031006006054 3049008006074 3047009004897\n",
      " 3035016002621 3041017007885 3053016010811 3047019008810 3051018016560]\n",
      "./data/phase_lc/3059001014294.txt\n",
      "./data/phase_lc/3043004012966.txt\n",
      "./data/phase_lc/3031006006054.txt\n",
      "./data/phase_lc/3049008006074.txt\n",
      "./data/phase_lc/3047009004897.txt\n",
      "./data/phase_lc/3035016002621.txt\n",
      "./data/phase_lc/3041017007885.txt\n",
      "./data/phase_lc/3053016010811.txt\n",
      "./data/phase_lc/3047019008810.txt\n",
      "./data/phase_lc/3051018016560.txt\n",
      "[1 2 3 4]\n",
      "[3021001008691 3025002012208 3025002007364]\n",
      "[3031001016447 3027001005640 3025002006386]\n",
      "[3021009013686 3031009008983 3051007008434]\n",
      "[3029029004674 3061017021825 3027037024630]\n",
      "ACEP train has (122, 2)\n",
      "Cep-II train has (122, 2)\n",
      "ACEP test has (31, 2)\n",
      "Cep-II test has (31, 2)\n",
      "The number of sample in Class 0 is 122 and is now augmented by 40 times. The augmented samples are 4880\n",
      "The number of sample in Class 1 is 122 and is now augmented by 40 times. The augmented samples are 4880\n",
      "[40, 40]\n",
      "[10 12]\n",
      "[3043003001790 3033007011579 3033007006272 3033012018924 3039011003666\n",
      " 3057009011334 3063013006226 3027026012731 3023036023403 3063019100837]\n",
      "./data/phase_lc/3043003001790.txt\n",
      "./data/phase_lc/3033007011579.txt\n",
      "./data/phase_lc/3033007006272.txt\n",
      "./data/phase_lc/3033012018924.txt\n",
      "./data/phase_lc/3039011003666.txt\n",
      "./data/phase_lc/3057009011334.txt\n",
      "./data/phase_lc/3063013006226.txt\n",
      "./data/phase_lc/3027026012731.txt\n",
      "./data/phase_lc/3023036023403.txt\n",
      "./data/phase_lc/3063019100837.txt\n",
      "[3053004015219 3031007013721 3059004009102 3031025007790 3063015005948\n",
      " 3027030003005 3035029003470 3061020025779 3063020061457 3055026014665]\n",
      "./data/phase_lc/3053004015219.txt\n",
      "./data/phase_lc/3031007013721.txt\n",
      "./data/phase_lc/3059004009102.txt\n",
      "./data/phase_lc/3031025007790.txt\n",
      "./data/phase_lc/3063015005948.txt\n",
      "./data/phase_lc/3027030003005.txt\n",
      "./data/phase_lc/3035029003470.txt\n",
      "./data/phase_lc/3061020025779.txt\n",
      "./data/phase_lc/3063020061457.txt\n",
      "./data/phase_lc/3055026014665.txt\n",
      "[10 12]\n",
      "[3069005028014 3023014002004 3051032037255]\n",
      "[3035017018795 3057030024254 3035046104843]\n",
      "--------------------------------------------------\n",
      "Feature Extraction for Split 2 is finished\n",
      "('TRAIN:', array([    0,     2,     3, ..., 23140, 23141, 23142]), 'TEST:', array([    1,     7,    13, ..., 23127, 23128, 23135]))\n",
      "eclipsing_binary_train has (7214, 2)\n",
      "pulsating_train has (8392, 2)\n",
      "rotational_train has (2909, 2)\n",
      "eclipsing_binary_test has (1804, 2)\n",
      "pulsating_test has (2097, 2)\n",
      "rotational_test has (727, 2)\n",
      "The number of sample in Class 0 is 7214 and is now augmented by 2 times. The augmented samples are 14428\n",
      "The number of sample in Class 1 is 2909 and is now augmented by 5 times. The augmented samples are 14545\n",
      "The number of sample in Class 2 is 8392 and is now augmented by 2 times. The augmented samples are 16784\n",
      "[2, 5, 2]\n",
      "[20 21 22]\n",
      "[3039001008411 3037001022068 3035001009810 3033001008037 3035001017496\n",
      " 3043001020365 3039001005538 3023001017263 3033001022932 3053001009855]\n",
      "./data/phase_lc/3039001008411.txt\n",
      "./data/phase_lc/3037001022068.txt\n",
      "./data/phase_lc/3035001009810.txt\n",
      "./data/phase_lc/3033001008037.txt\n",
      "./data/phase_lc/3035001017496.txt\n",
      "./data/phase_lc/3043001020365.txt\n",
      "./data/phase_lc/3039001005538.txt\n",
      "./data/phase_lc/3023001017263.txt\n",
      "./data/phase_lc/3033001022932.txt\n",
      "./data/phase_lc/3053001009855.txt\n",
      "[3025002005197 3041002022608 3065001010858 3039002016264 3029003013827\n",
      " 3033003008133 3059003010721 3033005006377 3033005013071 3021005005521]\n",
      "./data/phase_lc/3025002005197.txt\n",
      "./data/phase_lc/3041002022608.txt\n",
      "./data/phase_lc/3065001010858.txt\n",
      "./data/phase_lc/3039002016264.txt\n",
      "./data/phase_lc/3029003013827.txt\n",
      "./data/phase_lc/3033003008133.txt\n",
      "./data/phase_lc/3059003010721.txt\n",
      "./data/phase_lc/3033005006377.txt\n",
      "./data/phase_lc/3033005013071.txt\n",
      "./data/phase_lc/3021005005521.txt\n",
      "[3025001011039 3029001015115 3025001012723 3025001014168 3059001016897\n",
      " 3063001012862 3025001019803 3043001007206 3035001006590 3025001011286]\n",
      "./data/phase_lc/3025001011039.txt\n",
      "./data/phase_lc/3029001015115.txt\n",
      "./data/phase_lc/3025001012723.txt\n",
      "./data/phase_lc/3025001014168.txt\n",
      "./data/phase_lc/3059001016897.txt\n",
      "./data/phase_lc/3063001012862.txt\n",
      "./data/phase_lc/3025001019803.txt\n",
      "./data/phase_lc/3043001007206.txt\n",
      "./data/phase_lc/3035001006590.txt\n",
      "./data/phase_lc/3025001011286.txt\n",
      "[20 21 22]\n",
      "[3047001026124 3031001017967 3031001013347]\n",
      "[3057001012326 3029001005060 3051002017867]\n",
      "[3041001010523 3023002003478 3051002006292]\n",
      "ecl train has (3607, 2)\n",
      "EA_train has (3607, 2)\n",
      "ecl_test has (902, 2)\n",
      "EA_test has (902, 2)\n",
      "The number of sample in Class 0 is 3607 and is now augmented by 2 times. The augmented samples are 7214\n",
      "The number of sample in Class 1 is 3607 and is now augmented by 2 times. The augmented samples are 7214\n",
      "[2, 2]\n",
      "[5 6]\n",
      "[3039001008411 3037001022068 3035001009810 3033001008037 3035001017496\n",
      " 3043001020365 3039001005538 3023001017263 3033001022932 3053001009855]\n",
      "./data/phase_lc/3039001008411.txt\n",
      "./data/phase_lc/3037001022068.txt\n",
      "./data/phase_lc/3035001009810.txt\n",
      "./data/phase_lc/3033001008037.txt\n",
      "./data/phase_lc/3035001017496.txt\n",
      "./data/phase_lc/3043001020365.txt\n",
      "./data/phase_lc/3039001005538.txt\n",
      "./data/phase_lc/3023001017263.txt\n",
      "./data/phase_lc/3033001022932.txt\n",
      "./data/phase_lc/3053001009855.txt\n",
      "[3023001018280 3027001016241 3041001017008 3037001023690 3035002022562\n",
      " 3037002017551 3039002021528 3041002003991 3049002005470 3049002016589]\n",
      "./data/phase_lc/3023001018280.txt\n",
      "./data/phase_lc/3027001016241.txt\n",
      "./data/phase_lc/3041001017008.txt\n",
      "./data/phase_lc/3037001023690.txt\n",
      "./data/phase_lc/3035002022562.txt\n",
      "./data/phase_lc/3037002017551.txt\n",
      "./data/phase_lc/3039002021528.txt\n",
      "./data/phase_lc/3041002003991.txt\n",
      "./data/phase_lc/3049002005470.txt\n",
      "./data/phase_lc/3049002016589.txt\n",
      "[5 6]\n",
      "[3047001026124 3031001017967 3031001013347]\n",
      "[3043001017345 3037002016175 3045002020508]\n",
      "RR Lyrae train has (7001, 2)\n",
      "LPV train has (1029, 2)\n",
      "Cepheids train has (244, 2)\n",
      "Delta Scuti train has (118, 2)\n",
      "RR_Lyrae_test has (1749, 2)\n",
      "LPV_test has (257, 2)\n",
      "cepheids_test has (62, 2)\n",
      "Delta Scuti test has (29, 2)\n",
      "[23 24 25 26]\n",
      "[7001 1029  118  244]\n",
      "The number of sample in Class 0 is 7001 and is now augmented by 2 times. The augmented samples are 14002\n",
      "The number of sample in Class 1 is 1029 and is now augmented by 16 times. The augmented samples are 16464\n",
      "The number of sample in Class 2 is 118 and is now augmented by 144 times. The augmented samples are 16992\n",
      "The number of sample in Class 3 is 244 and is now augmented by 69 times. The augmented samples are 16836\n",
      "[2, 16, 144, 69]\n",
      "[23 24 25 26]\n",
      "[3025001011039 3029001015115 3025001012723 3025001014168 3059001016897\n",
      " 3063001012862 3025001019803 3043001007206 3035001006590 3025001011286]\n",
      "./data/phase_lc/3025001011039.txt\n",
      "./data/phase_lc/3029001015115.txt\n",
      "./data/phase_lc/3025001012723.txt\n",
      "./data/phase_lc/3025001014168.txt\n",
      "./data/phase_lc/3059001016897.txt\n",
      "./data/phase_lc/3063001012862.txt\n",
      "./data/phase_lc/3025001019803.txt\n",
      "./data/phase_lc/3043001007206.txt\n",
      "./data/phase_lc/3035001006590.txt\n",
      "./data/phase_lc/3025001011286.txt\n",
      "[3023002016726 3043002004587 3033007012816 3041014023399 3033017019433\n",
      " 3035017021821 3035017021291 3035017033877 3035017032421 3035017023361]\n",
      "./data/phase_lc/3023002016726.txt\n",
      "./data/phase_lc/3043002004587.txt\n",
      "./data/phase_lc/3033007012816.txt\n",
      "./data/phase_lc/3041014023399.txt\n",
      "./data/phase_lc/3033017019433.txt\n",
      "./data/phase_lc/3035017021821.txt\n",
      "./data/phase_lc/3035017021291.txt\n",
      "./data/phase_lc/3035017033877.txt\n",
      "./data/phase_lc/3035017032421.txt\n",
      "./data/phase_lc/3035017023361.txt\n",
      "[3029009009865 3053012007582 3027023013859 3045027006853 3055022007791\n",
      " 3031039018082 3061023005687 3053029008825 3027042040337 3027042046559]\n",
      "./data/phase_lc/3029009009865.txt\n",
      "./data/phase_lc/3053012007582.txt\n",
      "./data/phase_lc/3027023013859.txt\n",
      "./data/phase_lc/3045027006853.txt\n",
      "./data/phase_lc/3055022007791.txt\n",
      "./data/phase_lc/3031039018082.txt\n",
      "./data/phase_lc/3061023005687.txt\n",
      "./data/phase_lc/3053029008825.txt\n",
      "./data/phase_lc/3027042040337.txt\n",
      "./data/phase_lc/3027042046559.txt\n",
      "[3043003001790 3033007011579 3069005028014 3039011003666 3023014002004\n",
      " 3057009011334 3063013006226 3023036023403 3063019100837 3051032037255]\n",
      "./data/phase_lc/3043003001790.txt\n",
      "./data/phase_lc/3033007011579.txt\n",
      "./data/phase_lc/3069005028014.txt\n",
      "./data/phase_lc/3039011003666.txt\n",
      "./data/phase_lc/3023014002004.txt\n",
      "./data/phase_lc/3057009011334.txt\n",
      "./data/phase_lc/3063013006226.txt\n",
      "./data/phase_lc/3023036023403.txt\n",
      "./data/phase_lc/3063019100837.txt\n",
      "./data/phase_lc/3051032037255.txt\n",
      "[23 24 25 26]\n",
      "[3041001010523 3023002003478 3051002006292]\n",
      "[3033007004821 3065004018724 3033017001367]\n",
      "[3041013019377 3053014006946 3031035026267]\n",
      "[3033007006272 3033012018924 3027026012731]\n",
      "RRab train has (3460, 2)\n",
      "RRc train has (3002, 2)\n",
      "RRd train has (402, 2)\n",
      "Blazhko train has (137, 2)\n",
      "RRab test has (865, 2)\n",
      "RRc test has (750, 2)\n",
      "RRd test has (100, 2)\n",
      "Blazhko test has (34, 2)\n",
      "The number of sample in Class 0 is 3460 and is now augmented by 2 times. The augmented samples are 6920\n",
      "The number of sample in Class 1 is 3002 and is now augmented by 3 times. The augmented samples are 9006\n",
      "The number of sample in Class 2 is 402 and is now augmented by 24 times. The augmented samples are 9648\n",
      "The number of sample in Class 3 is 137 and is now augmented by 72 times. The augmented samples are 9864\n",
      "[2, 3, 24, 72]\n",
      "[1 2 3 4]\n",
      "[3025001011039 3029001015115 3025001012723 3025001014168 3059001016897\n",
      " 3063001012862 3025001019803 3043001007206 3035001006590 3025001011286]\n",
      "./data/phase_lc/3025001011039.txt\n",
      "./data/phase_lc/3029001015115.txt\n",
      "./data/phase_lc/3025001012723.txt\n",
      "./data/phase_lc/3025001014168.txt\n",
      "./data/phase_lc/3059001016897.txt\n",
      "./data/phase_lc/3063001012862.txt\n",
      "./data/phase_lc/3025001019803.txt\n",
      "./data/phase_lc/3043001007206.txt\n",
      "./data/phase_lc/3035001006590.txt\n",
      "./data/phase_lc/3025001011286.txt\n",
      "[3043001016067 3023001007314 3031001016447 3027001005640 3063001015076\n",
      " 3053001003881 3063001013464 3057001008573 3025002006386 3063001021212]\n",
      "./data/phase_lc/3043001016067.txt\n",
      "./data/phase_lc/3023001007314.txt\n",
      "./data/phase_lc/3031001016447.txt\n",
      "./data/phase_lc/3027001005640.txt\n",
      "./data/phase_lc/3063001015076.txt\n",
      "./data/phase_lc/3053001003881.txt\n",
      "./data/phase_lc/3063001013464.txt\n",
      "./data/phase_lc/3057001008573.txt\n",
      "./data/phase_lc/3025002006386.txt\n",
      "./data/phase_lc/3063001021212.txt\n",
      "[3025002019709 3029002016482 3057002021108 3037003016926 3061003016275\n",
      " 3039007017994 3021009013686 3047007014167 3031009008983 3049007004217]\n",
      "./data/phase_lc/3025002019709.txt\n",
      "./data/phase_lc/3029002016482.txt\n",
      "./data/phase_lc/3057002021108.txt\n",
      "./data/phase_lc/3037003016926.txt\n",
      "./data/phase_lc/3061003016275.txt\n",
      "./data/phase_lc/3039007017994.txt\n",
      "./data/phase_lc/3021009013686.txt\n",
      "./data/phase_lc/3047007014167.txt\n",
      "./data/phase_lc/3031009008983.txt\n",
      "./data/phase_lc/3049007004217.txt\n",
      "[3059001014294 3049008006074 3047009004897 3035016002621 3041017007885\n",
      " 3053016010811 3047019008810 3051018016560 3029029004674 3055019011694]\n",
      "./data/phase_lc/3059001014294.txt\n",
      "./data/phase_lc/3049008006074.txt\n",
      "./data/phase_lc/3047009004897.txt\n",
      "./data/phase_lc/3035016002621.txt\n",
      "./data/phase_lc/3041017007885.txt\n",
      "./data/phase_lc/3053016010811.txt\n",
      "./data/phase_lc/3047019008810.txt\n",
      "./data/phase_lc/3051018016560.txt\n",
      "./data/phase_lc/3029029004674.txt\n",
      "./data/phase_lc/3055019011694.txt\n",
      "[1 2 3 4]\n",
      "[3041001010523 3023002003478 3051002006292]\n",
      "[3037001007717 3041001008329 3021002019821]\n",
      "[3027004010129 3039006008644 3071007009567]\n",
      "[3043004012966 3031006006054 3021069043328]\n",
      "ACEP train has (122, 2)\n",
      "Cep-II train has (122, 2)\n",
      "ACEP test has (31, 2)\n",
      "Cep-II test has (31, 2)\n",
      "The number of sample in Class 0 is 122 and is now augmented by 40 times. The augmented samples are 4880\n",
      "The number of sample in Class 1 is 122 and is now augmented by 40 times. The augmented samples are 4880\n",
      "[40, 40]\n",
      "[10 12]\n",
      "[3043003001790 3033007011579 3069005028014 3039011003666 3023014002004\n",
      " 3057009011334 3063013006226 3023036023403 3063019100837 3051032037255]\n",
      "./data/phase_lc/3043003001790.txt\n",
      "./data/phase_lc/3033007011579.txt\n",
      "./data/phase_lc/3069005028014.txt\n",
      "./data/phase_lc/3039011003666.txt\n",
      "./data/phase_lc/3023014002004.txt\n",
      "./data/phase_lc/3057009011334.txt\n",
      "./data/phase_lc/3063013006226.txt\n",
      "./data/phase_lc/3023036023403.txt\n",
      "./data/phase_lc/3063019100837.txt\n",
      "./data/phase_lc/3051032037255.txt\n",
      "[3031007013721 3059004009102 3035017018795 3031025007790 3027030003005\n",
      " 3035029003470 3061020025779 3063020061457 3055026014665 3065020091392]\n",
      "./data/phase_lc/3031007013721.txt\n",
      "./data/phase_lc/3059004009102.txt\n",
      "./data/phase_lc/3035017018795.txt\n",
      "./data/phase_lc/3031025007790.txt\n",
      "./data/phase_lc/3027030003005.txt\n",
      "./data/phase_lc/3035029003470.txt\n",
      "./data/phase_lc/3061020025779.txt\n",
      "./data/phase_lc/3063020061457.txt\n",
      "./data/phase_lc/3055026014665.txt\n",
      "./data/phase_lc/3065020091392.txt\n",
      "[10 12]\n",
      "[3033007006272 3033012018924 3027026012731]\n",
      "[3053004015219 3063015005948 3021044048741]\n",
      "--------------------------------------------------\n",
      "Feature Extraction for Split 3 is finished\n",
      "('TRAIN:', array([    0,     1,     4, ..., 23140, 23141, 23142]), 'TEST:', array([    2,     3,     9, ..., 23121, 23134, 23138]))\n",
      "eclipsing_binary_train has (7214, 2)\n",
      "pulsating_train has (8394, 2)\n",
      "rotational_train has (2909, 2)\n",
      "eclipsing_binary_test has (1804, 2)\n",
      "pulsating_test has (2095, 2)\n",
      "rotational_test has (727, 2)\n",
      "The number of sample in Class 0 is 7214 and is now augmented by 2 times. The augmented samples are 14428\n",
      "The number of sample in Class 1 is 2909 and is now augmented by 5 times. The augmented samples are 14545\n",
      "The number of sample in Class 2 is 8394 and is now augmented by 2 times. The augmented samples are 16788\n",
      "[2, 5, 2]\n",
      "[20 21 22]\n",
      "[3039001008411 3047001026124 3037001022068 3035001009810 3033001008037\n",
      " 3031001017967 3035001017496 3043001020365 3023001017263 3031001013347]\n",
      "./data/phase_lc/3039001008411.txt\n",
      "./data/phase_lc/3047001026124.txt\n",
      "./data/phase_lc/3037001022068.txt\n",
      "./data/phase_lc/3035001009810.txt\n",
      "./data/phase_lc/3033001008037.txt\n",
      "./data/phase_lc/3031001017967.txt\n",
      "./data/phase_lc/3035001017496.txt\n",
      "./data/phase_lc/3043001020365.txt\n",
      "./data/phase_lc/3023001017263.txt\n",
      "./data/phase_lc/3031001013347.txt\n",
      "[3057001012326 3029001005060 3041002022608 3051002017867 3065001010858\n",
      " 3039002016264 3031003018713 3029003013827 3033005006377 3033005013071]\n",
      "./data/phase_lc/3057001012326.txt\n",
      "./data/phase_lc/3029001005060.txt\n",
      "./data/phase_lc/3041002022608.txt\n",
      "./data/phase_lc/3051002017867.txt\n",
      "./data/phase_lc/3065001010858.txt\n",
      "./data/phase_lc/3039002016264.txt\n",
      "./data/phase_lc/3031003018713.txt\n",
      "./data/phase_lc/3029003013827.txt\n",
      "./data/phase_lc/3033005006377.txt\n",
      "./data/phase_lc/3033005013071.txt\n",
      "[3041001010523 3029001015115 3025001012723 3025001014168 3025001019803\n",
      " 3043001007206 3035001006590 3025001011286 3063001015629 3065001030354]\n",
      "./data/phase_lc/3041001010523.txt\n",
      "./data/phase_lc/3029001015115.txt\n",
      "./data/phase_lc/3025001012723.txt\n",
      "./data/phase_lc/3025001014168.txt\n",
      "./data/phase_lc/3025001019803.txt\n",
      "./data/phase_lc/3043001007206.txt\n",
      "./data/phase_lc/3035001006590.txt\n",
      "./data/phase_lc/3025001011286.txt\n",
      "./data/phase_lc/3063001015629.txt\n",
      "./data/phase_lc/3065001030354.txt\n",
      "[20 21 22]\n",
      "[3039001005538 3057001005473 3033001013345]\n",
      "[3025002005197 3033003008133 3059003010721]\n",
      "[3025001011039 3059001016897 3063001012862]\n",
      "ecl train has (3607, 2)\n",
      "EA_train has (3607, 2)\n",
      "ecl_test has (902, 2)\n",
      "EA_test has (902, 2)\n",
      "The number of sample in Class 0 is 3607 and is now augmented by 2 times. The augmented samples are 7214\n",
      "The number of sample in Class 1 is 3607 and is now augmented by 2 times. The augmented samples are 7214\n",
      "[2, 2]\n",
      "[5 6]\n",
      "[3039001008411 3047001026124 3037001022068 3035001009810 3033001008037\n",
      " 3031001017967 3035001017496 3043001020365 3023001017263 3031001013347]\n",
      "./data/phase_lc/3039001008411.txt\n",
      "./data/phase_lc/3047001026124.txt\n",
      "./data/phase_lc/3037001022068.txt\n",
      "./data/phase_lc/3035001009810.txt\n",
      "./data/phase_lc/3033001008037.txt\n",
      "./data/phase_lc/3031001017967.txt\n",
      "./data/phase_lc/3035001017496.txt\n",
      "./data/phase_lc/3043001020365.txt\n",
      "./data/phase_lc/3023001017263.txt\n",
      "./data/phase_lc/3031001013347.txt\n",
      "[3023001018280 3027001016241 3041001017008 3037001023690 3043001017345\n",
      " 3035002022562 3037002017551 3037002016175 3041002003991 3049002005470]\n",
      "./data/phase_lc/3023001018280.txt\n",
      "./data/phase_lc/3027001016241.txt\n",
      "./data/phase_lc/3041001017008.txt\n",
      "./data/phase_lc/3037001023690.txt\n",
      "./data/phase_lc/3043001017345.txt\n",
      "./data/phase_lc/3035002022562.txt\n",
      "./data/phase_lc/3037002017551.txt\n",
      "./data/phase_lc/3037002016175.txt\n",
      "./data/phase_lc/3041002003991.txt\n",
      "./data/phase_lc/3049002005470.txt\n",
      "[5 6]\n",
      "[3039001005538 3057001005473 3033001013345]\n",
      "[3039002021528 3049002016589 3041003005615]\n",
      "RR Lyrae train has (7001, 2)\n",
      "LPV train has (1029, 2)\n",
      "Cepheids train has (246, 2)\n",
      "Delta Scuti train has (118, 2)\n",
      "RR_Lyrae_test has (1749, 2)\n",
      "LPV_test has (257, 2)\n",
      "cepheids_test has (60, 2)\n",
      "Delta Scuti test has (29, 2)\n",
      "[23 24 25 26]\n",
      "[7001 1029  118  246]\n",
      "The number of sample in Class 0 is 7001 and is now augmented by 2 times. The augmented samples are 14002\n",
      "The number of sample in Class 1 is 1029 and is now augmented by 16 times. The augmented samples are 16464\n",
      "The number of sample in Class 2 is 118 and is now augmented by 144 times. The augmented samples are 16992\n",
      "The number of sample in Class 3 is 246 and is now augmented by 69 times. The augmented samples are 16974\n",
      "[2, 16, 144, 69]\n",
      "[23 24 25 26]\n",
      "[3041001010523 3029001015115 3025001012723 3025001014168 3025001019803\n",
      " 3043001007206 3035001006590 3025001011286 3063001015629 3065001030354]\n",
      "./data/phase_lc/3041001010523.txt\n",
      "./data/phase_lc/3029001015115.txt\n",
      "./data/phase_lc/3025001012723.txt\n",
      "./data/phase_lc/3025001014168.txt\n",
      "./data/phase_lc/3025001019803.txt\n",
      "./data/phase_lc/3043001007206.txt\n",
      "./data/phase_lc/3035001006590.txt\n",
      "./data/phase_lc/3025001011286.txt\n",
      "./data/phase_lc/3063001015629.txt\n",
      "./data/phase_lc/3065001030354.txt\n",
      "[3023002016726 3043002004587 3033007004821 3065004018724 3033017019433\n",
      " 3035017021821 3035017021291 3033017001367 3035017032421 3035017023361]\n",
      "./data/phase_lc/3023002016726.txt\n",
      "./data/phase_lc/3043002004587.txt\n",
      "./data/phase_lc/3033007004821.txt\n",
      "./data/phase_lc/3065004018724.txt\n",
      "./data/phase_lc/3033017019433.txt\n",
      "./data/phase_lc/3035017021821.txt\n",
      "./data/phase_lc/3035017021291.txt\n",
      "./data/phase_lc/3033017001367.txt\n",
      "./data/phase_lc/3035017032421.txt\n",
      "./data/phase_lc/3035017023361.txt\n",
      "[3041013019377 3053012007582 3053014006946 3045027006853 3055022007791\n",
      " 3031035026267 3031039022708 3031039018082 3037037004851 3061023005687]\n",
      "./data/phase_lc/3041013019377.txt\n",
      "./data/phase_lc/3053012007582.txt\n",
      "./data/phase_lc/3053014006946.txt\n",
      "./data/phase_lc/3045027006853.txt\n",
      "./data/phase_lc/3055022007791.txt\n",
      "./data/phase_lc/3031035026267.txt\n",
      "./data/phase_lc/3031039022708.txt\n",
      "./data/phase_lc/3031039018082.txt\n",
      "./data/phase_lc/3037037004851.txt\n",
      "./data/phase_lc/3061023005687.txt\n",
      "[3033007006272 3069005028014 3033012018924 3039011003666 3023014002004\n",
      " 3063013006226 3027026012731 3063019100837 3029039024714 3041036011280]\n",
      "./data/phase_lc/3033007006272.txt\n",
      "./data/phase_lc/3069005028014.txt\n",
      "./data/phase_lc/3033012018924.txt\n",
      "./data/phase_lc/3039011003666.txt\n",
      "./data/phase_lc/3023014002004.txt\n",
      "./data/phase_lc/3063013006226.txt\n",
      "./data/phase_lc/3027026012731.txt\n",
      "./data/phase_lc/3063019100837.txt\n",
      "./data/phase_lc/3029039024714.txt\n",
      "./data/phase_lc/3041036011280.txt\n",
      "[23 24 25 26]\n",
      "[3025001011039 3059001016897 3063001012862]\n",
      "[3033007012816 3041014023399 3035017033877]\n",
      "[3029009009865 3027023013859 3033042045612]\n",
      "[3043003001790 3033007011579 3057009011334]\n",
      "RRab train has (3460, 2)\n",
      "RRc train has (3002, 2)\n",
      "RRd train has (402, 2)\n",
      "Blazhko train has (137, 2)\n",
      "RRab test has (865, 2)\n",
      "RRc test has (750, 2)\n",
      "RRd test has (100, 2)\n",
      "Blazhko test has (34, 2)\n",
      "The number of sample in Class 0 is 3460 and is now augmented by 2 times. The augmented samples are 6920\n",
      "The number of sample in Class 1 is 3002 and is now augmented by 3 times. The augmented samples are 9006\n",
      "The number of sample in Class 2 is 402 and is now augmented by 24 times. The augmented samples are 9648\n",
      "The number of sample in Class 3 is 137 and is now augmented by 72 times. The augmented samples are 9864\n",
      "[2, 3, 24, 72]\n",
      "[1 2 3 4]\n",
      "[3041001010523 3029001015115 3025001012723 3025001014168 3025001019803\n",
      " 3043001007206 3035001006590 3025001011286 3063001015629 3065001030354]\n",
      "./data/phase_lc/3041001010523.txt\n",
      "./data/phase_lc/3029001015115.txt\n",
      "./data/phase_lc/3025001012723.txt\n",
      "./data/phase_lc/3025001014168.txt\n",
      "./data/phase_lc/3025001019803.txt\n",
      "./data/phase_lc/3043001007206.txt\n",
      "./data/phase_lc/3035001006590.txt\n",
      "./data/phase_lc/3025001011286.txt\n",
      "./data/phase_lc/3063001015629.txt\n",
      "./data/phase_lc/3065001030354.txt\n",
      "[3031001016447 3037001007717 3027001005640 3041001008329 3053001003881\n",
      " 3063001013464 3025002006386 3021002019821 3059001010062 3023002009169]\n",
      "./data/phase_lc/3031001016447.txt\n",
      "./data/phase_lc/3037001007717.txt\n",
      "./data/phase_lc/3027001005640.txt\n",
      "./data/phase_lc/3041001008329.txt\n",
      "./data/phase_lc/3053001003881.txt\n",
      "./data/phase_lc/3063001013464.txt\n",
      "./data/phase_lc/3025002006386.txt\n",
      "./data/phase_lc/3021002019821.txt\n",
      "./data/phase_lc/3059001010062.txt\n",
      "./data/phase_lc/3023002009169.txt\n",
      "[3025002019709 3029002016482 3057002021108 3037003016926 3027004010129\n",
      " 3061003016275 3039006008644 3021009013686 3031009008983 3051007008434]\n",
      "./data/phase_lc/3025002019709.txt\n",
      "./data/phase_lc/3029002016482.txt\n",
      "./data/phase_lc/3057002021108.txt\n",
      "./data/phase_lc/3037003016926.txt\n",
      "./data/phase_lc/3027004010129.txt\n",
      "./data/phase_lc/3061003016275.txt\n",
      "./data/phase_lc/3039006008644.txt\n",
      "./data/phase_lc/3021009013686.txt\n",
      "./data/phase_lc/3031009008983.txt\n",
      "./data/phase_lc/3051007008434.txt\n",
      "[3059001014294 3043004012966 3031006006054 3035016002621 3041017007885\n",
      " 3053016010811 3047019008810 3051018016560 3029029004674 3055019011694]\n",
      "./data/phase_lc/3059001014294.txt\n",
      "./data/phase_lc/3043004012966.txt\n",
      "./data/phase_lc/3031006006054.txt\n",
      "./data/phase_lc/3035016002621.txt\n",
      "./data/phase_lc/3041017007885.txt\n",
      "./data/phase_lc/3053016010811.txt\n",
      "./data/phase_lc/3047019008810.txt\n",
      "./data/phase_lc/3051018016560.txt\n",
      "./data/phase_lc/3029029004674.txt\n",
      "./data/phase_lc/3055019011694.txt\n",
      "[1 2 3 4]\n",
      "[3025001011039 3059001016897 3063001012862]\n",
      "[3043001016067 3023001007314 3063001015076]\n",
      "[3039007017994 3047007014167 3049007004217]\n",
      "[3049008006074 3047009004897 3051023002638]\n",
      "ACEP train has (123, 2)\n",
      "Cep-II train has (123, 2)\n",
      "ACEP test has (30, 2)\n",
      "Cep-II test has (30, 2)\n",
      "The number of sample in Class 0 is 123 and is now augmented by 40 times. The augmented samples are 4920\n",
      "The number of sample in Class 1 is 123 and is now augmented by 40 times. The augmented samples are 4920\n",
      "[40, 40]\n",
      "[10 12]\n",
      "[3033007006272 3069005028014 3033012018924 3039011003666 3023014002004\n",
      " 3063013006226 3027026012731 3063019100837 3029039024714 3041036011280]\n",
      "./data/phase_lc/3033007006272.txt\n",
      "./data/phase_lc/3069005028014.txt\n",
      "./data/phase_lc/3033012018924.txt\n",
      "./data/phase_lc/3039011003666.txt\n",
      "./data/phase_lc/3023014002004.txt\n",
      "./data/phase_lc/3063013006226.txt\n",
      "./data/phase_lc/3027026012731.txt\n",
      "./data/phase_lc/3063019100837.txt\n",
      "./data/phase_lc/3029039024714.txt\n",
      "./data/phase_lc/3041036011280.txt\n",
      "[3053004015219 3031007013721 3059004009102 3035017018795 3031025007790\n",
      " 3063015005948 3027030003005 3035029003470 3063020061457 3055026014665]\n",
      "./data/phase_lc/3053004015219.txt\n",
      "./data/phase_lc/3031007013721.txt\n",
      "./data/phase_lc/3059004009102.txt\n",
      "./data/phase_lc/3035017018795.txt\n",
      "./data/phase_lc/3031025007790.txt\n",
      "./data/phase_lc/3063015005948.txt\n",
      "./data/phase_lc/3027030003005.txt\n",
      "./data/phase_lc/3035029003470.txt\n",
      "./data/phase_lc/3063020061457.txt\n",
      "./data/phase_lc/3055026014665.txt\n",
      "[10 12]\n",
      "[3043003001790 3033007011579 3057009011334]\n",
      "[3061020025779 3065020091392 3031043043407]\n",
      "--------------------------------------------------\n",
      "Feature Extraction for Split 4 is finished\n",
      "('TRAIN:', array([    0,     1,     2, ..., 23140, 23141, 23142]), 'TEST:', array([    8,    11,    14, ..., 23108, 23110, 23126]))\n",
      "eclipsing_binary_train has (7216, 2)\n",
      "pulsating_train has (8394, 2)\n",
      "rotational_train has (2909, 2)\n",
      "eclipsing_binary_test has (1802, 2)\n",
      "pulsating_test has (2095, 2)\n",
      "rotational_test has (727, 2)\n",
      "The number of sample in Class 0 is 7216 and is now augmented by 2 times. The augmented samples are 14432\n",
      "The number of sample in Class 1 is 2909 and is now augmented by 5 times. The augmented samples are 14545\n",
      "The number of sample in Class 2 is 8394 and is now augmented by 2 times. The augmented samples are 16788\n",
      "[2, 5, 2]\n",
      "[20 21 22]\n",
      "[3039001008411 3047001026124 3035001009810 3033001008037 3031001017967\n",
      " 3035001017496 3043001020365 3039001005538 3023001017263 3031001013347]\n",
      "./data/phase_lc/3039001008411.txt\n",
      "./data/phase_lc/3047001026124.txt\n",
      "./data/phase_lc/3035001009810.txt\n",
      "./data/phase_lc/3033001008037.txt\n",
      "./data/phase_lc/3031001017967.txt\n",
      "./data/phase_lc/3035001017496.txt\n",
      "./data/phase_lc/3043001020365.txt\n",
      "./data/phase_lc/3039001005538.txt\n",
      "./data/phase_lc/3023001017263.txt\n",
      "./data/phase_lc/3031001013347.txt\n",
      "[3057001012326 3029001005060 3025002005197 3041002022608 3051002017867\n",
      " 3039002016264 3031003018713 3029003013827 3033003008133 3059003010721]\n",
      "./data/phase_lc/3057001012326.txt\n",
      "./data/phase_lc/3029001005060.txt\n",
      "./data/phase_lc/3025002005197.txt\n",
      "./data/phase_lc/3041002022608.txt\n",
      "./data/phase_lc/3051002017867.txt\n",
      "./data/phase_lc/3039002016264.txt\n",
      "./data/phase_lc/3031003018713.txt\n",
      "./data/phase_lc/3029003013827.txt\n",
      "./data/phase_lc/3033003008133.txt\n",
      "./data/phase_lc/3059003010721.txt\n",
      "[3041001010523 3025001011039 3029001015115 3059001016897 3063001012862\n",
      " 3025001019803 3043001007206 3025001011286 3021001012412 3021001008760]\n",
      "./data/phase_lc/3041001010523.txt\n",
      "./data/phase_lc/3025001011039.txt\n",
      "./data/phase_lc/3029001015115.txt\n",
      "./data/phase_lc/3059001016897.txt\n",
      "./data/phase_lc/3063001012862.txt\n",
      "./data/phase_lc/3025001019803.txt\n",
      "./data/phase_lc/3043001007206.txt\n",
      "./data/phase_lc/3025001011286.txt\n",
      "./data/phase_lc/3021001012412.txt\n",
      "./data/phase_lc/3021001008760.txt\n",
      "[20 21 22]\n",
      "[3037001022068 3063001006018 3021001008400]\n",
      "[3065001010858 3039006006977 3049005010331]\n",
      "[3025001012723 3025001014168 3035001006590]\n",
      "ecl train has (3608, 2)\n",
      "EA_train has (3608, 2)\n",
      "ecl_test has (901, 2)\n",
      "EA_test has (901, 2)\n",
      "The number of sample in Class 0 is 3608 and is now augmented by 2 times. The augmented samples are 7216\n",
      "The number of sample in Class 1 is 3608 and is now augmented by 2 times. The augmented samples are 7216\n",
      "[2, 2]\n",
      "[5 6]\n",
      "[3039001008411 3047001026124 3035001009810 3033001008037 3031001017967\n",
      " 3035001017496 3043001020365 3039001005538 3023001017263 3031001013347]\n",
      "./data/phase_lc/3039001008411.txt\n",
      "./data/phase_lc/3047001026124.txt\n",
      "./data/phase_lc/3035001009810.txt\n",
      "./data/phase_lc/3033001008037.txt\n",
      "./data/phase_lc/3031001017967.txt\n",
      "./data/phase_lc/3035001017496.txt\n",
      "./data/phase_lc/3043001020365.txt\n",
      "./data/phase_lc/3039001005538.txt\n",
      "./data/phase_lc/3023001017263.txt\n",
      "./data/phase_lc/3031001013347.txt\n",
      "[3023001018280 3027001016241 3037001023690 3043001017345 3037002017551\n",
      " 3037002016175 3039002021528 3041002003991 3049002005470 3049002016589]\n",
      "./data/phase_lc/3023001018280.txt\n",
      "./data/phase_lc/3027001016241.txt\n",
      "./data/phase_lc/3037001023690.txt\n",
      "./data/phase_lc/3043001017345.txt\n",
      "./data/phase_lc/3037002017551.txt\n",
      "./data/phase_lc/3037002016175.txt\n",
      "./data/phase_lc/3039002021528.txt\n",
      "./data/phase_lc/3041002003991.txt\n",
      "./data/phase_lc/3049002005470.txt\n",
      "./data/phase_lc/3049002016589.txt\n",
      "[5 6]\n",
      "[3037001022068 3063001006018 3021001008400]\n",
      "[3041001017008 3035002022562 3039003004405]\n",
      "RR Lyrae train has (7001, 2)\n",
      "LPV train has (1029, 2)\n",
      "Cepheids train has (246, 2)\n",
      "Delta Scuti train has (118, 2)\n",
      "RR_Lyrae_test has (1749, 2)\n",
      "LPV_test has (257, 2)\n",
      "cepheids_test has (60, 2)\n",
      "Delta Scuti test has (29, 2)\n",
      "[23 24 25 26]\n",
      "[7001 1029  118  246]\n",
      "The number of sample in Class 0 is 7001 and is now augmented by 2 times. The augmented samples are 14002\n",
      "The number of sample in Class 1 is 1029 and is now augmented by 16 times. The augmented samples are 16464\n",
      "The number of sample in Class 2 is 118 and is now augmented by 144 times. The augmented samples are 16992\n",
      "The number of sample in Class 3 is 246 and is now augmented by 69 times. The augmented samples are 16974\n",
      "[2, 16, 144, 69]\n",
      "[23 24 25 26]\n",
      "[3041001010523 3025001011039 3029001015115 3059001016897 3063001012862\n",
      " 3025001019803 3043001007206 3025001011286 3021001012412 3021001008760]\n",
      "./data/phase_lc/3041001010523.txt\n",
      "./data/phase_lc/3025001011039.txt\n",
      "./data/phase_lc/3029001015115.txt\n",
      "./data/phase_lc/3059001016897.txt\n",
      "./data/phase_lc/3063001012862.txt\n",
      "./data/phase_lc/3025001019803.txt\n",
      "./data/phase_lc/3043001007206.txt\n",
      "./data/phase_lc/3025001011286.txt\n",
      "./data/phase_lc/3021001012412.txt\n",
      "./data/phase_lc/3021001008760.txt\n",
      "[3043002004587 3033007012816 3033007004821 3065004018724 3041014023399\n",
      " 3035017021821 3033017001367 3035017033877 3035017023361 3035017023797]\n",
      "./data/phase_lc/3043002004587.txt\n",
      "./data/phase_lc/3033007012816.txt\n",
      "./data/phase_lc/3033007004821.txt\n",
      "./data/phase_lc/3065004018724.txt\n",
      "./data/phase_lc/3041014023399.txt\n",
      "./data/phase_lc/3035017021821.txt\n",
      "./data/phase_lc/3033017001367.txt\n",
      "./data/phase_lc/3035017033877.txt\n",
      "./data/phase_lc/3035017023361.txt\n",
      "./data/phase_lc/3035017023797.txt\n",
      "[3029009009865 3041013019377 3053014006946 3027023013859 3031035026267\n",
      " 3031039022708 3037037004851 3061023005687 3053029008825 3027042040337]\n",
      "./data/phase_lc/3029009009865.txt\n",
      "./data/phase_lc/3041013019377.txt\n",
      "./data/phase_lc/3053014006946.txt\n",
      "./data/phase_lc/3027023013859.txt\n",
      "./data/phase_lc/3031035026267.txt\n",
      "./data/phase_lc/3031039022708.txt\n",
      "./data/phase_lc/3037037004851.txt\n",
      "./data/phase_lc/3061023005687.txt\n",
      "./data/phase_lc/3053029008825.txt\n",
      "./data/phase_lc/3027042040337.txt\n",
      "[3043003001790 3033007011579 3033007006272 3069005028014 3033012018924\n",
      " 3039011003666 3023014002004 3057009011334 3063013006226 3027026012731]\n",
      "./data/phase_lc/3043003001790.txt\n",
      "./data/phase_lc/3033007011579.txt\n",
      "./data/phase_lc/3033007006272.txt\n",
      "./data/phase_lc/3069005028014.txt\n",
      "./data/phase_lc/3033012018924.txt\n",
      "./data/phase_lc/3039011003666.txt\n",
      "./data/phase_lc/3023014002004.txt\n",
      "./data/phase_lc/3057009011334.txt\n",
      "./data/phase_lc/3063013006226.txt\n",
      "./data/phase_lc/3027026012731.txt\n",
      "[23 24 25 26]\n",
      "[3025001012723 3025001014168 3035001006590]\n",
      "[3023002016726 3033017019433 3035017021291]\n",
      "[3053012007582 3045027006853 3055022007791]\n",
      "[3035067050834 3047057089104 3049056005599]\n",
      "RRab train has (3460, 2)\n",
      "RRc train has (3002, 2)\n",
      "RRd train has (402, 2)\n",
      "Blazhko train has (137, 2)\n",
      "RRab test has (865, 2)\n",
      "RRc test has (750, 2)\n",
      "RRd test has (100, 2)\n",
      "Blazhko test has (34, 2)\n",
      "The number of sample in Class 0 is 3460 and is now augmented by 2 times. The augmented samples are 6920\n",
      "The number of sample in Class 1 is 3002 and is now augmented by 3 times. The augmented samples are 9006\n",
      "The number of sample in Class 2 is 402 and is now augmented by 24 times. The augmented samples are 9648\n",
      "The number of sample in Class 3 is 137 and is now augmented by 72 times. The augmented samples are 9864\n",
      "[2, 3, 24, 72]\n",
      "[1 2 3 4]\n",
      "[3041001010523 3025001011039 3029001015115 3059001016897 3063001012862\n",
      " 3025001019803 3043001007206 3025001011286 3021001012412 3021001008760]\n",
      "./data/phase_lc/3041001010523.txt\n",
      "./data/phase_lc/3025001011039.txt\n",
      "./data/phase_lc/3029001015115.txt\n",
      "./data/phase_lc/3059001016897.txt\n",
      "./data/phase_lc/3063001012862.txt\n",
      "./data/phase_lc/3025001019803.txt\n",
      "./data/phase_lc/3043001007206.txt\n",
      "./data/phase_lc/3025001011286.txt\n",
      "./data/phase_lc/3021001012412.txt\n",
      "./data/phase_lc/3021001008760.txt\n",
      "[3043001016067 3023001007314 3031001016447 3037001007717 3027001005640\n",
      " 3063001015076 3041001008329 3063001013464 3057001008573 3025002006386]\n",
      "./data/phase_lc/3043001016067.txt\n",
      "./data/phase_lc/3023001007314.txt\n",
      "./data/phase_lc/3031001016447.txt\n",
      "./data/phase_lc/3037001007717.txt\n",
      "./data/phase_lc/3027001005640.txt\n",
      "./data/phase_lc/3063001015076.txt\n",
      "./data/phase_lc/3041001008329.txt\n",
      "./data/phase_lc/3063001013464.txt\n",
      "./data/phase_lc/3057001008573.txt\n",
      "./data/phase_lc/3025002006386.txt\n",
      "[3025002019709 3037003016926 3027004010129 3039006008644 3039007017994\n",
      " 3021009013686 3047007014167 3031009008983 3049007004217 3051007008434]\n",
      "./data/phase_lc/3025002019709.txt\n",
      "./data/phase_lc/3037003016926.txt\n",
      "./data/phase_lc/3027004010129.txt\n",
      "./data/phase_lc/3039006008644.txt\n",
      "./data/phase_lc/3039007017994.txt\n",
      "./data/phase_lc/3021009013686.txt\n",
      "./data/phase_lc/3047007014167.txt\n",
      "./data/phase_lc/3031009008983.txt\n",
      "./data/phase_lc/3049007004217.txt\n",
      "./data/phase_lc/3051007008434.txt\n",
      "[3043004012966 3031006006054 3049008006074 3047009004897 3035016002621\n",
      " 3041017007885 3047019008810 3051018016560 3029029004674 3055019011694]\n",
      "./data/phase_lc/3043004012966.txt\n",
      "./data/phase_lc/3031006006054.txt\n",
      "./data/phase_lc/3049008006074.txt\n",
      "./data/phase_lc/3047009004897.txt\n",
      "./data/phase_lc/3035016002621.txt\n",
      "./data/phase_lc/3041017007885.txt\n",
      "./data/phase_lc/3047019008810.txt\n",
      "./data/phase_lc/3051018016560.txt\n",
      "./data/phase_lc/3029029004674.txt\n",
      "./data/phase_lc/3055019011694.txt\n",
      "[1 2 3 4]\n",
      "[3025001012723 3025001014168 3035001006590]\n",
      "[3053001003881 3057002013481 3045002023095]\n",
      "[3029002016482 3057002021108 3061003016275]\n",
      "[3059001014294 3053016010811 3047027025878]\n",
      "ACEP train has (123, 2)\n",
      "Cep-II train has (123, 2)\n",
      "ACEP test has (30, 2)\n",
      "Cep-II test has (30, 2)\n",
      "The number of sample in Class 0 is 123 and is now augmented by 40 times. The augmented samples are 4920\n",
      "The number of sample in Class 1 is 123 and is now augmented by 40 times. The augmented samples are 4920\n",
      "[40, 40]\n",
      "[10 12]\n",
      "[3043003001790 3033007011579 3033007006272 3069005028014 3033012018924\n",
      " 3039011003666 3023014002004 3057009011334 3063013006226 3027026012731]\n",
      "./data/phase_lc/3043003001790.txt\n",
      "./data/phase_lc/3033007011579.txt\n",
      "./data/phase_lc/3033007006272.txt\n",
      "./data/phase_lc/3069005028014.txt\n",
      "./data/phase_lc/3033012018924.txt\n",
      "./data/phase_lc/3039011003666.txt\n",
      "./data/phase_lc/3023014002004.txt\n",
      "./data/phase_lc/3057009011334.txt\n",
      "./data/phase_lc/3063013006226.txt\n",
      "./data/phase_lc/3027026012731.txt\n",
      "[3053004015219 3035017018795 3031025007790 3063015005948 3035029003470\n",
      " 3061020025779 3063020061457 3055026014665 3065020091392 3021044048741]\n",
      "./data/phase_lc/3053004015219.txt\n",
      "./data/phase_lc/3035017018795.txt\n",
      "./data/phase_lc/3031025007790.txt\n",
      "./data/phase_lc/3063015005948.txt\n",
      "./data/phase_lc/3035029003470.txt\n",
      "./data/phase_lc/3061020025779.txt\n",
      "./data/phase_lc/3063020061457.txt\n",
      "./data/phase_lc/3055026014665.txt\n",
      "./data/phase_lc/3065020091392.txt\n",
      "./data/phase_lc/3021044048741.txt\n",
      "[10 12]\n",
      "[3035067050834 3047057089104 3049056005599]\n",
      "[3031007013721 3059004009102 3027030003005]\n",
      "--------------------------------------------------\n",
      "Feature Extraction for Split 5 is finished\n"
     ]
    }
   ],
   "source": [
    "n_splits         = 5\n",
    "data_preparation = True\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "X   = ascii_files.File_Name\n",
    "y   = ascii_files.Type\n",
    "\n",
    "period_data                   = ascii_data[['Period', 'File_Name', 'Type']]# Use for Test set Period\n",
    "periods                       = ascii_data[['File_Name', 'Period']] # Use for Test set Period\n",
    "update_ascii_period           = pd.DataFrame(periods.File_Name.astype(str) + '_1') # Use for Training set Period\n",
    "update_ascii_period['Period'] = periods.Period\n",
    "\n",
    "split_num = 1\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    if data_preparation: \n",
    "\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X.index[train_index], X.index[test_index]\n",
    "        y_train, y_test = y.index[train_index], y.index[test_index]\n",
    "\n",
    "        X_training      = ascii_files.iloc[train_index]\n",
    "        X_testing       = ascii_files.iloc[test_index]\n",
    "\n",
    "        data_columns = ['magnitude', 'time']\n",
    "        features     = ['Skew', 'Mean', 'Std', 'SmallKurtosis', 'Amplitude', 'Meanvariance']\n",
    "        data_dir     = './data/phase_lc/'\n",
    "        \n",
    "        RRab_train        = stars_label(X_training, true_class_1)\n",
    "        RRc_train         = stars_label(X_training, true_class_2) \n",
    "        RRd_train         = stars_label(X_training, true_class_3)\n",
    "        blazhko_train     = stars_label(X_training, true_class_4)\n",
    "        contact_Bi_train  = stars_label(X_training, true_class_5)\n",
    "        semi_det_Bi_train = stars_label(X_training, true_class_6)\n",
    "        rot_train         = stars_label(X_training, true_class_7)\n",
    "        LPV_train         = stars_label(X_training, true_class_8)\n",
    "        delta_scuti_train = stars_label(X_training, true_class_9)\n",
    "        ACEP_train        = stars_label(X_training, true_class_10)\n",
    "        cep_ii_train      = stars_label(X_training, true_class_12)\n",
    "\n",
    "        RRab_test        = stars_label(X_testing, true_class_1)\n",
    "        RRc_test         = stars_label(X_testing, true_class_2) \n",
    "        RRd_test         = stars_label(X_testing, true_class_3)\n",
    "        blazhko_test     = stars_label(X_testing, true_class_4)\n",
    "        contact_Bi_test  = stars_label(X_testing, true_class_5)\n",
    "        semi_det_Bi_test = stars_label(X_testing, true_class_6)\n",
    "        rot_test         = stars_label(X_testing, true_class_7)\n",
    "        LPV_test         = stars_label(X_testing, true_class_8)\n",
    "        delta_scuti_test = stars_label(X_testing, true_class_9)\n",
    "        ACEP_test        = stars_label(X_testing, true_class_10)\n",
    "        cep_ii_test      = stars_label(X_testing, true_class_12)\n",
    "        \n",
    "        '-----------------------------------------------------------------------------'\n",
    "                                        # FIRST LAYER\n",
    "        '-----------------------------------------------------------------------------'\n",
    "        training_data_FL, testing_data_FL, y_FL_training_counts = first_layer()\n",
    "\n",
    "        # This part is calculating the number of times each class need to be augmented    \n",
    "        ns_FL = num_augmentation(nAugmentation=17000, y_training_counts=y_FL_training_counts)\n",
    "        print(ns_FL)\n",
    "\n",
    "\n",
    "        # Each class is augmented using their respective number of samples and features are extracted\n",
    "        augmentation_data, feature_file = augmentation_and_featureExtraction(data_dir=data_dir, \\\n",
    "                                              data_ = training_data_FL,data_columns=data_columns,\\\n",
    "                                              features=features,split_num=split_num,update_ascii_period=update_ascii_period,\\\n",
    "                                              number_of_samples=ns_FL,save_folder_training = './data/GP/HC/layer1_EclRotPul/training_set/')\n",
    "\n",
    "        # Features from each class from the test set are extracted and save in a single \n",
    "        feature_file_testSet    = feature_extraction_test_set(data_dir=data_dir,\\\n",
    "                                      X_testing=testing_data_FL,data_columns=data_columns,\\\n",
    "                                      features=features,split_num=split_num,save_folder_test= './data/GP/HC/layer1_EclRotPul/test_set/')\n",
    "        \n",
    "        '-------------------------------------------------------------------------------'\n",
    "                                # SECOND LAYER ECLIPSING BINARY\n",
    "        '-------------------------------------------------------------------------------'\n",
    "        training_data_SL_EB, testing_data_SL_EB, y_SL_EB_training_counts = second_layer_EB()\n",
    "                \n",
    "        # This part is calculating the number of times each class need to be augmented    \n",
    "        ns_SL_EB = num_augmentation(nAugmentation=10000, y_training_counts=y_SL_EB_training_counts)\n",
    "        print(ns_SL_EB)\n",
    "\n",
    "\n",
    "        # Each class is augmented using their respective number of samples and features are extracted\n",
    "        augmentation_data_SL_EB, feature_file_SL_EB = augmentation_and_featureExtraction(data_dir=data_dir, \\\n",
    "                                              data_ = training_data_SL_EB,data_columns=data_columns,\\\n",
    "                                              features=features,split_num=split_num,update_ascii_period=update_ascii_period,\\\n",
    "                                              number_of_samples=ns_SL_EB,save_folder_training = './data/GP/HC/layer2_EB/training_set/')\n",
    "\n",
    "        # Features from each class from the test set are extracted and save in a single \n",
    "        feature_file_testSet_SL_EB    = feature_extraction_test_set(data_dir=data_dir,\\\n",
    "                                      X_testing=testing_data_SL_EB,data_columns=data_columns,\\\n",
    "                                      features=features,split_num=split_num,save_folder_test= './data/GP/HC/layer2_EB/test_set/')\n",
    "        \n",
    "        '-----------------------------------------------------------------------------'\n",
    "                            # SECOND LAYER RR LYRAE PULSATING LPV CEPHEIDS\n",
    "        '-----------------------------------------------------------------------------'\n",
    "        training_data_SL_RLCD, testing_data_SL_RLCD, y_SL_RLCD_training_counts = second_layer_RLCD()\n",
    "\n",
    "\n",
    "        # This part is calculating the number of times each class need to be augmented    \n",
    "        ns_SL_RLCD = num_augmentation(nAugmentation=17000, y_training_counts=y_SL_RLCD_training_counts)\n",
    "        print(ns_SL_RLCD)\n",
    "\n",
    "\n",
    "        # Each class is augmented using their respective number of samples and features are extracted\n",
    "        augmentation_data_SL_RLCD, feature_file_SL_RLCD = augmentation_and_featureExtraction(data_dir=data_dir, \\\n",
    "                                              data_ = training_data_SL_RLCD,data_columns=data_columns,\\\n",
    "                                              features=features,split_num=split_num,update_ascii_period=update_ascii_period,\\\n",
    "                                              number_of_samples=ns_SL_RLCD,save_folder_training = './data/GP/HC/layer2_RLCD/training_set/')\n",
    "\n",
    "        # Features from each class from the test set are extracted and save in a single \n",
    "        feature_file_testSet_SL_RLCD    = feature_extraction_test_set(data_dir=data_dir,\\\n",
    "                                      X_testing=testing_data_SL_RLCD,data_columns=data_columns,\\\n",
    "                                      features=features,split_num=split_num,save_folder_test= './data/GP/HC/layer2_RLCD/test_set/')\n",
    "        \n",
    "        '-----------------------------------------------------------------------------'\n",
    "                        # THIRD LAYER RR LYRAE: RRab, RRc, RRd, Blazhko\n",
    "        '-----------------------------------------------------------------------------'\n",
    "        training_data_TL_RRLyrae, testing_data_TL_RRLyrae, y_TL_RRLyrae_training_counts = third_layer_RRLyrae()\n",
    "        \n",
    "        # This part is calculating the number of times each class need to be augmented    \n",
    "        ns_TL_RRLyrae = num_augmentation(nAugmentation=10000, y_training_counts=y_TL_RRLyrae_training_counts)\n",
    "        print(ns_TL_RRLyrae)\n",
    "\n",
    "\n",
    "        # Each class is augmented using their respective number of samples and features are extracted\n",
    "        augmentation_data_TL_RRLyrae, feature_file_TL_RRLyrae = augmentation_and_featureExtraction(data_dir=data_dir, \\\n",
    "                                              data_ = training_data_TL_RRLyrae,data_columns=data_columns,\\\n",
    "                                              features=features,split_num=split_num,update_ascii_period=update_ascii_period,\\\n",
    "                                              number_of_samples=ns_TL_RRLyrae,save_folder_training = './data/GP/HC/layer3_RRLyrae/training_set/')\n",
    "\n",
    "        # Features from each class from the test set are extracted and save in a single \n",
    "        feature_file_testSet_TL_RRLyrae    = feature_extraction_test_set(data_dir=data_dir,\\\n",
    "                                      X_testing=testing_data_TL_RRLyrae,data_columns=data_columns,\\\n",
    "                                      features=features,split_num=split_num,save_folder_test= './data/GP/HC/layer3_RRLyrae/test_set/')\n",
    "\n",
    "        '-----------------------------------------------------------------------------'\n",
    "                                # THIRD LAYER Cepheids: ACEP and Cep-II\n",
    "        '-----------------------------------------------------------------------------'\n",
    "        training_data_TL_cep, testing_data_TL_cep, y_TL_cep_training_counts = third_layer_Cepheids()\n",
    "        \n",
    "        # This part is calculating the number of times each class need to be augmented    \n",
    "        ns_TL_cep = num_augmentation(nAugmentation=5000, y_training_counts=y_TL_cep_training_counts)\n",
    "        print(ns_TL_cep)\n",
    "\n",
    "\n",
    "        # Each class is augmented using their respective number of samples and features are extracted\n",
    "        augmentation_data_TL_cep, feature_file_TL_cep = augmentation_and_featureExtraction(data_dir=data_dir, \\\n",
    "                                              data_ = training_data_TL_cep,data_columns=data_columns,\\\n",
    "                                              features=features,split_num=split_num,update_ascii_period=update_ascii_period,\\\n",
    "                                              number_of_samples=ns_TL_cep,save_folder_training = './data/GP/HC/layer3_Cepheids/training_set/')\n",
    "\n",
    "        # Features from each class from the test set are extracted and save in a single \n",
    "        feature_file_testSet_TL_cep    = feature_extraction_test_set(data_dir=data_dir,\\\n",
    "                                      X_testing=testing_data_TL_cep,data_columns=data_columns,\\\n",
    "                                      features=features,split_num=split_num,save_folder_test= './data/GP/HC/layer3_Cepheids/test_set/')\n",
    "   \n",
    "        \n",
    "    print('-'*50)\n",
    "    print('Feature Extraction for Split {} is finished'.format(split_num))  \n",
    "    split_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d66fa76e2e8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mall_testing_set_FL\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_dir_FL\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Test_features.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0meclipsing_class\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mall_training_set_FL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_training_set_FL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNew_label\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0meclipsing_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_examples_FL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mrotational_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_training_set_FL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_training_set_FL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNew_label\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mrotational_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_examples_FL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mpulsating_class\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mall_training_set_FL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_training_set_FL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNew_label\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mpulsating_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_examples_FL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   2642\u001b[0m                              \"provide positive value.\")\n\u001b[1;32m   2643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2644\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2645\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_copy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice (numpy/random/mtrand/mtrand.c:17481)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "multi_class  = True\n",
    "save_model=False\n",
    "load_model=False\n",
    "acc_rf_FL = [];mcc_rf_FL = [];acc_xgb_FL = [];mcc_xgb_FL = [];\\\n",
    "acc_rf_SL_EB = [];mcc_rf_SL_EB = [];acc_xgb_SL_EB = [];mcc_xgb_SL_EB = [];\\\n",
    "acc_rf_SL_RLCD = [];mcc_rf_SL_RLCD = [];acc_xgb_SL_RLCD = [];mcc_xgb_SL_RLCD = [];\\\n",
    "acc_rf_TL_RL = [];mcc_rf_TL_RL = [];acc_xgb_TL_RL = [];mcc_xgb_TL_RL = [];\\\n",
    "acc_rf_TL_Cep = [];mcc_rf_TL_Cep = [];acc_xgb_TL_Cep = [];mcc_xgb_TL_Cep = []\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "for split_num in range(n_splits):\n",
    "    split_num += 1\n",
    "    \n",
    "    '-----------------------------------------------------------------------------'\n",
    "                                # FIRST LAYER\n",
    "    '-----------------------------------------------------------------------------'\n",
    "    num_examples_FL = 14000\n",
    "    training_dir_FL      = './data/GP/HC/layer1_EclRotPul/training_set/Split_'+str(split_num)+'/'\n",
    "    testing_dir_FL       = './data/GP/HC/layer1_EclRotPul/test_set/Split_'+str(split_num)+'/'\n",
    "    all_training_set_FL  = pd.read_csv(training_dir_FL+'Training_features.csv',sep=',')\n",
    "    all_testing_set_FL   = pd.read_csv(testing_dir_FL+'Test_features.csv',sep=',')\n",
    "        \n",
    "    eclipsing_class  = all_training_set_FL[all_training_set_FL.New_label==eclipsing_label].sample(n=num_examples_FL)\n",
    "    rotational_class = all_training_set_FL[all_training_set_FL.New_label==rotational_label].sample(n=num_examples_FL)\n",
    "    pulsating_class  = all_training_set_FL[all_training_set_FL.New_label==pulsating_label].sample(n=num_examples_FL)\n",
    "\n",
    "    if multi_class:\n",
    "        training_set_FL = pd.concat([eclipsing_class,rotational_class,pulsating_class], axis=0)   \n",
    "        testing_set_FL = all_testing_set_FL\n",
    "    else:\n",
    "        training_set_FL = pd.concat([eclipsing_class,pulsating_class], axis=0) \n",
    "        testing_set_FL = all_testing_set_FL\n",
    "        \n",
    "    # Performing normalisation\n",
    "    X_train_FL, y_train_FL, filename_train_np_FL, X_test_FL, y_test_FL, filename_test_np_FL = normalisation(training_set_FL,testing_set_FL)\n",
    "\n",
    "    # Random Forest Classifier    \n",
    "    if multi_class:\n",
    "        classes_types_FL = ['Eclipsing','Rotational','Pulsating']\n",
    "        types_FL         ='Type_FL_'+str(split_num)\n",
    "        nClasses_FL      = len(classes_types_FL)\n",
    "        \n",
    "    else:\n",
    "        classes_types_FL = ['Eclipsing','Rotational']\n",
    "        types_FL         ='Type_Binary_Split_'+str(split_num)\n",
    "        nClasses_FL      = 2\n",
    "\n",
    "    opt_rf_FL, fit_model_rf_FL = analysis_rf(X_train_FL, y_train_FL, types_FL, save_model) # This part can be commented if you don't want to train the algorithm\n",
    "    print(opt_rf_FL)   \n",
    "    ypred_rf_FL, accuracy_rf_FL, MCC_rf_FL, conf_mat_rf_FL = final_prediction(fit_model_rf_FL,X_train_FL, y_train_FL, X_test_FL, y_test_FL, classes_types_FL, types_FL, nClasses_FL,load_model)\n",
    "\n",
    "    # XGBoost Classifier   \n",
    "    opt_xgb_FL, fit_model_xgb_FL = analysis_XGB(X_train_FL, y_train_FL, types_FL, save_model,multi=True) # This part can be commented when no training\n",
    "    print(opt_xgb_FL)    \n",
    "    ypred_xgb_FL, accuracy_xgb_FL, MCC_xgb_FL, conf_mat_xgb_FL = final_prediction_XGB(fit_model_xgb_FL, X_train_FL, y_train_FL, X_test_FL, y_test_FL, classes_types_FL, types_FL, nClasses_FL, load_model) \n",
    "    \n",
    "    acc_rf_FL.append(accuracy_rf_FL)\n",
    "    mcc_rf_FL.append(MCC_rf_FL)\n",
    "    acc_xgb_FL.append(accuracy_xgb_FL)\n",
    "    mcc_xgb_FL.append(MCC_xgb_FL)\n",
    "        \n",
    "    '-------------------------------------------------------------------------------'\n",
    "                                # SECOND LAYER ECLIPSING BINARY\n",
    "    '-------------------------------------------------------------------------------'\n",
    "    num_examples_SL_EB     = 7200\n",
    "    training_dir_SL_EB     = './data/GP/HC/layer2_EB/training_set/Split_'+str(split_num)+'/'\n",
    "    testing_dir_SL_EB      = './data/GP/HC/layer2_EB/test_set/Split_'+str(split_num)+'/'\n",
    "    all_training_set_SL_EB = pd.read_csv(training_dir_SL_EB+'Training_features.csv',sep=',')\n",
    "    all_testing_set_SL_EB  = pd.read_csv(testing_dir_SL_EB+'Test_features.csv',sep=',')\n",
    "        \n",
    "    ecl_class  = all_training_set_SL_EB[all_training_set_SL_EB.New_label==true_class_5].sample(n=num_examples_SL_EB)\n",
    "    EA_class   = all_training_set_SL_EB[all_training_set_SL_EB.New_label==true_class_6].sample(n=num_examples_SL_EB)\n",
    "\n",
    "    training_set_SL_EB = pd.concat([ecl_class,EA_class], axis=0) \n",
    "    testing_set_SL_EB  = all_testing_set_SL_EB\n",
    "        \n",
    "    # Performing normalisation\n",
    "    X_train_SL_EB, y_train_SL_EB, filename_train_np_SL_EB, X_test_SL_EB, y_test_SL_EB, filename_test_np_SL_EB = normalisation(training_set_SL_EB,testing_set_SL_EB)\n",
    "\n",
    "    # Random Forest Classifier    \n",
    "\n",
    "    classes_types_SL_EB = ['Ecl','EA']\n",
    "    types_SL_EB         ='Type_SL_Ecl_EA_'+str(split_num)\n",
    "    nClasses_SL_EB      = 2\n",
    "\n",
    "    opt_rf_SL_EB, fit_model_rf_SL_EB = analysis_rf(X_train_SL_EB, y_train_SL_EB, types_SL_EB, save_model) # This part can be commented if you don't want to train the algorithm\n",
    "    print(opt_rf_FL)   \n",
    "    ypred_rf_SL_EB, accuracy_rf_SL_EB, MCC_rf_SL_EB, conf_mat_rf_SL_EB = final_prediction(fit_model_rf_SL_EB,X_train_SL_EB, y_train_SL_EB, X_test_SL_EB, y_test_SL_EB, classes_types_SL_EB, types_SL_EB, nClasses_SL_EB,load_model)\n",
    "\n",
    "    # XGBoost Classifier   \n",
    "    opt_xgb_SL_EB, fit_model_xgb_SL_EB = analysis_XGB(X_train_SL_EB, y_train_SL_EB, types_SL_EB, save_model,multi=False) # This part can be commented when no training\n",
    "    print(opt_xgb_SL_EB)    \n",
    "    ypred_xgb_SL_EB, accuracy_xgb_SL_EB, MCC_xgb_SL_EB, conf_mat_xgb_SL_EB = final_prediction_XGB(fit_model_xgb_SL_EB, X_train_SL_EB, y_train_SL_EB, X_test_SL_EB, y_test_SL_EB, classes_types_SL_EB, types_SL_EB, nClasses_SL_EB, load_model) \n",
    "    \n",
    "    acc_rf_SL_EB.append(accuracy_rf_SL_EB)\n",
    "    mcc_rf_SL_EB.append(MCC_rf_SL_EB)\n",
    "    acc_xgb_SL_EB.append(accuracy_xgb_SL_EB)\n",
    "    mcc_xgb_SL_EB.append(MCC_xgb_SL_EB)\n",
    "    \n",
    "    '-----------------------------------------------------------------------------'\n",
    "                        # SECOND LAYER RR LYRAE PULSATING LPV CEPHEIDS\n",
    "    '-----------------------------------------------------------------------------'\n",
    "    num_examples_SL_RLCD     = 13900\n",
    "    training_dir_SL_RLCD     = './data/GP/HC/layer2_RLCD/training_set/Split_'+str(split_num)+'/'\n",
    "    testing_dir_SL_RLCD      = './data/GP/HC/layer2_RLCD/test_set/Split_'+str(split_num)+'/'\n",
    "    all_training_set_SL_RLCD = pd.read_csv(training_dir_SL_RLCD+'Training_features.csv',sep=',')\n",
    "    all_testing_set_SL_RLCD  = pd.read_csv(testing_dir_SL_RLCD+'Test_features.csv',sep=',')\n",
    "        \n",
    "    RR_Lyrae_class = all_training_set_SL_RLCD[all_training_set_SL_RLCD.New_label==RR_Lyrae_label].sample(n=num_examples_SL_RLCD)\n",
    "    LPV_class      = all_training_set_SL_RLCD[all_training_set_SL_RLCD.New_label==LPV_label].sample(n=num_examples_SL_RLCD)\n",
    "    Cep_class      = all_training_set_SL_RLCD[all_training_set_SL_RLCD.New_label==cepheids_label].sample(n=num_examples_SL_RLCD)\n",
    "    ds_class       = all_training_set_SL_RLCD[all_training_set_SL_RLCD.New_label==delta_scuti_label].sample(n=num_examples_SL_RLCD)\n",
    "\n",
    "    training_set_SL_RLCD = pd.concat([RR_Lyrae_class,LPV_class,Cep_class,ds_class], axis=0) \n",
    "    testing_set_SL_RLCD  = all_testing_set_SL_RLCD\n",
    "        \n",
    "    # Performing normalisation\n",
    "    X_train_SL_RLCD, y_train_SL_RLCD, filename_train_np_SL_RLCD, X_test_SL_RLCD, y_test_SL_RLCD, filename_test_np_SL_RLCD = normalisation(training_set_SL_RLCD,testing_set_SL_RLCD)\n",
    "\n",
    "    # Random Forest Classifier    \n",
    "\n",
    "    classes_types_SL_RLCD = ['RR Lyrae','LPV', 'Cepheids', '$\\delta$-Scuti']\n",
    "    types_SL_RLCD         ='Type_SL_RLCD_'+str(split_num)\n",
    "    nClasses_SL_RLCD      = len(classes_types_SL_RLCD)\n",
    "\n",
    "    opt_rf_SL_RLCD, fit_model_rf_SL_RLCD = analysis_rf(X_train_SL_RLCD, y_train_SL_RLCD, types_SL_RLCD, save_model) # This part can be commented if you don't want to train the algorithm\n",
    "    print(opt_rf_SL_RLCD)   \n",
    "    ypred_rf_SL_RLCD, accuracy_rf_SL_RLCD, MCC_rf_SL_RLCD, conf_mat_rf_SL_RLCD = final_prediction(fit_model_rf_SL_RLCD,X_train_SL_RLCD, y_train_SL_RLCD, X_test_SL_RLCD, y_test_SL_RLCD, classes_types_SL_RLCD, types_SL_RLCD, nClasses_SL_RLCD,load_model)\n",
    "\n",
    "    # XGBoost Classifier   \n",
    "    opt_xgb_SL_RLCD, fit_model_xgb_SL_RLCD = analysis_XGB(X_train_SL_RLCD, y_train_SL_RLCD, types_SL_RLCD, save_model,multi=True) # This part can be commented when no training\n",
    "    print(opt_xgb_SL_RLCD)    \n",
    "    ypred_xgb_SL_RLCD, accuracy_xgb_SL_RLCD, MCC_xgb_SL_RLCD, conf_mat_xgb_SL_RLCD = final_prediction_XGB(fit_model_xgb_SL_RLCD, X_train_SL_RLCD, y_train_SL_RLCD, X_test_SL_RLCD, y_test_SL_RLCD, classes_types_SL_RLCD, types_SL_RLCD, nClasses_SL_RLCD, load_model) \n",
    "    \n",
    "    acc_rf_SL_RLCD.append(accuracy_rf_SL_RLCD)\n",
    "    mcc_rf_SL_RLCD.append(MCC_rf_SL_RLCD)\n",
    "    acc_xgb_SL_RLCD.append(accuracy_xgb_SL_RLCD)\n",
    "    mcc_xgb_SL_RLCD.append(MCC_xgb_SL_RLCD)\n",
    "    \n",
    "    '-----------------------------------------------------------------------------'\n",
    "                    # THIRD LAYER RR LYRAE: RRab, RRc, RRd, Blazhko\n",
    "    '-----------------------------------------------------------------------------'\n",
    "    num_examples_TL_RL     = 6900\n",
    "    training_dir_TL_RL     = './data/GP/HC/layer3_RRLyrae/training_set/Split_'+str(split_num)+'/'\n",
    "    testing_dir_TL_RL      = './data/GP/HC/layer3_RRLyrae/test_set/Split_'+str(split_num)+'/'\n",
    "    all_training_set_TL_RL = pd.read_csv(training_dir_TL_RL+'Training_features.csv',sep=',')\n",
    "    all_testing_set_TL_RL  = pd.read_csv(testing_dir_TL_RL+'Test_features.csv',sep=',')\n",
    "        \n",
    "    RRab_class    = all_training_set_TL_RL[all_training_set_TL_RL.New_label==true_class_1].sample(n=num_examples_TL_RL)\n",
    "    RRc_class     = all_training_set_TL_RL[all_training_set_TL_RL.New_label==true_class_2].sample(n=num_examples_TL_RL)\n",
    "    RRd_class     = all_training_set_TL_RL[all_training_set_TL_RL.New_label==true_class_3].sample(n=num_examples_TL_RL)\n",
    "    blazhko_class = all_training_set_TL_RL[all_training_set_TL_RL.New_label==true_class_4].sample(n=num_examples_TL_RL)\n",
    "\n",
    "    training_set_TL_RL = pd.concat([RRab_class,RRc_class,RRd_class,blazhko_class], axis=0) \n",
    "    testing_set_TL_RL  = all_testing_set_TL_RL\n",
    "        \n",
    "    # Performing normalisation\n",
    "    X_train_TL_RL, y_train_TL_RL, filename_train_np_TL_RL, X_test_TL_RL, y_test_TL_RL, filename_test_np_TL_RL = normalisation(training_set_TL_RL,testing_set_TL_RL)\n",
    "\n",
    "    # Random Forest Classifier    \n",
    "    classes_types_TL_RL = ['RRab', 'RRc', 'RRd', \"Blazhko\"]\n",
    "    types_TL_RL         ='Type_TL_RRLyrae_'+str(split_num)\n",
    "    nClasses_TL_RL      = len(classes_types_TL_RL)\n",
    "\n",
    "    opt_rf_TL_RL, fit_model_rf_TL_RL = analysis_rf(X_train_TL_RL, y_train_TL_RL, types_TL_RL, save_model) # This part can be commented if you don't want to train the algorithm\n",
    "    print(opt_rf_TL_RL)   \n",
    "    ypred_rf_TL_RL, accuracy_rf_TL_RL, MCC_rf_TL_RL, conf_mat_rf_TL_RL = final_prediction(fit_model_rf_TL_RL,X_train_TL_RL, y_train_TL_RL, X_test_TL_RL, y_test_TL_RL, classes_types_TL_RL, types_TL_RL, nClasses_TL_RL,load_model)\n",
    "\n",
    "    # XGBoost Classifier   \n",
    "    opt_xgb_TL_RL, fit_model_xgb_TL_RL = analysis_XGB(X_train_TL_RL, y_train_TL_RL, types_TL_RL, save_model,multi=True) # This part can be commented when no training\n",
    "    print(opt_xgb_TL_RL)    \n",
    "    ypred_xgb_TL_RL, accuracy_xgb_TL_RL, MCC_xgb_TL_RL, conf_mat_xgb_TL_RL = final_prediction_XGB(fit_model_xgb_TL_RL, X_train_TL_RL, y_train_TL_RL, X_test_TL_RL, y_test_TL_RL, classes_types_TL_RL, types_TL_RL, nClasses_TL_RL, load_model) \n",
    "    \n",
    "    acc_rf_TL_RL.append(accuracy_rf_TL_RL)\n",
    "    mcc_rf_TL_RL.append(MCC_rf_TL_RL)\n",
    "    acc_xgb_TL_RL.append(accuracy_xgb_TL_RL)\n",
    "    mcc_xgb_TL_RL.append(MCC_xgb_TL_RL)\n",
    "    \n",
    "    '-----------------------------------------------------------------------------'\n",
    "                                # THIRD LAYER Cepheids: ACEP and Cep-II\n",
    "    '-----------------------------------------------------------------------------'\n",
    "    num_examples_TL_Cep     = 4800\n",
    "    training_dir_TL_Cep     = './data/GP/HC/layer3_Cepheids/training_set/Split_'+str(split_num)+'/'\n",
    "    testing_dir_TL_Cep      = './data/GP/HC/layer3_Cepheids/test_set/Split_'+str(split_num)+'/'\n",
    "    all_training_set_TL_Cep = pd.read_csv(training_dir_TL_Cep+'Training_features.csv',sep=',')\n",
    "    all_testing_set_TL_Cep  = pd.read_csv(testing_dir_TL_Cep+'Test_features.csv',sep=',')\n",
    "        \n",
    "    ACEP_class  = all_training_set_TL_Cep[all_training_set_TL_Cep.New_label==true_class_10].sample(n=num_examples_TL_Cep)\n",
    "    Cep_ii_class   = all_training_set_TL_Cep[all_training_set_TL_Cep.New_label==true_class_12].sample(n=num_examples_TL_Cep)\n",
    "\n",
    "    training_set_TL_Cep = pd.concat([ACEP_class,Cep_ii_class], axis=0) \n",
    "    testing_set_TL_Cep  = all_testing_set_TL_Cep\n",
    "        \n",
    "    # Performing normalisation\n",
    "    X_train_TL_Cep, y_train_TL_Cep, filename_train_np_TL_Cep, X_test_TL_Cep, y_test_TL_Cep, filename_test_np_TL_Cep = normalisation(training_set_TL_Cep,testing_set_TL_Cep)\n",
    "\n",
    "    # Random Forest Classifier    \n",
    "\n",
    "    classes_types_TL_Cep = ['ACEP','CEP-II']\n",
    "    types_TL_Cep         ='Type_TL_Cepheids_'+str(split_num)\n",
    "    nClasses_TL_Cep      = 2\n",
    "\n",
    "    opt_rf_TL_Cep, fit_model_rf_TL_Cep = analysis_rf(X_train_TL_Cep, y_train_TL_Cep, types_TL_Cep, save_model) # This part can be commented if you don't want to train the algorithm\n",
    "    print(opt_rf_TL_Cep)   \n",
    "    ypred_rf_TL_Cep, accuracy_rf_TL_Cep, MCC_rf_TL_Cep, conf_mat_rf_TL_Cep = final_prediction(fit_model_rf_TL_Cep,X_train_TL_Cep, y_train_TL_Cep, X_test_TL_Cep, y_test_TL_Cep, classes_types_TL_Cep, types_TL_Cep, nClasses_TL_Cep,load_model)\n",
    "\n",
    "    # XGBoost Classifier   \n",
    "    opt_xgb_TL_Cep, fit_model_xgb_TL_Cep = analysis_XGB(X_train_TL_Cep, y_train_TL_Cep, types_TL_Cep, save_model,multi=False) # This part can be commented when no training\n",
    "    print(opt_xgb_TL_Cep)    \n",
    "    ypred_xgb_TL_Cep, accuracy_xgb_TL_Cep, MCC_xgb_TL_Cep, conf_mat_xgb_TL_Cep = final_prediction_XGB(fit_model_xgb_TL_Cep, X_train_TL_Cep, y_train_TL_Cep, X_test_TL_Cep, y_test_TL_Cep, classes_types_TL_Cep, types_TL_Cep, nClasses_TL_Cep, load_model) \n",
    "    \n",
    "    acc_rf_TL_Cep.append(accuracy_rf_TL_Cep)\n",
    "    mcc_rf_TL_Cep.append(MCC_rf_TL_Cep)\n",
    "    acc_xgb_TL_Cep.append(accuracy_xgb_TL_Cep)\n",
    "    mcc_xgb_TL_Cep.append(MCC_xgb_TL_Cep)\n",
    "    \n",
    "    \n",
    "    print('-'*50)\n",
    "    print('Classification for Split {} is finished'.format(split_num))\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "metrics = open(\"./hierarchical-results_gp/metrics.txt\", 'w')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase Random Forest for ' + str(classes_types_FL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_rf_FL)*100,np.std(acc_rf_FL)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_rf_FL)*100,np.std(mcc_rf_FL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase Random Forest for ' + str(classes_types_SL_EB) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_rf_SL_EB)*100,np.std(acc_rf_SL_EB)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_rf_SL_EB)*100,np.std(mcc_rf_SL_EB)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase Random Forest for ' + str(classes_types_SL_RLCD) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_rf_SL_RLCD)*100,np.std(acc_rf_SL_RLCD)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_rf_SL_RLCD)*100,np.std(mcc_rf_SL_RLCD)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase Random Forest for ' + str(classes_types_TL_RL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_rf_TL_RL)*100,np.std(acc_rf_TL_RL)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_rf_TL_RL)*100,np.std(mcc_rf_TL_RL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase Random Forest for ' + str(classes_types_TL_Cep) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_rf_TL_Cep)*100,np.std(acc_rf_TL_Cep)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_rf_TL_Cep)*100,np.std(mcc_rf_TL_Cep)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_FL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_xgb_FL)*100,np.std(acc_xgb_FL)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_xgb_FL)*100,np.std(mcc_xgb_FL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_SL_EB) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_xgb_SL_EB)*100,np.std(acc_xgb_SL_EB)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_xgb_SL_EB)*100,np.std(mcc_xgb_SL_EB)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_SL_RLCD) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_xgb_SL_RLCD)*100,np.std(acc_xgb_SL_RLCD)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_xgb_SL_RLCD)*100,np.std(mcc_xgb_SL_RLCD)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_TL_RL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_xgb_TL_RL)*100,np.std(acc_xgb_TL_RL)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_xgb_TL_RL)*100,np.std(mcc_xgb_TL_RL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_TL_Cep) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_xgb_TL_Cep)*100,np.std(acc_xgb_TL_Cep)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_xgb_TL_Cep)*100,np.std(mcc_xgb_TL_Cep)) + '\\n')\n",
    "metrics.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
