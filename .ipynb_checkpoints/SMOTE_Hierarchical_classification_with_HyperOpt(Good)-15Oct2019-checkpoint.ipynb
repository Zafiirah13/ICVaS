{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good Code for Hierarchical Classification\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "#plt.switch_backend('agg')\n",
    "% matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#import FATS\n",
    "from keras.utils import np_utils\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "figSize  = (12, 8)\n",
    "fontSize = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "http://rikunert.com/SMOTE_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from scipy import interp\n",
    "from itertools import cycle, islice\n",
    "\n",
    "\n",
    "# Some preprocessing utilities\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.manifold.t_sne import TSNE\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE,ADASYN\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n",
    "\n",
    "# The different classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix,balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stars_label(data, label):\n",
    "    '''Set variable names to specific class label'''\n",
    "    stars = data[data.True_class_labels == label]\n",
    "    return stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Layer Hierarchical Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_layer():\n",
    "    '''\n",
    "    We define first layer of the hierarchical tree. The first layer consists of Eclipsing Binaries, Rotational,\n",
    "    and Pulsating \n",
    "    '''\n",
    "    \n",
    "    # First Layer\n",
    "    eclipsing_binary_train       = pd.concat([contact_Bi_train, semi_det_Bi_train], axis=0)\n",
    "    eclipsing_binary_train_class = np.full(len(eclipsing_binary_train), eclipsing_label, dtype=int)\n",
    "\n",
    "    rotational_train       = rot_train\n",
    "    rotational_train_class = np.full(len(rotational_train),rotational_label, dtype=int)\n",
    "\n",
    "    pulsating_train       = pd.concat([RRab_train, RRc_train, RRd_train, blazhko_train, LPV_train, delta_scuti_train, ACEP_train, cep_ii_train] ,axis=0)\n",
    "    pulsating_train_class = np.full(len(pulsating_train), pulsating_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"eclipsing_binary_train has {}\".format(eclipsing_binary_train.shape))\n",
    "    print(\"pulsating_train has {}\".format(pulsating_train.shape))\n",
    "    print(\"rotational_train has {}\".format(rotational_train.shape))\n",
    "\n",
    "    eclipsing_binary_test       = pd.concat([contact_Bi_test, semi_det_Bi_test], axis=0)\n",
    "    eclipsing_binary_test_class = np.full(len(eclipsing_binary_test), eclipsing_label, dtype=int)\n",
    "\n",
    "    rotational_test       = rot_test\n",
    "    rotational_test_class = np.full(len(rotational_test), rotational_label, dtype=int)\n",
    "\n",
    "    pulsating_test       = pd.concat([RRab_test, RRc_test, RRd_test, blazhko_test, LPV_test, delta_scuti_test, ACEP_test, cep_ii_test] ,axis=0)\n",
    "    pulsating_test_class = np.full(len(pulsating_test), pulsating_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"eclipsing_binary_test has {}\".format(eclipsing_binary_test.shape))\n",
    "    print(\"pulsating_test has {}\".format(pulsating_test.shape))\n",
    "    print(\"rotational_test has {}\".format(rotational_test.shape))\n",
    "    \n",
    "    first_layer_train       = pd.concat([eclipsing_binary_train, rotational_train, pulsating_train], axis=0)\n",
    "    first_layer_train_class = np.concatenate((eclipsing_binary_train_class, rotational_train_class, pulsating_train_class), axis=0)\n",
    "    training_data_FL        = pd.DataFrame(first_layer_train)\n",
    "    training_data_FL['New_label'] = first_layer_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    first_layer_test       = pd.concat([eclipsing_binary_test, rotational_test, pulsating_test], axis=0)\n",
    "    first_layer_test_class = np.concatenate((eclipsing_binary_test_class, rotational_test_class, pulsating_test_class), axis=0)\n",
    "    testing_data_FL        = pd.DataFrame(first_layer_test)\n",
    "    testing_data_FL['New_label'] = first_layer_test_class\n",
    "    \n",
    "    y_FL_training, y_FL_training_counts = np.unique(first_layer_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_FL, testing_data_FL, y_FL_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Layer Hierarchical level for first Branch: Eclipsing Binaries (Ecl & EA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def second_layer_EB():\n",
    "    \n",
    "    # Second Layer Eclipsing Binary    \n",
    "    ecl_train = contact_Bi_train\n",
    "    ecl_train_class = np.full(len(ecl_train), true_class_5, dtype=int)\n",
    "\n",
    "    EA_train       = semi_det_Bi_train\n",
    "    EA_train_class = np.full(len(EA_train),true_class_6, dtype=int)\n",
    " \n",
    "    print(\"ecl train has {}\".format(ecl_train.shape))\n",
    "    print(\"EA_train has {}\".format(EA_train.shape))\n",
    "\n",
    "    ecl_test       = contact_Bi_test\n",
    "    ecl_test_class = np.full(len(ecl_test), true_class_5, dtype=int)\n",
    "\n",
    "    EA_test       = semi_det_Bi_test\n",
    "    EA_test_class = np.full(len(EA_test), true_class_6, dtype=int)\n",
    "\n",
    "    print(\"ecl_test has {}\".format(ecl_test.shape))\n",
    "    print(\"EA_test has {}\".format(EA_test.shape))\n",
    "\n",
    "    \n",
    "    second_layer_EB_train       = pd.concat([ecl_train, EA_train], axis=0)\n",
    "    second_layer_EB_train_class = np.concatenate((ecl_train_class,EA_train_class), axis=0)\n",
    "    training_data_SL_EB         = pd.DataFrame(second_layer_EB_train)\n",
    "    training_data_SL_EB['New_label'] = second_layer_EB_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    second_layer_EB_test       = pd.concat([ecl_test, EA_test], axis=0)\n",
    "    second_layer_EB_test_class = np.concatenate((ecl_test_class, EA_test_class), axis=0)\n",
    "    testing_data_SL_EB         = pd.DataFrame(second_layer_EB_test)\n",
    "    testing_data_SL_EB['New_label'] = second_layer_EB_test_class\n",
    "    \n",
    "    y_SL_EB_training, y_SL_EB_training_counts = np.unique(second_layer_EB_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_SL_EB, testing_data_SL_EB, y_SL_EB_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Layer Hierarchical level for 2nd Branch: RLCD\n",
    "### RR Lyrae, LPV, Cepheid and $\\delta$-Scuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 2 RR Lyrae, LPV, Cepheid, Delta-Scuti\n",
    "def second_layer_RLCD():\n",
    "    \n",
    "    # First Layer\n",
    "    RR_Lyrae_train       = pd.concat([RRab_train,RRc_train,RRd_train,blazhko_train], axis=0)\n",
    "    RR_Lyrae_train_class = np.full(len(RR_Lyrae_train), RR_Lyrae_label, dtype=int)\n",
    "\n",
    "    LPV_train_class = np.full(len(LPV_train),LPV_label, dtype=int)\n",
    "\n",
    "    cepheids_train       = pd.concat([ACEP_train,cep_ii_train] ,axis=0)\n",
    "    cepheids_train_class = np.full(len(cepheids_train), cepheids_label, dtype=int)\n",
    "    \n",
    "    ds_train       = delta_scuti_train\n",
    "    ds_train_class = np.full(len(ds_train), delta_scuti_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"RR Lyrae train has {}\".format(RR_Lyrae_train.shape))\n",
    "    print(\"LPV train has {}\".format(LPV_train.shape))\n",
    "    print(\"Cepheids train has {}\".format(cepheids_train.shape))\n",
    "    print(\"Delta Scuti train has {}\".format(ds_train.shape))\n",
    "\n",
    "    RR_Lyrae_test       = pd.concat([RRab_test,RRc_test,RRd_test,blazhko_test], axis=0)\n",
    "    RR_Lyrae_test_class = np.full(len(RR_Lyrae_test), RR_Lyrae_label, dtype=int)\n",
    "\n",
    "    LPV_test_class = np.full(len(LPV_test), LPV_label, dtype=int)\n",
    "\n",
    "    cepheids_test       = pd.concat([ACEP_test, cep_ii_test] ,axis=0)\n",
    "    cepheids_test_class = np.full(len(cepheids_test), cepheids_label, dtype=int)\n",
    "    \n",
    "    ds_test       = delta_scuti_test\n",
    "    ds_test_class = np.full(len(ds_test), delta_scuti_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"RR_Lyrae_test has {}\".format(RR_Lyrae_test.shape))\n",
    "    print(\"LPV_test has {}\".format(LPV_test.shape))\n",
    "    print(\"cepheids_test has {}\".format(cepheids_test.shape))\n",
    "    print(\"Delta Scuti test has {}\".format(ds_test.shape))\n",
    "    \n",
    "    second_layer_RLCD_train       = pd.concat([RR_Lyrae_train,LPV_train,cepheids_train,ds_train], axis=0)\n",
    "    second_layer_RLCD_train_class = np.concatenate((RR_Lyrae_train_class,LPV_train_class,cepheids_train_class,ds_train_class), axis=0)\n",
    "    training_data_SL_RLCD         = pd.DataFrame(second_layer_RLCD_train)\n",
    "    training_data_SL_RLCD['New_label'] = second_layer_RLCD_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    second_layer_RLCD_test       = pd.concat([RR_Lyrae_test,LPV_test,cepheids_test,ds_test], axis=0)\n",
    "    second_layer_RLCD_test_class = np.concatenate((RR_Lyrae_test_class,LPV_test_class,cepheids_test_class,ds_test_class), axis=0)\n",
    "    testing_data_SL_RLCD         = pd.DataFrame(second_layer_RLCD_test)\n",
    "    testing_data_SL_RLCD['New_label'] = second_layer_RLCD_test_class\n",
    "    \n",
    "    y_SL_RLCD_training, y_SL_RLCD_training_counts = np.unique(second_layer_RLCD_train_class, return_counts=True)\n",
    "\n",
    "    print(y_SL_RLCD_training)\n",
    "    print(y_SL_RLCD_training_counts)\n",
    "    return training_data_SL_RLCD, testing_data_SL_RLCD, y_SL_RLCD_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Layer Hierarchical level for first Branch: RRLyrae\n",
    "### RRab, RRc, RRd, and Blazhko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 3 RR Lyrae classes\n",
    "def third_layer_RRLyrae():\n",
    "    \n",
    "    # Third Layer\n",
    "    RRab_train_class    = np.full(len(RRab_train), true_class_1, dtype=int)\n",
    "    RRc_train_class     = np.full(len(RRc_train), true_class_2, dtype=int)\n",
    "    RRd_train_class     = np.full(len(RRd_train), true_class_3, dtype=int)\n",
    "    blazhko_train_class = np.full(len(blazhko_train), true_class_4, dtype=int)\n",
    "\n",
    "    print(\"RRab train has {}\".format(RRab_train.shape))\n",
    "    print(\"RRc train has {}\".format(RRc_train.shape))\n",
    "    print(\"RRd train has {}\".format(RRd_train.shape))\n",
    "    print(\"Blazhko train has {}\".format(blazhko_train.shape))\n",
    "    \n",
    "    RRab_test_class    = np.full(len(RRab_test), true_class_1, dtype=int)\n",
    "    RRc_test_class     = np.full(len(RRc_test), true_class_2, dtype=int)\n",
    "    RRd_test_class     = np.full(len(RRd_test), true_class_3, dtype=int)\n",
    "    blazhko_test_class = np.full(len(blazhko_test), true_class_4, dtype=int)\n",
    "\n",
    "    print(\"RRab test has {}\".format(RRab_test.shape))\n",
    "    print(\"RRc test has {}\".format(RRc_test.shape))\n",
    "    print(\"RRd test has {}\".format(RRd_test.shape))\n",
    "    print(\"Blazhko test has {}\".format(blazhko_test.shape))\n",
    "\n",
    "    \n",
    "    third_layer_RRLyrae_train       = pd.concat([RRab_train,RRc_train,RRd_train,blazhko_train], axis=0)\n",
    "    third_layer_RRLyrae_train_class = np.concatenate((RRab_train_class,RRc_train_class,RRd_train_class,blazhko_train_class), axis=0)\n",
    "    training_data_TL_RRLyrae        = pd.DataFrame(third_layer_RRLyrae_train)\n",
    "    training_data_TL_RRLyrae['New_label'] = third_layer_RRLyrae_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    third_layer_RRLyrae_test       = pd.concat([RRab_test,RRc_test,RRd_test,blazhko_test], axis=0)\n",
    "    third_layer_RRLyrae_test_class = np.concatenate((RRab_test_class,RRc_test_class,RRd_test_class,blazhko_test_class), axis=0)\n",
    "    testing_data_TL_RRLyrae         = pd.DataFrame(third_layer_RRLyrae_test)\n",
    "    testing_data_TL_RRLyrae['New_label'] = third_layer_RRLyrae_test_class\n",
    "    \n",
    "    y_TL_RRLyrae_training, y_TL_RRLyrae_training_counts = np.unique(third_layer_RRLyrae_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_TL_RRLyrae, testing_data_TL_RRLyrae, y_TL_RRLyrae_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Layer Hierarchical level for 2nd Branch: Cepheids\n",
    "### ACEP and Cep-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 3 RR Lyrae classes\n",
    "def third_layer_Cepheids():\n",
    "    \n",
    "    # Third Layer\n",
    "    ACEP_train_class   = np.full(len(ACEP_train), true_class_10, dtype=int)\n",
    "    cep_ii_train_class = np.full(len(cep_ii_train), true_class_12, dtype=int)\n",
    "\n",
    "    print(\"ACEP train has {}\".format(ACEP_train.shape))\n",
    "    print(\"Cep-II train has {}\".format(cep_ii_train.shape))\n",
    "\n",
    "\n",
    "    ACEP_test_class   = np.full(len(ACEP_test), true_class_10, dtype=int)\n",
    "    cep_ii_test_class = np.full(len(cep_ii_test), true_class_12, dtype=int)\n",
    "\n",
    "    print(\"ACEP test has {}\".format(ACEP_test.shape))\n",
    "    print(\"Cep-II test has {}\".format(cep_ii_test.shape))\n",
    "    \n",
    "    third_layer_cep_train       = pd.concat([ACEP_train,cep_ii_train], axis=0)\n",
    "    third_layer_cep_train_class = np.concatenate((ACEP_train_class,cep_ii_train_class), axis=0)\n",
    "    training_data_TL_cep        = pd.DataFrame(third_layer_cep_train)\n",
    "    training_data_TL_cep['New_label'] = third_layer_cep_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    third_layer_cep_test       = pd.concat([ACEP_test,cep_ii_test], axis=0)\n",
    "    third_layer_cep_test_class = np.concatenate((ACEP_test_class,cep_ii_test_class), axis=0)\n",
    "    testing_data_TL_cep        = pd.DataFrame(third_layer_cep_test)\n",
    "    testing_data_TL_cep['New_label'] = third_layer_cep_test_class\n",
    "    \n",
    "    y_TL_cep_training, y_TL_cep_training_counts = np.unique(third_layer_cep_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_TL_cep, testing_data_TL_cep, y_TL_cep_training_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalisation(x_train,x_test,label):\n",
    "    scaler                = StandardScaler().fit(x_train.iloc[:,0:nFeatures])\n",
    "    X_train_normalisation = pd.DataFrame(scaler.transform(x_train.iloc[:,0:nFeatures]))\n",
    "    y_train_label         = x_train.New_label\n",
    "    filename_train        = x_train.File_Name\n",
    "\n",
    "    X_test_normalisation = pd.DataFrame(scaler.transform(x_test.iloc[:,0:nFeatures]))\n",
    "    y_test_label         = x_test[label]\n",
    "    filename_test        = x_test.File_Name\n",
    "    \n",
    "    # A check to see whether the mean of x_train and X_test are ~ 0 with std 1.0\n",
    "#     print(X_train_normalisation.mean(axis=0))\n",
    "#     print(X_train_normalisation.std(axis=0))\n",
    "#     print(X_test_normalisation.mean(axis=0))\n",
    "#     print(X_test_normalisation.std(axis=0))\n",
    "    \n",
    "    return X_train_normalisation, y_train_label, filename_train, X_test_normalisation,\\\n",
    "           y_test_label, filename_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gridsearch(X_train,y_train,classifer, param_grid, n_iter, cv, filename='./results'):\n",
    "    grid  = RandomizedSearchCV(classifer, param_grid, n_iter = n_iter, cv = cv, scoring = \"accuracy\", n_jobs = -1,random_state=1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    opt_parameters = grid.best_params_\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    params_file = open(filename, 'w')\n",
    "    params_file.write(str(grid.best_params_))\n",
    "    params_file.close()\n",
    "    return opt_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "def model_save(classifier_optimize, X_train, y_train, filename_model, save_model=False):\n",
    "    fit_model      = classifier_optimize.fit(X_train, y_train)\n",
    "    \n",
    "    if save_model:\n",
    "        pickle.dump(fit_model, open(filename_model, 'wb'))\n",
    "        \n",
    "    return fit_model\n",
    "\n",
    "def model_fit(fit_model, filename_model, X_train, y_train, X_test, y_test, classifier_model='Random Forest Classifier',classes=[\"Type 1\" , \"Type 2\"], filename ='./results/',load_model=False):\n",
    "    if load_model:\n",
    "        fit_model      = pickle.load(open(filename_model, 'rb'))\n",
    "    \n",
    "    else:\n",
    "        fit_model = fit_model\n",
    "        \n",
    "    ypred          = fit_model.predict(X_test)\n",
    "    probability    = fit_model.predict_proba(X_test)\n",
    "    accuracy       = accuracy_score(y_test, ypred)\n",
    "    MCC            = matthews_corrcoef(y_test, ypred)\n",
    "    conf_mat       = confusion_matrix(y_test, ypred)\n",
    "    balance_accuracy = balanced_accuracy_score(y_test, ypred)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(y_test)\n",
    "    yTest = np_utils.to_categorical(labels,len(classes))\n",
    "    auc_value      = roc_auc_score(yTest,probability)\n",
    "    \n",
    "    misclassified     = np.where(y_test != ypred)[0]\n",
    " \n",
    "    \n",
    "    name_file = open(filename + \".txt\", 'w')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('******* Testing Phase '+ str(classifier_model) +' for ' + str(classes) + ' *******\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(\"Accuracy: \"                    + \"%f\" % float(accuracy) + '\\n')\n",
    "    name_file.write(\"Mathews Correlation Coef: \"    + \"%f\" % float(MCC)      + '\\n')\n",
    "    name_file.write(\"Balanced Accuracy: \"    + \"%f\" % float(balance_accuracy)      + '\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('Classification Report\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(classification_report(y_test, ypred, target_names = classes)+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('Classification Report using imabalanced metrics\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(classification_report_imbalanced(y_test, ypred, target_names = classes)+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.close()\n",
    "        \n",
    "    return ypred, balance_accuracy, MCC, conf_mat, misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_misclassification(misclassified,y_test,test_data,ypred, save_dir=r'c:\\data\\np.txt'):\n",
    "    test_data['Prediction'] = ypred\n",
    "    new_DF = test_data.drop(test_data.index[misclassified]) # This dataset is the test set after removing the misclassification which are used in the next layer   \n",
    "    misclassified_data = test_data.iloc[misclassified] # Dataframe to store all misclassification\n",
    "    misclassified_data.to_csv(save_dir, sep=' ',index=None)\n",
    "    print('Test set has shape {}'.format(test_data.shape))\n",
    "    print('Misclassified data has shape {}'.format(misclassified_data.shape))\n",
    "    print('New test set has shape {}'.format(new_DF.shape))\n",
    "    return misclassified_data, new_DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_ROC_curve(X_test, y_test, nClasses, fit_model,plots_dir,classes_types):\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(y_test)\n",
    "    yTest = np_utils.to_categorical(labels, nClasses)\n",
    "    preds = fit_model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(nClasses):\n",
    "        fpr[i], tpr[i], _ = roc_curve(yTest[:,i], preds[:,i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(nClasses)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(nClasses):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= nClasses\n",
    "    \n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(yTest.ravel(),preds.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linewidth=4)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2.0)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate',fontsize=fontSize)\n",
    "    plt.ylabel('True Positive Rate',fontsize=fontSize)\n",
    "    plt.tick_params(axis='both', labelsize=fontSize)\n",
    "    plt.legend(loc=\"best\",prop={'size':14},bbox_to_anchor=(1,0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir+'_ROC.pdf',bbox_inches = 'tight',pad_inches = 0.1)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    colors = cycle(['b', 'darkorange', 'gold', 'purple', 'y', 'brown', 'pink','cornflowerblue', 'olive','green', 'r', ])\n",
    "\n",
    "    #colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(nClasses), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2.0,\n",
    "                 label='{0} (area = {1:0.2f})'\n",
    "                 ''.format(classes_types[i], roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2.0)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate',fontsize=fontSize)\n",
    "    plt.ylabel('True Positive Rate',fontsize=fontSize)\n",
    "    plt.tick_params(axis='both', labelsize=fontSize)\n",
    "    \n",
    "    plt.legend(loc=\"best\",prop={'size':14},bbox_to_anchor=(1,0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plots_dir+'_all_ROC.pdf',bbox_inches = 'tight',pad_inches = 0.1)\n",
    "    plt.show()\n",
    "    \n",
    "    return fpr,tpr,roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes_types,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Reds):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(9,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=16)\n",
    "    cb=plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    cb.ax.tick_params(labelsize=16)\n",
    "    tick_marks = np.arange(len(classes_types))\n",
    "    plt.xticks(tick_marks, classes_types, rotation=45)\n",
    "    plt.yticks(tick_marks, classes_types)\n",
    "    plt.tick_params(axis='x', labelsize=16)\n",
    "    plt.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if (cm[i, j] < 0.01) or (cm[i,j] >= 0.75)  else \"black\",fontsize=18)\n",
    "\n",
    "    \n",
    "    plt.ylabel('True label',fontsize = 16)\n",
    "    plt.xlabel('Predicted label', fontsize = 16)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(conf_mat, classes_types, classifier_model, plot_title, X_test, y_test, nClasses,cmap=plt.cm.Reds):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    plot_confusion_matrix(conf_mat, classes_types, normalize=True, title='Confusion matrix for ' + str(classifier_model) )\n",
    "    plt.savefig(plot_title +'_CM.pdf',bbox_inches = 'tight',pad_inches = 0.1)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smote_augmentation(training,testing,label,aug_tech='ADASYN'):\n",
    "    X_train_normalisation, y_train_np, filename_train, X_test_normalisation,\\\n",
    "    y_test_np, filename_test = normalisation(training,testing,label) \n",
    "    \n",
    "    \n",
    "    y_label_bf = np.unique(y_train_np)\n",
    "    if augmentation:\n",
    "        for i in range(len(y_label_bf)):\n",
    "            print(\"Before OverSampling, counts of label {}: {}\".format(y_label_bf[i],(y_train_np[y_train_np==y_label_bf[i]]).shape))\n",
    "\n",
    "        if (aug_tech == 'ADASYN'):\n",
    "            ada          = ADASYN(ratio ='all')#\n",
    "            X_train_aug, y_train_aug = ada.fit_sample(X_train_normalisation, y_train_np.ravel())\n",
    "            data_1 = pd.DataFrame(X_train_aug)\n",
    "            data_1['True_class_labels'] = y_train_aug\n",
    "            X_train_norm = data_1.iloc[:,0:nFeatures]\n",
    "            y_train_norm = data_1.iloc[:,nFeatures]\n",
    "        \n",
    "        \n",
    "        else:\n",
    "\n",
    "            # sm = SMOTE(random_state=2, ratio = 1.0,kind='svm')\n",
    "            sm = SMOTE(ratio = 'all')\n",
    "            X_train_aug, y_train_aug = sm.fit_sample(X_train_normalisation, y_train_np.ravel())\n",
    "            data_1 = pd.DataFrame(X_train_aug)\n",
    "            data_1['True_class_labels'] = y_train_aug\n",
    "            X_train_norm = data_1.iloc[:,0:nFeatures]\n",
    "            y_train_norm = data_1.iloc[:,nFeatures]\n",
    "\n",
    "\n",
    "        y_label_af = np.unique(y_train_norm)\n",
    "        print('-'*70)\n",
    "        for j in range(len(y_label_af)):\n",
    "                print(\"After OverSampling, counts of label {}: {}\".format(y_label_af[j],y_train_norm.loc[y_train_norm==y_label_af[j]].shape))\n",
    "\n",
    "\n",
    "        X_train = X_train_norm   \n",
    "        y_train = y_train_norm\n",
    "        y_test  = y_test_np\n",
    "        X_test  = X_test_normalisation\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nFeatures = 7\n",
    "\n",
    "# The directory to save the files\n",
    "plots_dir                 = './hierarchical-results_SMOTE/plots/'\n",
    "results_dir               = './hierarchical-results_SMOTE/results/'\n",
    "misclassify_dir           = r'./hierarchical-results_SMOTE/results/Misclassification_'\n",
    "\n",
    "\n",
    "eclipsing_label = 20;rotational_label = 21;pulsating_label = 22;RR_Lyrae_label = 23; LPV_label = 24;\\\n",
    "delta_scuti_label = 25; cepheids_label   = 26\n",
    "\n",
    "true_class_1=1;true_class_2=2;true_class_3=3;true_class_4=4;true_class_5=5;true_class_6=6;true_class_7=7;\\\n",
    "true_class_8=8;true_class_9=9;true_class_10=10;true_class_11=11;true_class_12=12;true_class_13=13\n",
    "n_splits         = 5\n",
    "data_preparation = False\n",
    "multi_class      = True\n",
    "augmentation     = True\n",
    "save_model       = False\n",
    "load_model       = False\n",
    "\n",
    "\n",
    "\n",
    "acc_xgb_FL = [];mcc_xgb_FL = [];acc_rf_FL = [];mcc_rf_FL = [];\\\n",
    "acc_xgb_SL_EB = [];mcc_xgb_SL_EB = [];acc_rf_SL_EB = [];mcc_rf_SL_EB = [];\\\n",
    "acc_xgb_SL_RLCD = [];mcc_xgb_SL_RLCD = [];acc_rf_SL_RLCD = [];mcc_rf_SL_RLCD = [];\\\n",
    "acc_xgb_TL_RL = [];mcc_xgb_TL_RL = [];acc_rf_TL_RL = [];mcc_rf_TL_RL = [];\\\n",
    "acc_xgb_TL_Cep = [];mcc_xgb_TL_Cep = [];acc_rf_TL_Cep = [];mcc_rf_TL_Cep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set contains 16195 samples\n",
      "The test set contains 21242 samples\n"
     ]
    }
   ],
   "source": [
    "train_dir = './data/training_set_features/'\n",
    "test_dir  = './data/test_set_features/'\n",
    "\n",
    "# Load features for the different variable stars\n",
    "features_1_tr  = pd.read_csv(train_dir+'Type'+str(true_class_1)+'_features.csv')\n",
    "features_2_tr  = pd.read_csv(train_dir+'Type'+str(true_class_2)+'_features.csv')\n",
    "features_3_tr  = pd.read_csv(train_dir+'Type'+str(true_class_3)+'_features.csv')\n",
    "features_4_tr  = pd.read_csv(train_dir+'Type'+str(true_class_4)+'_features.csv')\n",
    "features_5_tr  = pd.read_csv(train_dir+'Type'+str(true_class_5)+'_features.csv')\n",
    "features_6_tr  = pd.read_csv(train_dir+'Type'+str(true_class_6)+'_features.csv')\n",
    "features_7_tr  = pd.read_csv(train_dir+'Type'+str(true_class_7)+'_features.csv')\n",
    "features_8_tr  = pd.read_csv(train_dir+'Type'+str(true_class_8)+'_features.csv')\n",
    "features_9_tr  = pd.read_csv(train_dir+'Type'+str(true_class_9)+'_features.csv')\n",
    "features_10_tr = pd.read_csv(train_dir+'Type'+str(true_class_10)+'_features.csv')\n",
    "features_12_tr = pd.read_csv(train_dir+'Type'+str(true_class_12)+'_features.csv')\n",
    "\n",
    "features_1_te  = pd.read_csv(test_dir+'Type'+str(true_class_1)+'_features.csv')\n",
    "features_2_te  = pd.read_csv(test_dir+'Type'+str(true_class_2)+'_features.csv')\n",
    "features_3_te  = pd.read_csv(test_dir+'Type'+str(true_class_3)+'_features.csv')\n",
    "features_4_te  = pd.read_csv(test_dir+'Type'+str(true_class_4)+'_features.csv')\n",
    "features_5_te  = pd.read_csv(test_dir+'Type'+str(true_class_5)+'_features.csv')\n",
    "features_6_te  = pd.read_csv(test_dir+'Type'+str(true_class_6)+'_features.csv')\n",
    "features_7_te  = pd.read_csv(test_dir+'Type'+str(true_class_7)+'_features.csv')\n",
    "features_8_te  = pd.read_csv(test_dir+'Type'+str(true_class_8)+'_features.csv')\n",
    "features_9_te  = pd.read_csv(test_dir+'Type'+str(true_class_9)+'_features.csv')\n",
    "features_10_te = pd.read_csv(test_dir+'Type'+str(true_class_10)+'_features.csv')\n",
    "features_12_te = pd.read_csv(test_dir+'Type'+str(true_class_12)+'_features.csv')\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_training = shuffle(pd.concat([features_1_tr,features_2_tr,features_3_tr,features_4_tr,features_5_tr,\\\n",
    "            features_6_tr,features_7_tr,features_8_tr,features_9_tr,features_10_tr,features_12_tr],\\\n",
    "            ignore_index=True))\n",
    "\n",
    "\n",
    "X_testing = shuffle(pd.concat([features_1_te,features_2_te,features_3_te,features_4_te,features_5_te,\\\n",
    "            features_6_te,features_7_te,features_8_te,features_9_te,features_10_te,features_12_te],\\\n",
    "            ignore_index=True))\n",
    "\n",
    "print('The training set contains {} samples'.format(X_training.shape[0]))\n",
    "print('The test set contains {} samples'.format(X_testing.shape[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>RA</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Period</th>\n",
       "      <th>V_CSS</th>\n",
       "      <th>Npts</th>\n",
       "      <th>V_amp</th>\n",
       "      <th>Type</th>\n",
       "      <th>Prior_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3039001008411</td>\n",
       "      <td>0.10758</td>\n",
       "      <td>-39.61419</td>\n",
       "      <td>0.514380</td>\n",
       "      <td>14.709183</td>\n",
       "      <td>230</td>\n",
       "      <td>0.260621</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3041001010523</td>\n",
       "      <td>0.13228</td>\n",
       "      <td>-41.48192</td>\n",
       "      <td>0.589281</td>\n",
       "      <td>19.164993</td>\n",
       "      <td>145</td>\n",
       "      <td>0.829397</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3025001011039</td>\n",
       "      <td>0.15796</td>\n",
       "      <td>-25.18378</td>\n",
       "      <td>0.616695</td>\n",
       "      <td>17.559777</td>\n",
       "      <td>147</td>\n",
       "      <td>0.485934</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3043001016067</td>\n",
       "      <td>0.18672</td>\n",
       "      <td>-43.13301</td>\n",
       "      <td>0.333486</td>\n",
       "      <td>15.206383</td>\n",
       "      <td>212</td>\n",
       "      <td>0.371547</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3029001015115</td>\n",
       "      <td>0.20548</td>\n",
       "      <td>-28.81702</td>\n",
       "      <td>0.583042</td>\n",
       "      <td>14.818986</td>\n",
       "      <td>196</td>\n",
       "      <td>0.832350</td>\n",
       "      <td>1</td>\n",
       "      <td>TZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       File_Name       RA       Dec    Period      V_CSS  Npts     V_amp  \\\n",
       "0  3039001008411  0.10758 -39.61419  0.514380  14.709183   230  0.260621   \n",
       "1  3041001010523  0.13228 -41.48192  0.589281  19.164993   145  0.829397   \n",
       "2  3025001011039  0.15796 -25.18378  0.616695  17.559777   147  0.485934   \n",
       "3  3043001016067  0.18672 -43.13301  0.333486  15.206383   212  0.371547   \n",
       "4  3029001015115  0.20548 -28.81702  0.583042  14.818986   196  0.832350   \n",
       "\n",
       "   Type Prior_ID  \n",
       "0     5      NaN  \n",
       "1     1      NaN  \n",
       "2     1      NaN  \n",
       "3     2      NaN  \n",
       "4     1       TZ  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_data = pd.read_csv('../SSS_Per_Tab.txt', delim_whitespace=True,\\\n",
    "names=[\"File_Name\", \"RA\", \"Dec\", \"Period\", \"V_CSS\", \"Npts\", \"V_amp\", \"Type\", \"Prior_ID\"])\n",
    "\n",
    "period_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "periods = period_data[['File_Name', 'Period']]\n",
    "X_training['File_Name'] = X_training['File_Name'].astype(int)\n",
    "X_testing['File_Name'] = X_testing['File_Name'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>Period</th>\n",
       "      <th>File_Name</th>\n",
       "      <th>True_class_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9452</th>\n",
       "      <td>1.020265</td>\n",
       "      <td>15.028564</td>\n",
       "      <td>0.098795</td>\n",
       "      <td>1.324000</td>\n",
       "      <td>0.204075</td>\n",
       "      <td>0.006574</td>\n",
       "      <td>0.435117</td>\n",
       "      <td>3021041040656</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>-0.241530</td>\n",
       "      <td>14.085678</td>\n",
       "      <td>0.224534</td>\n",
       "      <td>-1.364468</td>\n",
       "      <td>0.346250</td>\n",
       "      <td>0.015941</td>\n",
       "      <td>0.697790</td>\n",
       "      <td>3047064090676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7986</th>\n",
       "      <td>0.092216</td>\n",
       "      <td>15.791862</td>\n",
       "      <td>0.076333</td>\n",
       "      <td>-0.455136</td>\n",
       "      <td>0.145286</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.222498</td>\n",
       "      <td>3039081021079</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4707</th>\n",
       "      <td>0.203514</td>\n",
       "      <td>15.135419</td>\n",
       "      <td>0.115836</td>\n",
       "      <td>-0.876797</td>\n",
       "      <td>0.199050</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.328295</td>\n",
       "      <td>3021140035971</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3333</th>\n",
       "      <td>0.047964</td>\n",
       "      <td>17.652021</td>\n",
       "      <td>0.205368</td>\n",
       "      <td>-0.312531</td>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.011634</td>\n",
       "      <td>0.326617</td>\n",
       "      <td>3033124091172</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1         2         3         4         5    Period  \\\n",
       "9452  1.020265  15.028564  0.098795  1.324000  0.204075  0.006574  0.435117   \n",
       "2956 -0.241530  14.085678  0.224534 -1.364468  0.346250  0.015941  0.697790   \n",
       "7986  0.092216  15.791862  0.076333 -0.455136  0.145286  0.004834  0.222498   \n",
       "4707  0.203514  15.135419  0.115836 -0.876797  0.199050  0.007653  0.328295   \n",
       "3333  0.047964  17.652021  0.205368 -0.312531  0.406400  0.011634  0.326617   \n",
       "\n",
       "          File_Name  True_class_labels  \n",
       "9452  3021041040656                  6  \n",
       "2956  3047064090676                  1  \n",
       "7986  3039081021079                  5  \n",
       "4707  3021140035971                  2  \n",
       "3333  3033124091172                  2  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = X_training.join(periods.set_index('File_Name'), on='File_Name')\n",
    "training_set = training_set[['0','1','2','3','4','5','Period', 'File_Name', 'True_class_labels']]\n",
    "training_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>Period</th>\n",
       "      <th>File_Name</th>\n",
       "      <th>True_class_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>-0.066943</td>\n",
       "      <td>14.839103</td>\n",
       "      <td>0.155439</td>\n",
       "      <td>-1.482748</td>\n",
       "      <td>0.238150</td>\n",
       "      <td>0.010475</td>\n",
       "      <td>0.297547</td>\n",
       "      <td>3069008009339</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10680</th>\n",
       "      <td>0.076181</td>\n",
       "      <td>12.746181</td>\n",
       "      <td>0.085520</td>\n",
       "      <td>-1.137480</td>\n",
       "      <td>0.142900</td>\n",
       "      <td>0.006709</td>\n",
       "      <td>0.438605</td>\n",
       "      <td>3059087014112</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8787</th>\n",
       "      <td>0.359599</td>\n",
       "      <td>17.877790</td>\n",
       "      <td>0.219914</td>\n",
       "      <td>-0.267232</td>\n",
       "      <td>0.423973</td>\n",
       "      <td>0.012301</td>\n",
       "      <td>0.329560</td>\n",
       "      <td>3055033050538</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15677</th>\n",
       "      <td>0.598825</td>\n",
       "      <td>17.470716</td>\n",
       "      <td>0.221224</td>\n",
       "      <td>0.098760</td>\n",
       "      <td>0.398553</td>\n",
       "      <td>0.012663</td>\n",
       "      <td>0.299594</td>\n",
       "      <td>3021042028251</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21024</th>\n",
       "      <td>-0.172351</td>\n",
       "      <td>13.762360</td>\n",
       "      <td>0.842078</td>\n",
       "      <td>-1.184844</td>\n",
       "      <td>1.325150</td>\n",
       "      <td>0.061187</td>\n",
       "      <td>329.815304</td>\n",
       "      <td>3047057030258</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1         2         3         4         5  \\\n",
       "2412  -0.066943  14.839103  0.155439 -1.482748  0.238150  0.010475   \n",
       "10680  0.076181  12.746181  0.085520 -1.137480  0.142900  0.006709   \n",
       "8787   0.359599  17.877790  0.219914 -0.267232  0.423973  0.012301   \n",
       "15677  0.598825  17.470716  0.221224  0.098760  0.398553  0.012663   \n",
       "21024 -0.172351  13.762360  0.842078 -1.184844  1.325150  0.061187   \n",
       "\n",
       "           Period      File_Name  True_class_labels  \n",
       "2412     0.297547  3069008009339                  2  \n",
       "10680    0.438605  3059087014112                  5  \n",
       "8787     0.329560  3055033050538                  5  \n",
       "15677    0.299594  3021042028251                  5  \n",
       "21024  329.815304  3047057030258                  8  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set = X_testing.join(periods.set_index('File_Name'), on='File_Name')\n",
    "testing_set = testing_set[['0','1','2','3','4','5','Period', 'File_Name', 'True_class_labels']]\n",
    "testing_set.iloc[150:155,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "RRab_train        = stars_label(training_set, true_class_1)\n",
    "RRc_train         = stars_label(training_set, true_class_2) \n",
    "RRd_train         = stars_label(training_set, true_class_3)\n",
    "blazhko_train     = stars_label(training_set, true_class_4)\n",
    "contact_Bi_train  = stars_label(training_set, true_class_5)\n",
    "semi_det_Bi_train = stars_label(training_set, true_class_6)\n",
    "rot_train         = stars_label(training_set, true_class_7)\n",
    "LPV_train         = stars_label(training_set, true_class_8)\n",
    "delta_scuti_train = stars_label(training_set, true_class_9)\n",
    "ACEP_train        = stars_label(training_set, true_class_10)\n",
    "cep_ii_train      = stars_label(training_set, true_class_12)\n",
    "\n",
    "RRab_test        = stars_label(testing_set, true_class_1)\n",
    "RRc_test         = stars_label(testing_set, true_class_2) \n",
    "RRd_test         = stars_label(testing_set, true_class_3)\n",
    "blazhko_test     = stars_label(testing_set, true_class_4)\n",
    "contact_Bi_test  = stars_label(testing_set, true_class_5)\n",
    "semi_det_Bi_test = stars_label(testing_set, true_class_6)\n",
    "rot_test         = stars_label(testing_set, true_class_7)\n",
    "LPV_test         = stars_label(testing_set, true_class_8)\n",
    "delta_scuti_test = stars_label(testing_set, true_class_9)\n",
    "ACEP_test        = stars_label(testing_set, true_class_10)\n",
    "cep_ii_test      = stars_label(testing_set, true_class_12)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-45-ece471f6f52e>, line 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-ece471f6f52e>\"\u001b[0;36m, line \u001b[0;32m64\u001b[0m\n\u001b[0;31m    space ={'min_samples_split':hp.choice('min_samples_split', np.arange(,20,dtype=int)),            'n_estimators': hp.choice('n_estimators', [50,150,250,350,450,550,650,750,850,950]),            'class_weight':hp.choice('class_weight', ['balanced']),            'max_features':hp.choice('max_features',['auto', 'sqrt', 'log2'])}\u001b[0m\n\u001b[0m                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'-----------------------------------------------------------------------------'\n",
    "                                # FIRST LAYER\n",
    "'-----------------------------------------------------------------------------'\n",
    "training_data_FL, testing_data_FL, y_FL_training_counts = first_layer()\n",
    "X_train_FL, y_train_FL, X_test_FL, y_test_FL = smote_augmentation(training_data_FL, testing_data_FL,label='New_label',aug_tech='SMOTE')#aug_tech='ADASYN'\n",
    "\n",
    "if multi_class:\n",
    "    classes_types_FL = ['Eclipsing','Rotational','Pulsating']\n",
    "    types_FL         ='Type_FL'\n",
    "    nClasses_FL      = len(classes_types_FL)\n",
    "\n",
    "else:\n",
    "    classes_types_FL = ['Eclipsing','Rotational']\n",
    "    types_FL         ='Type_Binary'\n",
    "    nClasses_FL      = 2\n",
    "    \n",
    "\n",
    "X_tr = X_train_FL; y_tr=y_train_FL\n",
    "\n",
    "def objectives_xgb(space):\n",
    "    classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                               subsample = space['subsample'])\n",
    "        \n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "def objectives_rf(space):\n",
    "    classifier = RandomForestClassifier(min_samples_split=space['min_samples_split'],n_estimators = space['n_estimators'],\\\n",
    "                                class_weight=space['class_weight'],max_features=space['max_features'])\n",
    "        \n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "def hyperparameter_optimization_xgb(multi=False):\n",
    "    if multi:\n",
    "        objective = 'multi:softmax'\n",
    "    else:\n",
    "        objective = 'binary:logistic'\n",
    "        \n",
    "    space ={'objective':objective,'max_depth':hp.choice('max_depth', np.arange(1,15,dtype=int)),\\\n",
    "                    'eta': hp.quniform ('eta', 0.0, 0.09, 0.01),'subsample': hp.quniform ('x_subsample', 0.4,0.8,0.1)}\n",
    "    trials         = Trials()\n",
    "    opt_parameters = fmin(fn=objectives_xgb,space=space,algo=tpe.suggest,max_evals=5,trials=trials)\n",
    "    print(opt_parameters)\n",
    "    \n",
    "    best_params = space_eval(space, opt_parameters)\n",
    "    print(\"BEST PARAMETERS: \" + str(best_params))\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "def hyperparameter_optimization_rf(multi=False):\n",
    "    space ={'min_samples_split':hp.choice('min_samples_split', np.arange(,20,dtype=int)),\\\n",
    "            'n_estimators': hp.choice('n_estimators', [50,150,250,350,450,550,650,750,850,950]),\\\n",
    "            'class_weight':hp.choice('class_weight', ['balanced']),\\\n",
    "            'max_features':hp.choice('max_features',['auto', 'sqrt', 'log2'])}\n",
    "    \n",
    "\n",
    "    trials         = Trials()\n",
    "    opt_parameters = fmin(fn=objectives_rf,space=space,algo=tpe.suggest,max_evals=5,trials=trials)\n",
    "    print(opt_parameters)\n",
    "    \n",
    "    best_params = space_eval(space, opt_parameters)\n",
    "    print(\"BEST PARAMETERS: \" + str(best_params))\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "\n",
    "def analysis_XGB(X_train, y_train, types,save_model=False,multi=False):\n",
    "    opt_parameters_XGB = hyperparameter_optimization_xgb(multi=multi)\n",
    "    print(opt_parameters_XGB)\n",
    "    fit_model_XGB = model_save(XGBClassifier(**opt_parameters_XGB), X_train=X_train, y_train=y_train,\\\n",
    "                           filename_model= results_dir + types + '_XGB_model.sav', save_model=save_model)\n",
    "    return opt_parameters_XGB, fit_model_XGB\n",
    "\n",
    "def analysis_RF(X_train, y_train, types,save_model=False,multi=False):\n",
    "    opt_parameters_RF = hyperparameter_optimization_rf(multi=multi)\n",
    "    print(opt_parameters_RF)\n",
    "    fit_model_RF = model_save(RandomForestClassifier(**opt_parameters_RF), X_train=X_train, y_train=y_train,\\\n",
    "                           filename_model= results_dir + types + '_RF_model.sav', save_model=save_model)\n",
    "    return opt_parameters_RF, fit_model_RF\n",
    "\n",
    "\n",
    "def final_prediction_XGB(fitModel,X_train, y_train, X_test, y_test, testing_set,classes, types,nClasses,load_model=False):\n",
    "    ypred, accuracy, MCC, conf_mat, misclassified  = model_fit(fitModel,filename_model= results_dir + types +'_XGB_model.sav', X_train=X_train, y_train=y_train, X_test = X_test, y_test=y_test,\\\n",
    "                                                 classifier_model='XGBoost Classifier',classes=classes, filename =results_dir + types +'_XGB', load_model=load_model)\n",
    "    misclassified_data, new_DF = find_misclassification(misclassified,y_test, test_data=testing_set,ypred=ypred, save_dir=results_dir + types +'Misclassification_XGB.csv')\n",
    "\n",
    "    plotting = plot(conf_mat, classes_types=classes, classifier_model='XGBoost Classifier',\\\n",
    "                                  plot_title= plots_dir + types +'_XGB', X_test=X_test, y_test=y_test, nClasses=nClasses,cmap=plt.cm.Blues)\n",
    "    \n",
    "    fpr,tpr,roc_auc=plot_ROC_curve(X_test, y_test, nClasses=nClasses, fit_model=fit_model,plots_dir=plots_dir+types,classes_types=classes)\n",
    "\n",
    "    return ypred, accuracy, MCC, conf_mat, new_DF, misclassified, fpr,tpr,roc_auc\n",
    "\n",
    "def final_prediction_RF(fitModel,X_train, y_train, X_test, y_test, testing_set,classes, types,nClasses,load_model=False):\n",
    "    ypred, accuracy, MCC, conf_mat, misclassified  = model_fit(fitModel,filename_model= results_dir + types +'_RF_model.sav', X_train=X_train, y_train=y_train, X_test = X_test, y_test=y_test,\\\n",
    "                                                 classifier_model='RF Classifier',classes=classes, filename =results_dir + types +'_RF', load_model=load_model)\n",
    "    misclassified_data, new_DF = find_misclassification(misclassified,y_test, test_data=testing_set,ypred=ypred, save_dir=results_dir + types +'Misclassification_RF.csv')\n",
    "\n",
    "    plotting = plot(conf_mat, classes_types=classes, classifier_model='RF Classifier',\\\n",
    "                                  plot_title= plots_dir + types +'_RF', X_test=X_test, y_test=y_test, nClasses=nClasses,cmap=plt.cm.Blues)\n",
    "    fpr,tpr,roc_auc=plot_ROC_curve(X_test, y_test, nClasses=nClasses, fit_model=fit_model,plots_dir=plots_dir+types,classes_types=classes)\n",
    "\n",
    "    return ypred, accuracy, MCC, conf_mat, new_DF, misclassified,fpr,tpr,roc_auc\n",
    "\n",
    "\n",
    "# XGBoost Classifier   \n",
    "\n",
    "\n",
    "opt_xgb_FL, fit_model_xgb_FL = analysis_XGB(X_train_FL,y_train_FL, types_FL, save_model,multi=True) # This part can be commented when no training\n",
    "ypred_xgb_FL, accuracy_xgb_FL, MCC_xgb_FL, conf_mat_xgb_FL,new_DF_xgb_FL, misclassified_xgb_FL,fpr_xgb_FL,tpr_xgb_FL,roc_auc_xgb_FL  = final_prediction_XGB(fit_model_xgb_FL, X_train_FL, y_train_FL, X_test_FL, y_test_FL, testing_data_FL, classes_types_FL, types_FL, nClasses_FL, load_model)         \n",
    "\n",
    "\n",
    "opt_rf_FL, fit_model_rf_FL = analysis_RF(X_train_FL,y_train_FL, types_FL, save_model,multi=True) # This part can be commented when no training\n",
    "ypred_rf_FL, accuracy_rf_FL, MCC_rf_FL, conf_mat_rf_FL,new_DF_rf_FL, misclassified_rf_FL,fpr_rf_FL,tpr_rf_FL,roc_auc_rf_FL  = final_prediction_RF(fit_model_rf_FL, X_train_FL, y_train_FL, X_test_FL, y_test_FL, testing_data_FL, classes_types_FL, types_FL, nClasses_FL, load_model)         \n",
    "\n",
    "\n",
    "acc_xgb_FL.append(accuracy_xgb_FL)\n",
    "mcc_xgb_FL.append(MCC_xgb_FL)\n",
    "\n",
    "acc_rf_FL.append(accuracy_rf_FL)\n",
    "mcc_rf_FL.append(MCC_rf_FL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'second_layer_EB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-185efedde231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                         \u001b[0;31m# SECOND LAYER ECLIPSING BINARY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m'-------------------------------------------------------------------------------'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtraining_data_SL_EB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_data_SL_EB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_SL_EB_training_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecond_layer_EB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# all_testing_set_SL_EB = new_DF_xgb_FL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'second_layer_EB' is not defined"
     ]
    }
   ],
   "source": [
    "'-------------------------------------------------------------------------------'\n",
    "                        # SECOND LAYER ECLIPSING BINARY\n",
    "'-------------------------------------------------------------------------------'\n",
    "training_data_SL_EB, testing_data_SL_EB, y_SL_EB_training_counts = second_layer_EB()\n",
    "\n",
    "# all_testing_set_SL_EB = new_DF_xgb_FL\n",
    "# #         all_testing_set_SL_EB = new_DF_xgb_FL.drop(['New_label', 'Prediction'],axis=1)\n",
    "# ecl_class_test = all_testing_set_SL_EB[all_testing_set_SL_EB.True_class_labels==true_class_5]\n",
    "# EA_class_test  = all_testing_set_SL_EB[all_testing_set_SL_EB.True_class_labels==true_class_6]\n",
    "# testing_set_SL_EB  = pd.concat([ecl_class_test,EA_class_test], axis=0)\n",
    "\n",
    "testing_set_SL_EB = testing_data_SL_EB\n",
    "\n",
    "X_train_SL_EB, y_train_SL_EB, X_test_SL_EB, y_test_SL_EB = smote_augmentation(training_data_SL_EB, testing_set_SL_EB,label='True_class_labels',aug_tech='SMOTE')\n",
    "\n",
    "# Random Forest Classifier    \n",
    "classes_types_SL_EB = ['Ecl','EA']\n",
    "types_SL_EB         ='Type_SL_Ecl_EA'\n",
    "nClasses_SL_EB      = 2\n",
    "\n",
    "X_tr = X_train_SL_EB; y_tr=y_train_SL_EB\n",
    "\n",
    "def objectives_xgb(space):\n",
    "    classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                               subsample = space['subsample'])\n",
    "        \n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "def objectives_rf(space):\n",
    "    classifier = RandomForestClassifier(min_samples_split=space['min_samples_split'],n_estimators = space['n_estimators'],\\\n",
    "                                class_weight=space['class_weight'],max_features=space['max_features'])\n",
    "        \n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "# XGBoost Classifier   \n",
    "opt_xgb_SL_EB, fit_model_xgb_SL_EB = analysis_XGB(X_train_SL_EB, y_train_SL_EB, types_SL_EB, save_model,multi=False) # This part can be commented when no training  \n",
    "ypred_xgb_SL_EB, accuracy_xgb_SL_EB, MCC_xgb_SL_EB, conf_mat_xgb_SL_EB,new_DF_xgb_SL_EB,misclassified_xgb_SL_EB = final_prediction_XGB(fit_model_xgb_SL_EB, X_train_SL_EB, y_train_SL_EB, X_test_SL_EB, y_test_SL_EB,testing_set_SL_EB, classes_types_SL_EB, types_SL_EB, nClasses_SL_EB, load_model) \n",
    "\n",
    "opt_rf_SL_EB, fit_model_rf_SL_EB = analysis_RF(X_train_SL_EB, y_train_SL_EB, types_SL_EB, save_model,multi=False) # This part can be commented when no training  \n",
    "ypred_rf_SL_EB, accuracy_rf_SL_EB, MCC_rf_SL_EB, conf_mat_rf_SL_EB,new_DF_rf_SL_EB,misclassified_rf_SL_EB = final_prediction_RF(fit_model_rf_SL_EB, X_train_SL_EB, y_train_SL_EB, X_test_SL_EB, y_test_SL_EB,testing_set_SL_EB, classes_types_SL_EB, types_SL_EB, nClasses_SL_EB, load_model) \n",
    "\n",
    "\n",
    "acc_xgb_SL_EB.append(accuracy_xgb_SL_EB)\n",
    "mcc_xgb_SL_EB.append(MCC_xgb_SL_EB)\n",
    "\n",
    "acc_rf_SL_EB.append(accuracy_rf_SL_EB)\n",
    "mcc_rf_SL_EB.append(MCC_rf_SL_EB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR Lyrae train has (6122, 9)\n",
      "LPV train has (900, 9)\n",
      "Cepheids train has (214, 9)\n",
      "Delta Scuti train has (102, 9)\n",
      "RR_Lyrae_test has (2628, 9)\n",
      "LPV_test has (386, 9)\n",
      "cepheids_test has (92, 9)\n",
      "Delta Scuti test has (45, 9)\n",
      "[23 24 25 26]\n",
      "[6122  900  102  214]\n",
      "Before OverSampling, counts of label 23: (6122,)\n",
      "Before OverSampling, counts of label 24: (900,)\n",
      "Before OverSampling, counts of label 25: (102,)\n",
      "Before OverSampling, counts of label 26: (214,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 23: (6122,)\n",
      "After OverSampling, counts of label 24: (6122,)\n",
      "After OverSampling, counts of label 25: (6122,)\n",
      "After OverSampling, counts of label 26: (6122,)\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.08, 'max_depth': 8}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.05, 'max_depth': 10}\n",
      "Accuracy 0.991 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.04, 'max_depth': 2}\n",
      "Accuracy 0.995 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 4}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.06, 'max_depth': 12}\n",
      "100%|| 5/5 [03:03<00:00, 37.84s/it, best loss: 0.00396038415366]\n",
      "{'eta': 0.06, 'max_depth': 11, 'x_subsample': 0.8}\n",
      "BEST PARAMETERS: {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.06, 'max_depth': 12}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.06, 'max_depth': 12}\n",
      "Test set has shape (3151, 11)\n",
      "Misclassified data has shape (43, 11)\n",
      "New test set has shape (3108, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.98820396  0.          0.00152207  0.01027397]\n",
      " [ 0.          0.99222798  0.          0.00777202]\n",
      " [ 0.02222222  0.          0.97777778  0.        ]\n",
      " [ 0.07608696  0.01086957  0.          0.91304348]]\n",
      "Accuracy 0.995 params {'max_features': 'log2', 'min_samples_split': 18, 'n_estimators': 250, 'class_weight': 'balanced'}\n",
      "Accuracy 0.996 params {'max_features': 'log2', 'min_samples_split': 12, 'n_estimators': 50, 'class_weight': 'balanced'}\n",
      "Accuracy 0.995 params {'max_features': 'auto', 'min_samples_split': 15, 'n_estimators': 150, 'class_weight': 'balanced'}\n",
      "Accuracy 0.996 params {'max_features': 'auto', 'min_samples_split': 17, 'n_estimators': 350, 'class_weight': 'balanced'}\n",
      "Accuracy 0.996 params {'max_features': 'auto', 'min_samples_split': 6, 'n_estimators': 350, 'class_weight': 'balanced'}\n",
      "100%|| 5/5 [03:00<00:00, 41.65s/it, best loss: 0.00396038415366]\n",
      "{'max_features': 0, 'min_samples_split': 4, 'n_estimators': 3, 'class_weight': 0}\n",
      "BEST PARAMETERS: {'max_features': 'auto', 'min_samples_split': 6, 'n_estimators': 350, 'class_weight': 'balanced'}\n",
      "{'max_features': 'auto', 'min_samples_split': 6, 'n_estimators': 350, 'class_weight': 'balanced'}\n",
      "Test set has shape (3151, 11)\n",
      "Misclassified data has shape (44, 11)\n",
      "New test set has shape (3107, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.98668189  0.          0.00228311  0.01103501]\n",
      " [ 0.          0.99222798  0.          0.00777202]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.06521739  0.          0.          0.93478261]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1337a1f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12087bc10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'-----------------------------------------------------------------------------'\n",
    "                # SECOND LAYER RR LYRAE PULSATING LPV CEPHEIDS\n",
    "'-----------------------------------------------------------------------------'\n",
    "training_data_SL_RLCD, testing_data1_SL_RLCD, y_SL_RLCD_training_counts = second_layer_RLCD()\n",
    "# all_testing_set_SL_RLCD  = new_DF_xgb_FL\n",
    "# all_testing_set_SL_RLCD  = new_DF_xgb_FL.drop(['New_label', 'Prediction'],axis=1)\n",
    "\n",
    "# RRab_test                = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_1]\n",
    "# RRc_test                 = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_2]\n",
    "# RRd_test                 = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_3]\n",
    "# blazhko_test             = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_4]\n",
    "# LPV_test                 = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_8]\n",
    "# ACEP_test                = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_10]\n",
    "# cep_ii_test              = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_12]\n",
    "# delta_scuti_test         = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_9]\n",
    "\n",
    "# RR_Lyrae_test       = pd.concat([RRab_test,RRc_test,RRd_test,blazhko_test], axis=0)\n",
    "# RR_Lyrae_test_class = np.full(len(RR_Lyrae_test), RR_Lyrae_label, dtype=int)\n",
    "# LPV_test_class      = np.full(len(LPV_test), LPV_label, dtype=int)\n",
    "# cepheids_test       = pd.concat([ACEP_test, cep_ii_test] ,axis=0)\n",
    "# cepheids_test_class = np.full(len(cepheids_test), cepheids_label, dtype=int)\n",
    "# ds_test             = delta_scuti_test\n",
    "# ds_test_class       = np.full(len(ds_test), delta_scuti_label, dtype=int)\n",
    "\n",
    "# second_layer_RLCD_test       = pd.concat([RR_Lyrae_test,LPV_test,cepheids_test,ds_test], axis=0)\n",
    "# second_layer_RLCD_test_class = np.concatenate((RR_Lyrae_test_class,LPV_test_class,cepheids_test_class,ds_test_class), axis=0)\n",
    "# testing_data_SL_RLCD         = pd.DataFrame(second_layer_RLCD_test)\n",
    "# testing_data_SL_RLCD['New_label'] = second_layer_RLCD_test_class\n",
    "# testing_set_SL_RLCD          = testing_data_SL_RLCD\n",
    "\n",
    "testing_set_SL_RLCD = testing_data1_SL_RLCD\n",
    "\n",
    "X_train_SL_RLCD, y_train_SL_RLCD, X_test_SL_RLCD, y_test_SL_RLCD = smote_augmentation(training_data_SL_RLCD,testing_set_SL_RLCD,label='New_label',aug_tech='SMOTE')\n",
    "\n",
    "X_tr = X_train_SL_RLCD; y_tr=y_train_SL_RLCD\n",
    "\n",
    "def objectives_xgb(space):\n",
    "    classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                               subsample = space['subsample'])\n",
    "        \n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "def objectives_rf(space):\n",
    "    classifier = RandomForestClassifier(min_samples_split=space['min_samples_split'],n_estimators = space['n_estimators'],\\\n",
    "                                class_weight=space['class_weight'],max_features=space['max_features'])\n",
    "        \n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "# XGBoost Classifier    \n",
    "classes_types_SL_RLCD = ['RR Lyrae','LPV', 'Cepheids', '$\\delta$-Scuti']\n",
    "types_SL_RLCD         ='Type_SL_RLCD'\n",
    "nClasses_SL_RLCD      = len(classes_types_SL_RLCD)\n",
    "\n",
    "opt_xgb_SL_RLCD, fit_model_xgb_SL_RLCD = analysis_XGB(X_train_SL_RLCD, y_train_SL_RLCD, types_SL_RLCD, save_model,multi=True) # This part can be commented if you don't want to train the algorithm  \n",
    "ypred_xgb_SL_RLCD, accuracy_xgb_SL_RLCD, MCC_xgb_SL_RLCD, conf_mat_xgb_SL_RLCD, new_DF_xgb_SL_RLCD,misclassified_xgb_SL_RLCD = final_prediction_XGB(fit_model_xgb_SL_RLCD,X_train_SL_RLCD, y_train_SL_RLCD, X_test_SL_RLCD, y_test_SL_RLCD,testing_set_SL_RLCD, classes_types_SL_RLCD, types_SL_RLCD, nClasses_SL_RLCD,load_model)\n",
    "\n",
    "opt_rf_SL_RLCD, fit_model_rf_SL_RLCD = analysis_RF(X_train_SL_RLCD, y_train_SL_RLCD, types_SL_RLCD, save_model,multi=True) # This part can be commented if you don't want to train the algorithm  \n",
    "ypred_rf_SL_RLCD, accuracy_rf_SL_RLCD, MCC_rf_SL_RLCD, conf_mat_rf_SL_RLCD, new_DF_rf_SL_RLCD,misclassified_rf_SL_RLCD = final_prediction_RF(fit_model_rf_SL_RLCD,X_train_SL_RLCD, y_train_SL_RLCD, X_test_SL_RLCD, y_test_SL_RLCD,testing_set_SL_RLCD, classes_types_SL_RLCD, types_SL_RLCD, nClasses_SL_RLCD,load_model)\n",
    "\n",
    "\n",
    "acc_xgb_SL_RLCD.append(accuracy_xgb_SL_RLCD)\n",
    "mcc_xgb_SL_RLCD.append(MCC_xgb_SL_RLCD)\n",
    "\n",
    "acc_rf_SL_RLCD.append(accuracy_rf_SL_RLCD)\n",
    "mcc_rf_SL_RLCD.append(MCC_rf_SL_RLCD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRab train has (3026, 9)\n",
      "RRc train has (2626, 9)\n",
      "RRd train has (351, 9)\n",
      "Blazhko train has (119, 9)\n",
      "RRab test has (1299, 9)\n",
      "RRc test has (1126, 9)\n",
      "RRd test has (151, 9)\n",
      "Blazhko test has (52, 9)\n",
      "Before OverSampling, counts of label 1: (3026,)\n",
      "Before OverSampling, counts of label 2: (2626,)\n",
      "Before OverSampling, counts of label 3: (351,)\n",
      "Before OverSampling, counts of label 4: (119,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 1: (3026,)\n",
      "After OverSampling, counts of label 2: (3026,)\n",
      "After OverSampling, counts of label 3: (3026,)\n",
      "After OverSampling, counts of label 4: (3026,)\n",
      "Accuracy 0.913 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.07, 'max_depth': 6}\n",
      "Accuracy 0.936 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 12}\n",
      "Accuracy 0.937 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 14}\n",
      "Accuracy 0.934 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.05, 'max_depth': 12}\n",
      "Accuracy 0.929 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.03, 'max_depth': 9}\n",
      "100%|| 5/5 [02:35<00:00, 29.57s/it, best loss: 0.0627872787279]\n",
      "{'eta': 0.03, 'max_depth': 13, 'x_subsample': 0.7000000000000001}\n",
      "BEST PARAMETERS: {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 14}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 14}\n",
      "Test set has shape (2628, 11)\n",
      "Misclassified data has shape (336, 11)\n",
      "New test set has shape (2292, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.94226328  0.          0.00461894  0.05311778]\n",
      " [ 0.0026643   0.86767318  0.12966252  0.        ]\n",
      " [ 0.05298013  0.47019868  0.46357616  0.01324503]\n",
      " [ 0.57692308  0.          0.01923077  0.40384615]]\n",
      "Accuracy 0.919 params {'max_features': 'auto', 'min_samples_split': 15, 'n_estimators': 450, 'class_weight': 'balanced'}\n",
      "Accuracy 0.915 params {'max_features': 'sqrt', 'min_samples_split': 19, 'n_estimators': 50, 'class_weight': 'balanced'}\n",
      "Accuracy 0.916 params {'max_features': 'log2', 'min_samples_split': 17, 'n_estimators': 450, 'class_weight': 'balanced'}\n",
      "Accuracy 0.925 params {'max_features': 'auto', 'min_samples_split': 10, 'n_estimators': 750, 'class_weight': 'balanced'}\n",
      "Accuracy 0.921 params {'max_features': 'sqrt', 'min_samples_split': 13, 'n_estimators': 350, 'class_weight': 'balanced'}\n",
      "100%|| 5/5 [03:46<00:00, 48.79s/it, best loss: 0.0752625262526]\n",
      "{'max_features': 0, 'min_samples_split': 8, 'n_estimators': 7, 'class_weight': 0}\n",
      "BEST PARAMETERS: {'max_features': 'auto', 'min_samples_split': 10, 'n_estimators': 750, 'class_weight': 'balanced'}\n",
      "{'max_features': 'auto', 'min_samples_split': 10, 'n_estimators': 750, 'class_weight': 'balanced'}\n",
      "Test set has shape (2628, 11)\n",
      "Misclassified data has shape (311, 11)\n",
      "New test set has shape (2317, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.94611239  0.          0.00384911  0.05003849]\n",
      " [ 0.0035524   0.87388988  0.12255773  0.        ]\n",
      " [ 0.05298013  0.42384106  0.50993377  0.01324503]\n",
      " [ 0.48076923  0.          0.          0.51923077]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12087bed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b046e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'-----------------------------------------------------------------------------'\n",
    "            # THIRD LAYER RR LYRAE: RRab, RRc, RRd, Blazhko\n",
    "'-----------------------------------------------------------------------------'\n",
    "training_data_TL_RRLyrae, testing_data_TL_RRLyrae, y_TL_RRLyrae_training_counts = third_layer_RRLyrae()\n",
    "# all_testing_set_TL_RL  = new_DF_xgb_SL_RLCD.drop([\"Prediction\"],axis=1)\n",
    "# RRab_class_test    = all_testing_set_TL_RL[all_testing_set_TL_RL.True_class_labels==true_class_1]\n",
    "# RRc_class_test     = all_testing_set_TL_RL[all_testing_set_TL_RL.True_class_labels==true_class_2]\n",
    "# RRd_class_test     = all_testing_set_TL_RL[all_testing_set_TL_RL.True_class_labels==true_class_3]\n",
    "# blazhko_class_test = all_testing_set_TL_RL[all_testing_set_TL_RL.True_class_labels==true_class_4]\n",
    "# testing_set_TL_RL  = pd.concat([RRab_class_test,RRc_class_test,RRd_class_test,blazhko_class_test], axis=0)\n",
    "testing_set_TL_RL = testing_data_TL_RRLyrae\n",
    "\n",
    "X_train_TL_RL, y_train_TL_RL,X_test_TL_RL, y_test_TL_RL = smote_augmentation(training_data_TL_RRLyrae,testing_set_TL_RL,label='True_class_labels',aug_tech='SMOTE')\n",
    "\n",
    "X_tr = X_train_TL_RL; y_tr=y_train_TL_RL\n",
    "\n",
    "def objectives_xgb(space):\n",
    "    classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                               subsample = space['subsample'])\n",
    "        \n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "def objectives_rf(space):\n",
    "    classifier = RandomForestClassifier(min_samples_split=space['min_samples_split'],n_estimators = space['n_estimators'],\\\n",
    "                                class_weight=space['class_weight'],max_features=space['max_features'])\n",
    "        \n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "classes_types_TL_RL = ['RRab', 'RRc', 'RRd', \"Blazhko\"]\n",
    "types_TL_RL         ='Type_TL_RRLyrae'\n",
    "nClasses_TL_RL      = len(classes_types_TL_RL)\n",
    "\n",
    "# XGBoost Classifier   \n",
    "opt_xgb_TL_RL, fit_model_xgb_TL_RL = analysis_XGB(X_train_TL_RL, y_train_TL_RL, types_TL_RL, save_model,multi=True) # This part can be commented when no training \n",
    "ypred_xgb_TL_RL, accuracy_xgb_TL_RL, MCC_xgb_TL_RL, conf_mat_xgb_TL_RL, new_DF_xgb_TL_RL, misclassified_xgb_TL_RL = final_prediction_XGB(fit_model_xgb_TL_RL, X_train_TL_RL, y_train_TL_RL, X_test_TL_RL, y_test_TL_RL,testing_set_TL_RL, classes_types_TL_RL, types_TL_RL, nClasses_TL_RL, load_model) \n",
    "\n",
    "opt_rf_TL_RL, fit_model_rf_TL_RL = analysis_RF(X_train_TL_RL, y_train_TL_RL, types_TL_RL, save_model,multi=True) # This part can be commented when no training \n",
    "ypred_rf_TL_RL, accuracy_rf_TL_RL, MCC_rf_TL_RL, conf_mat_rf_TL_RL, new_DF_rf_TL_RL, misclassified_rf_TL_RL = final_prediction_RF(fit_model_rf_TL_RL, X_train_TL_RL, y_train_TL_RL, X_test_TL_RL, y_test_TL_RL,testing_set_TL_RL, classes_types_TL_RL, types_TL_RL, nClasses_TL_RL, load_model) \n",
    "\n",
    "\n",
    "acc_xgb_TL_RL.append(accuracy_xgb_TL_RL)\n",
    "mcc_xgb_TL_RL.append(MCC_xgb_TL_RL)\n",
    "\n",
    "acc_rf_TL_RL.append(accuracy_rf_TL_RL)\n",
    "mcc_rf_TL_RL.append(MCC_rf_TL_RL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACEP train has (107, 9)\n",
      "Cep-II train has (107, 9)\n",
      "ACEP test has (46, 9)\n",
      "Cep-II test has (46, 9)\n",
      "Before OverSampling, counts of label 10: (107,)\n",
      "Before OverSampling, counts of label 12: (107,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 10: (107,)\n",
      "After OverSampling, counts of label 12: (107,)\n",
      "Accuracy 0.864 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.06, 'max_depth': 8}\n",
      "Accuracy 0.850 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.06, 'max_depth': 1}\n",
      "Accuracy 0.878 params {'objective': 'binary:logistic', 'subsample': 0.8, 'eta': 0.04, 'max_depth': 7}\n",
      "Accuracy 0.869 params {'objective': 'binary:logistic', 'subsample': 0.8, 'eta': 0.06, 'max_depth': 2}\n",
      "Accuracy 0.864 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.0, 'max_depth': 10}\n",
      "100%|| 5/5 [00:00<00:00,  8.29it/s, best loss: 0.121861471861]\n",
      "{'eta': 0.04, 'max_depth': 6, 'x_subsample': 0.8}\n",
      "BEST PARAMETERS: {'objective': 'binary:logistic', 'subsample': 0.8, 'eta': 0.04, 'max_depth': 7}\n",
      "{'objective': 'binary:logistic', 'subsample': 0.8, 'eta': 0.04, 'max_depth': 7}\n",
      "Test set has shape (92, 11)\n",
      "Misclassified data has shape (12, 11)\n",
      "New test set has shape (80, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.91304348  0.08695652]\n",
      " [ 0.17391304  0.82608696]]\n",
      "Accuracy 0.864 params {'max_features': 'sqrt', 'min_samples_split': 18, 'n_estimators': 50, 'class_weight': 'balanced'}\n",
      "Accuracy 0.864 params {'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 150, 'class_weight': 'balanced'}\n",
      "Accuracy 0.864 params {'max_features': 'auto', 'min_samples_split': 10, 'n_estimators': 250, 'class_weight': 'balanced'}\n",
      "Accuracy 0.864 params {'max_features': 'sqrt', 'min_samples_split': 8, 'n_estimators': 850, 'class_weight': 'balanced'}\n",
      "Accuracy 0.860 params {'max_features': 'log2', 'min_samples_split': 10, 'n_estimators': 150, 'class_weight': 'balanced'}\n",
      "100%|| 5/5 [00:20<00:00,  4.03s/it, best loss: 0.135930735931]\n",
      "{'max_features': 0, 'min_samples_split': 8, 'n_estimators': 2, 'class_weight': 0}\n",
      "BEST PARAMETERS: {'max_features': 'auto', 'min_samples_split': 10, 'n_estimators': 250, 'class_weight': 'balanced'}\n",
      "{'max_features': 'auto', 'min_samples_split': 10, 'n_estimators': 250, 'class_weight': 'balanced'}\n",
      "Test set has shape (92, 11)\n",
      "Misclassified data has shape (10, 11)\n",
      "New test set has shape (82, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.93478261  0.06521739]\n",
      " [ 0.15217391  0.84782609]]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x134713bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a6dda90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'-----------------------------------------------------------------------------'\n",
    "                        # THIRD LAYER Cepheids: ACEP and Cep-II\n",
    "'-----------------------------------------------------------------------------'\n",
    "training_data_TL_cep, testing_data_TL_cep, y_TL_cep_training_counts = third_layer_Cepheids()\n",
    "# all_testing_set_TL_Cep  = new_DF_xgb_SL_RLCD.drop([\"Prediction\"],axis=1)\n",
    "# ACEP_class_test    = all_testing_set_TL_Cep[all_testing_set_TL_Cep.True_class_labels==true_class_10]\n",
    "# Cep_ii_class_test  = all_testing_set_TL_Cep[all_testing_set_TL_Cep.True_class_labels==true_class_12]\n",
    "# testing_set_TL_Cep = pd.concat([ACEP_class_test,Cep_ii_class_test], axis=0)\n",
    "\n",
    "testing_set_TL_Cep = testing_data_TL_cep\n",
    "\n",
    "X_train_TL_Cep, y_train_TL_Cep, X_test_TL_Cep, y_test_TL_Cep = smote_augmentation(training_data_TL_cep,testing_set_TL_Cep,label='True_class_labels',aug_tech='SMOTE')\n",
    "\n",
    "X_tr = X_train_TL_Cep; y_tr=y_train_TL_Cep\n",
    "\n",
    "def objectives_xgb(space):\n",
    "    classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                               subsample = space['subsample'])\n",
    "        \n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "def objectives_rf(space):\n",
    "    classifier = RandomForestClassifier(min_samples_split=space['min_samples_split'],n_estimators = space['n_estimators'],\\\n",
    "                                class_weight=space['class_weight'],max_features=space['max_features'])\n",
    "        \n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "classes_types_TL_Cep = ['ACEP','CEP-II']\n",
    "types_TL_Cep         ='Type_TL_Cepheids'\n",
    "nClasses_TL_Cep      = 2\n",
    "\n",
    "# XGBoost Classifier   \n",
    "opt_xgb_TL_Cep, fit_model_xgb_TL_Cep = analysis_XGB(X_train_TL_Cep, y_train_TL_Cep, types_TL_Cep, save_model,multi=False) # This part can be commented when no training\n",
    "ypred_xgb_TL_Cep, accuracy_xgb_TL_Cep, MCC_xgb_TL_Cep, conf_mat_xgb_TL_Cep, new_DF_xgb_TL_Cep, misclassified_xgb_TL_Cep = final_prediction_XGB(fit_model_xgb_TL_Cep, X_train_TL_Cep, y_train_TL_Cep, X_test_TL_Cep, y_test_TL_Cep, testing_set_TL_Cep, classes_types_TL_Cep, types_TL_Cep, nClasses_TL_Cep, load_model) \n",
    "\n",
    "opt_rf_TL_Cep, fit_model_rf_TL_Cep = analysis_RF(X_train_TL_Cep, y_train_TL_Cep, types_TL_Cep, save_model,multi=False) # This part can be commented when no training\n",
    "ypred_rf_TL_Cep, accuracy_rf_TL_Cep, MCC_rf_TL_Cep, conf_mat_rf_TL_Cep, new_DF_rf_TL_Cep, misclassified_rf_TL_Cep = final_prediction_RF(fit_model_rf_TL_Cep, X_train_TL_Cep, y_train_TL_Cep, X_test_TL_Cep, y_test_TL_Cep, testing_set_TL_Cep, classes_types_TL_Cep, types_TL_Cep, nClasses_TL_Cep, load_model) \n",
    "\n",
    "\n",
    "acc_xgb_TL_Cep.append(accuracy_xgb_TL_Cep)\n",
    "mcc_xgb_TL_Cep.append(MCC_xgb_TL_Cep)\n",
    "\n",
    "acc_rf_TL_Cep.append(accuracy_rf_TL_Cep)\n",
    "mcc_rf_TL_Cep.append(MCC_rf_TL_Cep)\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "\n",
    "metrics = open(\"./hierarchical-results_SMOTE/metrics.txt\", 'w')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_FL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_xgb_FL)*100,np.std(acc_xgb_FL)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_xgb_FL)*100,np.std(mcc_xgb_FL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_SL_EB) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_xgb_SL_EB)*100,np.std(acc_xgb_SL_EB)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_xgb_SL_EB)*100,np.std(mcc_xgb_SL_EB)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_SL_RLCD) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_xgb_SL_RLCD)*100,np.std(acc_xgb_SL_RLCD)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_xgb_SL_RLCD)*100,np.std(mcc_xgb_SL_RLCD)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_TL_RL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_xgb_TL_RL)*100,np.std(acc_xgb_TL_RL)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_xgb_TL_RL)*100,np.std(mcc_xgb_TL_RL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_TL_Cep) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({}  {}) %\".format(np.mean(acc_xgb_TL_Cep)*100,np.std(acc_xgb_TL_Cep)) + '\\n')\n",
    "metrics.write(\"MCC: ({}  {})\".format(np.mean(mcc_xgb_TL_Cep)*100,np.std(mcc_xgb_TL_Cep)) + '\\n')\n",
    "metrics.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
